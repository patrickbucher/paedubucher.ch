<?xml version="1.0" ?>
<feed xmlns="http://www.w3.org/2005/Atom">
    <title>paedubucher.ch</title>
    <subtitle>paedubucher.ch Article Feed</subtitle>
    <link href="https://paedubucher.ch/atom.xml" rel="self"/>
    <link href="https://paedubucher.ch/"/>
    <id>https://paedubucher.ch/</id>
    <updated>2021-07-24T16:13:08.779280Z</updated>
    <entry>
        <title>Functional Programming in Python</title>
        <link href="https://paedubucher.ch/articles/2021-07-24-functional-programming-in-python.html"/>
        <author>
            <name>Patrick Bucher</name>
        </author>
        <id>https://paedubucher.ch/articles/2021-07-24-functional-programming-in-python.html</id>
        <updated>2021-07-24T18:07:00Z</updated>
        <content type="text/markdown; charset=UTF-8">
This overview is inspired by [Functional Programming in
Python](https://leanpub.com/functionalprogramminginpython) by Martin McBride.

# Introduction

Python supports three major programming paradigms:

- Procedural Programming: Code is structured in blocks (functions, loops, if
  statements); simple, but hard to maintain big code bases.
- Object Oriented Programming (OOP): Code is structured in interacting objects;
  this encapsulation makes independent testing easier, the approach scales
  better to larger code bases.
- Functional Programmnig (FP): Functions are used as the main building block;
  a declarative rather than imperative programming style is used.

Those paradigms are usually mixed; however, FP is often neglected.

## Functional Concepts

In FP, functions are _first class objects_: They can be stored in variables,
passed to other functions as parameters, or be returned from functions.

Functions that operate on functions are called _higher order functions_.

A _pure function_ calculates a result without any side effects. Its result only
depends on the input parameters, not on global state. Neither is global state
changed. A pure function called multiple times with the same arguments always
returns the same result.

FP fits well together with immutable objects such as strings and tuples.
Iterators, which do not allow for modification, but for _lazy evaluation_, are
often preferred over lists.

Higher order functions (`filter`, `map`, `apply`) and recursion are preferred
over structural constructs (`if`/`else` branching, loops).

New functions are created dynamically by combining existing functions.

## Pros and Cons

FP has a lot of _advantages_:

- Conciseness: more can be expressed in less lines of code thanks to more
  abstract constructs
- Clarity: the programmer's intention is put across better by using higher-order
  functions like `map` than loop constructs that have to be deciphered line by
  line
- Provability: without side effects, reasoning about programs is easier;
  mathematical correctness proofs become _possible_
- Concurrency: without side effects, functions can be executed independently and
  in parallel and won't cause race conditions

However, this comes with some _disadvantages_:

- Purity is often not possible, because the purpose of many programs is to
  change global state. A line between pure and impure code has to be drawn.
- A lot of learning effort is required to understand functional concepts such as
  lambda expressions, closures, partial functions, currying etc.
- Functional code can be less efficient than structured code due to constructs
  that are less efficient (recursion instead of loops) or more expensive
  (re-building data structures instead of modifying them).

# Functions as Objects

Functions can be stored in variables like, say, a string:

```python
name = 'Dilbert'

employee = name

def say_hi(name):
    return f'Hello, {name}'

greet = say_hi

print(name)
print(employee)
print(say_hi)
print(greet)
```

There are two variables (_aliases_) pointing to the same object in memory:

    Dilbert
    Dilbert
    &lt;function say_hi at 0x7f955db5b040&gt;
    &lt;function say_hi at 0x7f955db5b040&gt;

It's also possible (but hardly advisable) to overwrite a function reference:

```python
def greet(name):
    return f'Hello, {name}'

def say_hi(name):
    return greet(name)

print(say_hi('Dilbert'))

def greet(name):
    return f'Greetings, {name}'

print(say_hi('Wally'))
```

After overwriting the `greet` function, the second implementation is called:

    Hello, Dilbert
    Greetings, Wally

The implementation of `say_hi` function has been modified indirectly, which
could introduce subtle bugs.

Consider the following conversion functions:

```python
def miles_to_kilometers(miles):
    return miles / 1.60934

def usd_to_chf(usd):
    return usd / 0.92

print('500 miles =', miles_to_kilometers(500), 'km')
print('100 usd =', usd_to_chf(100), 'chf')
```

Which perform their conversion independently:

    500 miles = 310.68636832490336 km
    100 usd = 108.69565217391303 chf

However, both functions implement the same conversion mechanism, which can be
generalized:

```python
def convert(f, x):
    return f(x)

print('500 miles =', convert(miles_to_kilometers, 500), 'km')
print('100 usd =', convert(usd_to_chf, 100), 'chf')
```

Both a function and a number are passed to `convert`, which then applies the
function to the number. Any conversion can be made, also between different
types:

```python
def format_currency(x):
    return f'{x:.2f}'

convert(format_currency, 10/3)  # '3.33 chf'
```

## Sorting

The built-in `sorted` function accepts an optional `key` function that allows
for customized sorting:

```python
dilbert = ('Dilbert', 42)
alice = ('Alice', 37)
dogbert = ('Dogbert', 7)
ashok = ('Ashok', 21)

employees = [dilbert, alice, dogbert, ashok]

def get_name(employee):
    return employee[0]

def get_age(employee):
    return employee[1]

by_name = sorted(employees, key=get_name)
by_age = sorted(employees, key=get_age)

print(by_name)
print(by_age)
```

The list of employees is sorted twice: once by name, and once by age:

    [('Alice', 37), ('Ashok', 21), ('Dilbert', 42), ('Dogbert', 7)]
    [('Dogbert', 7), ('Ashok', 21), ('Alice', 37), ('Dilbert', 42)]

## Lambdas

The code can be shortened by using unnamed _lambda functions_:

```python
by_name = sorted(employees, key=lambda e: e[0])
by_age = sorted(employees, key=lambda e: e[1])
```

Lambdas consist of a single expression and, thus, should only be used for very
simple computations that can be clearly understood without a function name or
additional comments. Use a regular function if an expression is used more than
once.

Lambdas are function objects that can also be called directly:

```python
&gt;&gt;&gt; (lambda x: x ** 2)(5)
24
```

More practically, they can be returned from functions:

```python
def create_increment_function(step=1):
    return lambda x: x + step

add_one = create_increment_function()
add_two = create_increment_function(step=2)

add_one(5)  # 5
add_two(5)  # 7
```

Operators are not functions, but can be wrapped in lambda expressions for use
with higher-order functions:

```python
def calculate(op, a, b):
    return op(a, b)

calculate(lambda a, b: a + b, 3, 1)  # 4
calculate(lambda a, b: a * b, 3, 2)  # 6
```

## Operator Functions

The `operator` module contains pre-defined functions for common operators, so no
lambdas have to be implemented:

```python
import operator

def calculate(op, a, b):
    return op(a, b)

calculate(operator.add, 3, 1)  # 4
calculate(operator.mul, 3, 2)  # 6
```

See the [documentation of the `operator`
module](https://docs.python.org/3/library/operator.html#mapping-operators-to-functions)
for a full list of operators and their function equivalents.

## Partial Function Application

Functions can be _partially applied_, i.e. called with fewer arguments than
expected, which returns a function only expecting the missing arguments:

```python
from functools import partial

def f(a, b, c, x):
    return a * x**2 + b * x + c

f(2, 4, 6, 1)  # x=1: 2x² + 4x + 6 = 12

g = partial(f, 2, 4, 6)
g(1)   # x=1: 2x² + 4x + 6 = 12
g(2)   # x=2: 2x² + 4x + 6 = 22
```

# Mutability

Lists, dictionaries, and sets are _mutable_; numbers, strings, and tuples are
_immutable_. A frozen set is an immutable version of a set. References to any of
those objects are always mutable: by re-assigning a variable, the object pointed
to is _not changed_, but _another object_ is pointed to instead.

Notice that mutability is shallow. A tuple itself cannot be modified, but the
elements of a tuple containing of lists can be modified.

The `sort` method of a list modifies the underlying list, whereas the `sorted`
function returns a sorted copy of the given list.

A list can be copied by passing it to the `list` function:

```python
def tail(l):
    del l[0]
    return l

numbers = [1, 2, 3]
print(tail(numbers))  # [2, 3]
print(numbers)        # [2, 3], too (modified)

numbers = [1, 2, 3]
print(tail(list(numbers)))  # [2, 3]
print(numbers)              # [1, 2, 3], still (unmodified)
```

This, however, is very inefficient. Instead, the `tail` function could work with
slicing to guarantee immutability:

```python
def tail(l):
    return l[1:]

numbers = [1, 2, 3]
print(tail(numbers))  # [2, 3]
print(numbers)        # [1, 2, 3], still (unmodified)
```

Under the hood, slicing is copying, so this solution is not very efficient, too.

Modifications that affect every single item of a list can be expressed using
_list comprehensions_:

```python
numbers = [1, 2, 3]
twice = [x * 2 for x in numbers]
print(twice)  # [2, 4, 6]
```

# Recursion

Functions that call themselves are a common technique in functional programming.
A problem is thereby reduced to its base case, which is defined statically. In
the general case, a problem is simplified towards the base case:

```python
def factorial(n):
    if n == 0:
        return 1
    elif n &gt; 0:
        return n * factorial(n - 1)

print(factorial(2))  # 2
print(factorial(3))  # 6
print(factorial(4))  # 24
```

The bigger the argument `n` is chosen, the more functions are running at the
same time:

    factorial(6)
    6 * factorial(5)
    6 * 5 * factorial(4)
    6 * 5 * 4 * factorial(3)
    6 * 5 * 4 * 3 * factorial(2)
    6 * 5 * 4 * 3 * 2 * factorial(1)
    6 * 5 * 4 * 3 * 2 * 1 factorial(0)
    6 * 5 * 4 * 3 * 2 * 1 * 1
    6 * 5 * 4 * 3 * 2 * 1
    6 * 5 * 4 * 3 * 2
    6 * 5 * 4 * 6
    6 * 5 * 24
    6 * 120
    720

This doesn't scale well. An alternative approach to recursive functions are
tail-recursive functions, which carry intermediate results as an extra
accumulator parameter (`acc`):

```python
def factorial(n, acc=1):
    if n == 0:
        return acc
    elif n &gt; 0:
        return factorial(n-1, n * acc)
```

Which leads to an easier to understand call stack:

    factorial(6, 1)
    factorial(5, 6)
    factorial(4, 30)
    factorial(3, 120)
    factorial(2, 360)
    factorial(1, 720)
    factorial(0, 720)
    720

## (No) Tail Call Optimization

Some compilers are able to optimize tail-recursive functions by re-using stack
frames for multiple function calls. Unfortunately, Python doesn't support this
optimization, so other solutions needs to be considered, such as loops.

Recursion becomes even more inefficient as multiple additional functions are
called in each step, as a recursive implementation of a function to compute the
Fibonacci numbers requires:

```python
def fib(n):
    print(f'fib({n})')
    if n == 0:
        return 0
    elif n == 1:
        return 1
    else:
        return fib(n-2) + fib(n-1)
```

The `print` call makes the amount of (redundant) functions being called
apparent:

    &gt;&gt;&gt; fib(6)
    fib(6)
    fib(4)
    fib(2)
    fib(0)
    fib(1)
    fib(3)
    fib(1)
    fib(2)
    fib(0)
    fib(1)
    fib(5)
    fib(3)
    fib(1)
    fib(2)
    fib(0)
    fib(1)
    fib(4)
    fib(2)
    fib(0)
    fib(1)
    fib(3)
    fib(1)
    fib(2)
    fib(0)
    fib(1)
    8

The `fib` function is called with the argument `1` alone eight times. The
inefficiency becomes even more striking when using a bigger `n` and counting the
function calls (`fibonacci.py`):

```python
calls = 0

def fib(n):
    global calls
    calls += 1
    if n == 0:
        return 0
    elif n == 1:
        return 1
    else:
        return fib(n-2) + fib(n-1)

print(f'fib(35)={fib(35)} after {calls} calls')
```

Almost 30 million function calls in a bit less than five seconds are required to
compute the 35th Fibonacci number:

```bash
$ time python3 fibonacci.py
fib(35)=9227465 after 29860703 calls

real    0m4.948s
user    0m4.946s
sys     0m0.000s
```

## Memoization

When many intermediate results are computed multiple times, re-using those
results helps saving function calls. For this purpose, the function arguments
are (keys) are put together with the results (values) into a dictionary. This
technique is called _memoization_:

```python
calls = 0
cache = {}

def fib(n):
    global calls
    calls += 1
    if n in cache:
        return  cache[n]
    else:
        if n == 0:
            result = 0
        elif n == 1:
            result = 1
        else:
            result = fib(n-2) + fib(n-1)
        cache[n] = result
        return result

print(f'fib(35)={fib(35)} after {calls} calls')
```

Which reduces function calls by a factor of more than 4*10^5, and runtime by a
factor of roughly 167 (memoization comes with a slight overhead).

```bash
fib(35)=9227465 after 69 calls

real    0m0.030s
user    0m0.030s
sys     0m0.000s
```

Memoization is a _cross cutting concern_ that has little to do with the function
itself. Python's `functools` has a decorator `lru_cache` (least recently used
cache) which provides memoization out-of-the-box:

```python
from functools import lru_cache

calls = 0

@lru_cache
def fib(n):
    global calls
    calls += 1
    if n == 0:
        result = 0
    elif n == 1:
        result = 1
    else:
        result = fib(n-2) + fib(n-1)
    return result

print(f'fib(35)={fib(35)} after {calls} calls')
```

Even less functions are invoked, because the caching mechanism is around the
function:

```bash
fib(35)=9227465 after 36 calls

real    0m0.029s
user    0m0.025s
sys     0m0.004s
```

## Flattening Lists

Lists in Python can be nested:

```python
[1, [2, [3, 4, [5, 6], 7], 8], 9]
```

Such a list contains numbers and lists, which again contain numbers and lists,
and so on. It is often useful to _flatten_ such a list:

```python
[1, 2, 3, 4, 5, 6, 7, 8, 9]
```

For this purpose, a recursive implementation processes the nested list one by
one. The first element (`head`) of the remaining list is considered in each
function call, and the remaining elements (`tail`) are delegated to another
recursive call. Then, the solution is combined:

```python
def flatten(x):
    if not isinstance(x, list):
        # x is a number: first base case
        return [x]
    if x == []:
        # x is an empty list: second base case
        return x
    else:
        # x is a non-empty list: general case
        return flatten(x[0]) + flatten(x[1:])
```

Memoization won't help here, because `x` is different for every function call. A
more feasible approach would be to fall back to loops:

```python
def flatten(x):
    if not isinstance(x, list):
        # x is a number: first base case
        return [x]
    if x == []:
        # x is an empty list: second base case
        return x
    else:
        # x is a non-empty list: general case
        r = []
        for e in x:
            if isinstance(e, list):
                r += flatten(e)
            else:
                r.append(e)
        return r
```

A recursive function call here only takes place for each additional depth level,
not for every additional element.

# Closures

Functions can contain other functions. The inner function cannnot be seen from
the outside of the outer function, unless the outer function returns the inner
function. In this example, an inner function `grade` is used from the outer
function `grade_exams`.

```python
def grade_exams(candidate_scores, max_score):

    def grade(score):
        # Swiss grades: 1..6
        return score / max_score * 5 + 1
    
    candidate_grades = {}
    for candidate, score in candidate_scores.items():
        candidate_grades[candidate] = grade(score)

    return candidate_grades

exam_max_score = 50
exam_scores = {
    'Alice': 42,
    'Bob': 35,
    'Mallory': 49,
}
exam_grades = grade_exams(exam_scores, exam_max_score)
print(exam_grades) # {'Alice': 5.2, 'Bob': 4.5, 'Mallory': 5.9}
```

## Returning Inner Functions

Notice how each `score` is passed to `grade`, but `max_score` is taken from the
outer scope. The inner function even has access to the outer function's scope if
it is returned from the outer function and used elsewhere. The outer function
_encloses_ the inner function; this construct therefore is called a _closure_:

```python
def get_compute_salary_func(year):

    bonus_rates = {
        2018: 0.05,
        2019: 0.10,
        2020: 0.07,
    }
    bonus_rate = bonus_rates.get(year, 0.0)

    def compute_yearly_salary(monthly):
        base_salary = monthly * 12
        bonus = base_salary * bonus_rate
        return base_salary + bonus

    return compute_yearly_salary

compute_2016_salaries = get_compute_salary_func(2016)
compute_2018_salaries = get_compute_salary_func(2018)
compute_2020_salaries = get_compute_salary_func(2020)

print(compute_2016_salaries(80000))  #  960000.0
print(compute_2018_salaries(80000))  # 1008000.0
print(compute_2020_salaries(80000))  # 1027200.0
```

In the example above, the outer function `get_compute_salary_func` encloses the
inner function `compute_yearly_salary`; the latter using the variable
`bonus_rate` established in the former's scope. Even though the same function is
used multiple times, it computes different results, because the enclosing scope
is different.

## Map

A dictionary is a Python data structure that describes the relationship between
a key and a value in a static way. The `map` function can be seen as the dynamic
counterpart of a `dict`. It is a higher-order function that processes a
collection of items using a given function, and returns a collection consisting
of the function's return value for each item:

```python
max_score = 50
exam_scores = [42, 35, 49]

def grade(score):
    # Swiss grades: 1..6
    return score / max_score * 5 + 1

exam_grades = map(grade, exam_scores)
print(list(exam_grades))  # [5.2, 4.5, 5.9]
```

No explicit looping over the individual scores is needed, the `map` function
handles those details. The code can be further simplified by using a lambda
instead of a named function:

```python
max_score = 50
exam_scores = [42, 35, 49]
exam_grades = map(lambda score: score / max_score * 5 + 1, exam_scores)
print(list(exam_grades))
```

This works with any kind of functions, i.e. also with a closure _primed_ with
a value, like in the salary example from before:

```python
def get_compute_salary_func(year):

    bonus_rates = {
        2018: 0.05,
        2019: 0.10,
        2020: 0.07,
    }
    bonus_rate = bonus_rates.get(year, 0.0)

    def compute_yearly_salary(monthly):
        base_salary = monthly * 12
        bonus = base_salary * bonus_rate
        return base_salary + bonus

    return compute_yearly_salary

compute_2020_salaries = get_compute_salary_func(2020)
base_salaries = [80000, 90000, 100000]
total_salaries = map(compute_2020_salaries, base_salaries)
print(list(total_salaries))
```

## Composing Functions

Consider the value `x` that has to be processed by two functions `f` and `g`:

1. `y` is computed as `y=g(x)` (intermediate result)
2. `z` is computed as `z=f(y)` (final result)

This, of course, can be simplified by _composing_ the two functions `f` and `g`
as `f(g(x))`.

Consider this example, where exam scores are first mapped to exam grades, which
then are rounded in a second step:

```python
def get_grade_for_func(max_score):

    def grade(score):
        return score / max_score * 5 + 1

    return grade

def get_round_to_func(granularity):

    def round_to(value):
        scaled_up = value * (1 / granularity)
        rounded = round(scaled_up)
        scaled_down = rounded * granularity
        return scaled_down
    
    return round_to

max_score = 72
scores = [46, 70, 53, 38, 67]

grade_for = get_grade_for_func(max_score)
round_to = get_round_to_func(0.1)

exact_grades = map(grade_for, scores)
rounded_grades = map(round_to, exact_grades)

print(list(rounded_grades))  # [4.2, 5.9, 4.7, 3.6, 5.7]
```

This approach requires two calls to `map`, with each call iterating over all the
elements. If the grader and rounding function are composed to a single function,
the list only needs to be processed once:

```python
def get_grade_for_func(max_score):

    def grade(score):
        return score / max_score * 5 + 1

    return grade

def get_round_to_func(granularity):

    def round_to(value):
        scaled_up = value * (1 / granularity)
        rounded = round(scaled_up)
        scaled_down = rounded * granularity
        return scaled_down
    
    return round_to

max_score = 72
scores = [46, 70, 53, 38, 67]

grade_for = get_grade_for_func(max_score)
round_to = get_round_to_func(0.1)

def compose(f, g):

    def func(x):
        return f(g(x))

    return func

score_to_rounded_grade = compose(round_to, grade_for)

rounded_grades = map(score_to_rounded_grade, scores)

print(list(rounded_grades))  # [4.2, 5.9, 4.7, 3.6, 5.7]
```

This approach scales much better: Not only in terms of runtime efficiency, which
becomes noticable as the number of elements grows, but only if additional
computations need to be done for every item.

In this example, an additional point bonus is added to each score, so that the
maximum grade can be reached without a perfect score:

```python
def get_bonus_of_func(bonus):

    def add(score):
        return score + bonus

    return add

def get_grade_for_func(max_score):

    def grade(score):
        return score / max_score * 5 + 1

    return grade

def get_round_to_func(granularity):

    def round_to(value):
        scaled_up = value * (1 / granularity)
        rounded = round(scaled_up)
        scaled_down = rounded * granularity
        return scaled_down
    
    return round_to

max_score = 100
scores = [70, 80, 90, 40, 100]

bonus_of = get_bonus_of_func(max_score / 10)
grade_for = get_grade_for_func(max_score)
round_to = get_round_to_func(0.1)

def compose(f, g):

    def func(x):
        return f(g(x))

    return func

score_to_exact_grade = compose(grade_for, bonus_of)
score_to_rounded_grade = compose(round_to, score_to_exact_grade)

rounded_grades = map(score_to_rounded_grade, scores)

print(list(rounded_grades))  # [5.0, 5.5, 6.0, 3.5, 6.5]
```

Unfortunately, this brings up another issues: Grades higher than the maximum
grade of 6.0 are computed. However, this issues can be solved by composing even
further:

```python
def get_bonus_of_func(bonus):

    def add(score):
        return score + bonus

    return add

def get_grade_for_func(max_score):

    def grade(score):
        return score / max_score * 5 + 1

    return grade

def get_limit_of_func(max_grade):

    def limit(grade):
        return min(grade, max_grade)

    return limit


def get_round_to_func(granularity):

    def round_to(value):
        scaled_up = value * (1 / granularity)
        rounded = round(scaled_up)
        scaled_down = rounded * granularity
        return scaled_down
    
    return round_to

max_score = 100
scores = [70, 80, 90, 40, 100]

bonus_of = get_bonus_of_func(max_score / 10)
grade_for = get_grade_for_func(max_score)
limit_of = get_limit_of_func(6.0)
round_to = get_round_to_func(0.1)

def compose(f, g):

    def func(x):
        return f(g(x))

    return func

score_to_exact_grade = compose(grade_for, bonus_of)
score_to_bounded_grade = compose(limit_of, score_to_exact_grade)
score_to_rounded_grade = compose(round_to, score_to_bounded_grade)

rounded_grades = map(score_to_rounded_grade, scores)

print(list(rounded_grades))  # [5.0, 5.5, 6.0, 3.5, 6.0]
```

Compare this to a procedural approach, which is much shorter in terms of lines:

```python
max_score = 100
scores = [70, 80, 90, 40, 100]
bonus = max_score / 10
max_grade = 6.0
granularity = 0.1

grades = []
for score in scores:
    score = score + bonus
    grade = score / max_score * 5 + 1
    if grade &gt; max_grade:
        grade = max_grade
    grade = round(grade * 1 / granularity) * granularity
    grades.append(grade)

print(grades)  # [5.0, 5.5, 6.0, 3.5, 6.0]
```

However, this code is harder to reason about (&quot;Where did the error happen?&quot;),
especially if the computations are getting more involved. The functional
approach allows you to reason about and write tests for each function in
isolation. If the functions work correctly, are composed in the right way and
used with well-tested higher-order functions like `map` , the result will be
correct, too.

## Closures vs. Classes

Like objects, closures can hold state. In OOP, the state can be initialized
using a constructor. A method of the same class then can perform computations
based on both internal state and parameters:

```python
class Rounder:

    def __init__(self, granularity):
        self._granularity = granularity

    def round(self, value):
        scaled_up = value * (1 / self._granularity)
        rounded = round(scaled_up)
        scaled_down = rounded * self._granularity
        return scaled_down

grades = [5.234, 4.738, 3.269]
rounder = Rounder(0.05)
rounded = map(rounder.round, grades)
print(list(rounded))  # [5.25, 4.75, 3.25]
```

Python has a special method `__call__`, which allows objects to be used like
functions. The above implementation can be turned more pythonesque by, first,
renaming `round` to `__call__`, and, second, by using `rounder` as a function
(instead of its method `rounder.round`). Calls to `rounder()` will be delegated
to the `__call__` method:

```python
class Rounder:

    def __init__(self, granularity):
        self._granularity = granularity

    def __call__(self, value):
        scaled_up = value * (1 / self._granularity)
        rounded = round(scaled_up)
        scaled_down = rounded * self._granularity
        return scaled_down

grades = [5.234, 4.738, 3.269]
rounder = Rounder(0.05)
rounded = map(rounder, grades)
print(list(rounded))  # [5.25, 4.75, 3.25]
```

This approach is useful when objects first need to be configured in a
complicated but inconsistent manner. Think of the Builder pattern, that allows
to initialize objects only using a subset of available parameters:

```python
class Salary:

    _bonus = 0
    _taxes = 0
    _penalty = 0

    def __init__(self, amount):
        self._salary = amount

    def with_bonus(self, rate):
        self._bonus = rate
        return self

    def with_taxes(self, rate):
        self._taxes = rate
        return self

    def with_penalty(self, penalty):
        self._penalty = penalty
        return self

    def __call__(self):
        pre_bonus = (self._salary - self._penalty)
        pre_taxes = pre_bonus + pre_bonus * self._bonus
        return pre_taxes - pre_taxes * self._taxes

salary_1 = Salary(100000).with_bonus(0.1).with_penalty(5000)
salary_2 = Salary(100000).with_bonus(0.1).with_taxes(0.2)
print(salary_1())  # 104500.0
print(salary_2())  #  88000.0
```

A function returning a closure requires optional parameters for the same
purpose:

```python
def get_salary_func(bonus=0, taxes=0, penalty=0):

    def compute(salary):
        pre_bonus = (salary - penalty)
        pre_taxes = pre_bonus + pre_bonus * bonus
        return pre_taxes - pre_taxes * taxes

    return compute

salary_1 = get_salary_func(bonus=0.1, penalty=5000)
salary_2 = get_salary_func(bonus=0.1, taxes=0.2)
print(salary_1(100000))  # 104500.0
print(salary_2(100000))  #  88000.0
```

## Inspecting Closures

Python provides the special attributes `__closure__` and `__code__` to inspect
closures (see the [Data
Model](https://docs.python.org/3/reference/datamodel.html) for details).

The variables a function has access to by enclosing an outer scope—so-called
_free variables_—can be retrieved as a tuple using the `co_freewars` attribute
of the `__code__` attribute. To get the values of those free variables, inspect
the `cell_contents` attribute of each element of the `__closure__` attribute:

```python
def get_salary_func(bonus=0, taxes=0, penalty=0):

    def compute(salary):
        pre_bonus = (salary - penalty)
        pre_taxes = pre_bonus + pre_bonus * bonus
        return pre_taxes - pre_taxes * taxes

    return compute

salary = get_salary_func(bonus=0.1, penalty=5000)
print(salary.__code__.co_freevars)  # ('bonus', 'penalty', 'taxes')
print(salary.__closure__[0].cell_contents)  # 0.1
print(salary.__closure__[1].cell_contents)  # 5000
print(salary.__closure__[2].cell_contents)  # 0
```

This process can be simplified using an utility function:

```python
def get_salary_func(bonus=0, taxes=0, penalty=0):

    def compute(salary):
        pre_bonus = (salary - penalty)
        pre_taxes = pre_bonus + pre_bonus * bonus
        return pre_taxes - pre_taxes * taxes

    return compute

salary = get_salary_func(bonus=0.1, penalty=5000)

def inspect_closure(func):
    for i, name in enumerate(func.__code__.co_freevars):
        print(f'{name} = {func.__closure__[i].cell_contents}')

inspect_closure(salary)
```

Which outputs all the free variables of a closure:

    bonus = 0.1
    penalty = 5000
    taxes = 0

Notice that those are read-only values, don't attempt to manipulate those
closures: better create a new one.

# Iterators

An _iterator_ can be used to process the elements of a sequence one by one. When
passed to the `next()` function, the next element of the iterator's underlying
sequence is returned—or `StopIteration` thrown, in case the iterator is
_exhausted_, i.e. all of its elements have been processed.

Higher-order functions like `filter` or `map` return iterators:

```python
numbers = [1, 2, 3, 4, 5]
even = filter(lambda x: x % 2 == 0, numbers)
odd = map(lambda x: x + 1, even)
print(next(odd))  # 3
print(next(odd))  # 5
print(next(odd))  # StopIteration
```

An iterator can onle be processed once in forward direction. However, multiple
iterators can be used to process the same underlying sequence.

## Iterables

An _iterable_ is something (usually a sequence like list, tuple, string) that
can be turned into an iterator by passing it to the `iter()` function, which
returns a new iterator:

```python
numbers = [1, 2, 3]
i = iter(numbers)
print(next(i))  # 1
print(next(i))  # 2
print(next(i))  # 3
```

An iterator itself is also an iterable, so calls to `iter()` passing an iterator
return the same iterator with its current state:

```python
numbers = [1, 2, 3]
i = iter(numbers)
print(next(i))  # 1
j = iter(i)
print(next(j))  # 2
print(next(i))  # 3
```

## Loops use Iterators

Internally, Python relies heavily on iterators. A `for`/`in` loop works on any
iterable. First, `iter()` is called on the loop's iterable to get an iterator.
Then, for every iteration, `next()` is called on the iterator to get to the next
elements. Finally, the loop ends when `StopIteration` is raised.

Consider this `for`/`in` loop:

```python
numbers = [1, 2, 3]

for x in numbers:
    print(x)
```

Which could be re-written using explicit `iter()` and `next()` calls and a
`while` loop:

```python
numbers = [1, 2, 3]

i = iter(numbers)
while True:
    try:
        x = next(i)
        print(x)
    except StopIteration:
        break
```

## Lazy Evaluation

Iterators only must produce their values when requested using the `next()`
function, which means that they can use _lazy evaluation_. If an iteration is
stopped before the iterator has been exhausted, no remaining items have been
computed in vain. This can save computing power and memory, but potentially
increases the processing time needed for a single iteration. (Picking between
lazy and eager evaluation is a trade-off.) Iterators implemented using lazy
evaluation can be of infinite length.

The built-in `range()` function produces lazy sequences. However, the sequence's
length can be figured out using the built-in `len()` function considering the
limit arguments given to `range()`: `len(range(1, 5))` is `5 - 1 = 4`.

## Realizing Iterators

An iterator must be _realized_ before all of its items can be dealt with at
once, i.e. by printing out the whole sequence of items. For this purpose, the
according functions can be called:

```python
numbers = range(1, 4)

print(list(iter(numbers)))  # [1, 2, 3]
print(set(iter(numbers)))   # {1, 2, 3}
print(tuple(iter(numbers))) # (1, 2, 3)
```

Alternatively, the expansion operator `*` can be used with according literals:

```python
numbers = range(1, 4)

print([*iter(numbers)])  # [1, 2, 3]
print({*iter(numbers)})  # {1, 2, 3}
print((*iter(numbers),)) # (1, 2, 3)
```

Notice the trailing comma required for tuple expansion in the last example.

When working with strings, `str` will call the iterator's implementation of
the `__str__()` dunder method, which describes the iterator itself rather than
its items. Use the `join()` method on an empty string to realize a list of
characters:

```python
offsets = range(0, 26)
capital_a = 65
alphabet = map(lambda c: chr(c + capital_a), offsets)

print(str(alphabet))     # &lt;map object at 0x7fa5d875b4f0&gt;
print(''.join(alphabet)) # ABCDEFGHIJKLMNOPQRSTUVWXYZ
```

## Implementing an Iterator

An iterator can be implemented by providing two dunder methods: `__next__()` and
`__iter__()`. Calls of the built-in functions `next()` and `iter()` will be
forwarded to the argument's respective dunder methods.

The class `Factorials` implements an iterator that provides the successive
factorial numbers up to a limit passed to the constructor. The implementation
uses lazy evaluation:

```python
class Factorials():

    def __init__(self, n):
        if n &lt; 0:
            raise ValueError('n! is only defined for n &gt;= 0')
        self.n = n
        self.i = 0
        self.x = 1

    def __iter__(self):
        return self

    def __next__(self):
        if self.i &lt; self.n:
            self.i += 1
            self.x *= self.i
            return self.x
        else:
            raise StopIteration


print(list(Factorials(3))) # [1, 2, 6]
print(list(Factorials(5))) # [1, 2, 6, 24, 120]
print(list(Factorials(8))) # [1, 2, 6, 24, 120, 720, 5040, 40320]
```

In practice, _generators_ are often a better fit for such tasks.

# Transforming Iterables

Python provides functions to transform iterables, which are less prone to
side effects, and therefore the better fit than lists from a functional
perspective.

## Enumerating

The built-in `enumerate()` function transforms a sequence into an iterator of
tuples, each containing an index value and an item from the original sequence:

```python
names = ['Alice', 'Bob', 'Mallory']
for item in enumerate(names):
    print(item)
```

    (0, 'Alice')
    (1, 'Bob')
    (2, 'Mallory')

An optional start index can be provided, and the tuple can be unpacked using two
variables for the loop:

```python
names = ['Alice', 'Bob', 'Mallory']
for index, name in enumerate(names, 1):
    print(index, name)
```

    1 Alice
    2 Bob
    3 Mallory

## Zipping and Unzipping

Multiple sequences can be processed together using the built-in `zip()` function:

```python
names = ['Dilbert', 'Dogbert', 'Ashok']
jobs = ['Engineer', 'Consultant', 'Intern']
salaries = [120000, 250000, 18000]

for employee in zip(names, jobs, salaries):
    print(employee)
```

    ('Dilbert', 'Engineer', 120000)
    ('Dogbert', 'Consultant', 250000)
    ('Ashok', 'Intern', 18000)

Again, the tuple can be unpacked by using multiple variables for the loop:

```python
names = ['Dilbert', 'Dogbert', 'Ashok']
jobs = ['Engineer', 'Consultant', 'Intern']
salaries = [120000, 250000, 18000]

for name, job, salary in zip(names, jobs, salaries):
    print(name, job, salary)
```

    Dilbert Engineer 120000
    Dogbert Consultant 250000
    Ashok Intern 18000

Notice that `zip()` stops when the shortest sequence is exhausted:

```python
names = ['Dilbert', 'Dogbert', 'Ashok']
jobs = ['Engineer']
salaries = [120000, 250000]

for employee in zip(names, jobs, salaries):
    print(employee)
```

    ('Dilbert', 'Engineer', 120000)

If the original sequences (`names`, `jobs`, `salaries`) are considered columns
of an employee database, the results of the `zip()` operation can be seen as its
rows. This transformation can be reversed using `zip()`—by first unpacking the
resulting sequence, and then zipping it:

```python
names = ['Dilbert', 'Dogbert', 'Ashok']
jobs = ['Engineer', 'Consultant', 'Intern']
salaries = [120000, 250000, 18000]

employees = zip(names, jobs, salaries)
for col in zip(*employees):
    print(col)
```

    ('Dilbert', 'Dogbert', 'Ashok')
    ('Engineer', 'Consultant', 'Intern')
    (120000, 250000, 18000)

## Sorting and Reversing

Unlike the list's `sort()` method that sorts a list in-place, the built-in
`sorted()` function returns a sorted new list. Either operation allows for an
optional `key` argument, which defines the sorting criterion in terms of a
function applied to every item:

```python
names = ['Dilbert', 'Dogbert', 'Ashok']
jobs = ['Engineer', 'Consultant', 'Intern']
salaries = [120000, 250000, 18000]
employees = zip(names, jobs, salaries)

for employee in sorted(employees, key=lambda e: e[2]):
    print(employee)
```

    ('Ashok', 'Intern', 18000)
    ('Dilbert', 'Engineer', 120000)
    ('Dogbert', 'Consultant', 250000)

The lambda accessing the tuple element at index 2 can also be taken from the
`operator` module, which provides an `itemgetter` function that produces a
closure to access the right element:

```python
from operator import itemgetter

names = ['Dilbert', 'Dogbert', 'Ashok']
jobs = ['Engineer', 'Consultant', 'Intern']
salaries = [120000, 250000, 18000]
employees = zip(names, jobs, salaries)

for employee in sorted(employees, key=itemgetter(2)):
    print(employee)
```

    ('Ashok', 'Intern', 18000)
    ('Dilbert', 'Engineer', 120000)
    ('Dogbert', 'Consultant', 250000)

When dealing with classes instead of tuple, use the `attrgetter` function to
access attributes by name. The `methodcaller` function allows to call any method
on each item by its name:

```python
from operator import methodcaller

names = ['POINTY HAIRED BOSS', 'Dilbert', 'dogbert', 'alice']
for name in sorted(names, key=methodcaller('lower')):
    print(name)
```

Here, the `lower()` method is called on every name in order to sort the names in
a case-insensitive manner.

The sort order can be reversed either by setting the optional `reverse` argument
of the `sorted()` function to `True`, or by calling the `reversed()` built-in
function:

```python
names = ['Dilbert', 'Alice', 'Pointy Haired Boss', 'Dogbert', 'Ted']

names_desc = sorted(names, reverse=True)
print(names_desc)

names_desc = reversed(sorted(names))
print(list(names_desc))
```

    ['Ted', 'Pointy Haired Boss', 'Dogbert', 'Dilbert', 'Alice']
    ['Ted', 'Pointy Haired Boss', 'Dogbert', 'Dilbert', 'Alice']

Notice that the return value of `reversed()` needs to be realized first.

| Function     | accepts  | returns  |
|--------------|----------|----------|
| `sorted()`   | iterable | list     |
| `reversed()` | sequence | iterator |

The sorting operations are _stable_, so sorting multiple times will always
return the same order of items that share the same sorting criterion, but differ
otherwise:

```python
# Swiss-German date format
dates = [
    '24.06.1987',
    '13.05.1987',
    '31.12.1988',
    '31.07.1987',
    '17.09.1988',
    '05.02.1987',
    '01.03.1988',
]

by_year_1 = sorted(dates, key=lambda d: d[6:])
by_year_2 = sorted(by_year_1, key=lambda d: d[6:])
by_year_3 = sorted(by_year_2, key=lambda d: d[6:])
for date_1, date_2, date_3 in zip(by_year_1, by_year_2, by_year_3):
    print(date_1, '==', date_2, '==', date_3)
```

    24.06.1987 == 24.06.1987 == 24.06.1987
    13.05.1987 == 13.05.1987 == 13.05.1987
    31.07.1987 == 31.07.1987 == 31.07.1987
    05.02.1987 == 05.02.1987 == 05.02.1987
    31.12.1988 == 31.12.1988 == 31.12.1988
    17.09.1988 == 17.09.1988 == 17.09.1988
    01.03.1988 == 01.03.1988 == 01.03.1988

When counting backwards, the `reverse()` function can be used to create more
readable code:

```python
range_reverse = range(9, -1, -1)     # hard to read
reversed_range = reversed(range(10)) # easy to read

for a, b in zip(range_reverse, reversed_range):
    print(a, b)
```

    9 9
    8 8
    7 7
    6 6
    5 5
    4 4
    3 3
    2 2
    1 1
    0 0

## Pipelines

Adding `print()` calls to functions used with `filter()` and `map()` shows in
which order those functions are executed:

```python
def is_taxable(salary):
    print(f'is_taxable({salary})')
    return salary &gt; 100000

def calc_tax(salary):
    print(f'calc_tax({salary})')
    return salary * 0.05

salaries = [120000, 84000, 52000, 190000]
taxable = filter(is_taxable, salaries)
taxes = map(calc_tax, taxable)

for tax in taxes:
    print(tax)
```

    is_taxable(120000)
    calc_tax(120000)
    6000.0
    is_taxable(84000)
    is_taxable(52000)
    is_taxable(190000)
    calc_tax(190000)
    9500.0

Notice that those items are processed in a _pipeline_ one by one. Even though
the call to `map()` comes after the call to `filter()`, the `is_taxable()`
operation used by `filter()` has only been processed for the first element yet!

Removing the `taxable` intermediary variable and calling `map()` directly on the
result of `filter()` therefore won't have any impact on the order of processing:

```python
def is_taxable(salary):
    print(f'is_taxable({salary})')
    return salary &gt; 100000

def calc_tax(salary):
    print(f'calc_tax({salary})')
    return salary * 0.05

salaries = [120000, 84000, 52000, 190000]
taxes = map(calc_tax, filter(is_taxable, salaries))

for tax in taxes:
    print(tax)
```

    is_taxable(120000)
    calc_tax(120000)
    6000.0
    is_taxable(84000)
    is_taxable(52000)
    is_taxable(190000)
    calc_tax(190000)
    9500.0

But leave the loop at the bottom away, and _no items will be processed at all_:

```python
def is_taxable(salary):
    print(f'is_taxable({salary})')
    return salary &gt; 100000

def calc_tax(salary):
    print(f'calc_tax({salary})')
    return salary * 0.05

salaries = [120000, 84000, 52000, 190000]
taxes = map(calc_tax, filter(is_taxable, salaries))
print(taxes)
```

    &lt;map object at 0x7fd3bbf03fd0&gt;

This demonstrates that `filter()`, `map()` an the like use _lazy evaluation_.

## Multiple Map Parameters

The `map()` function can be used on multiple sequences in one go—if used with a
function that expects the same number of arguments as sequences are used:

```python
numbers = [7, 4, 3, 2]
factors = [1.0, 1.5, 0.5, 2.0]

results = map(lambda n, f: n * f, numbers, factors)
print(list(results)) # [7.0, 6.0, 1.5, 4.0]
```

Again, instead of defining a lambda, an `operator` can be used:

```python
from operator import mul

numbers = [7, 4, 3, 2]
factors = [1.0, 1.5, 0.5, 2.0]

results = map(mul, numbers, factors)
print(list(results)) # [7.0, 6.0, 1.5, 4.0]
```

Any number of sequences can be passed to `map()`, as long as the operation
performed on them accepts the same number of parameters:

```python
def f(a, b, x):
    y = a * x + b
    return y

slopes = [1, 2, 3, 4]
coefficients = [1, 0, 2, 0]
xs = [1.5, 3.0, 2.5, 0.0]

results = map(f, slopes, coefficients, xs)
print(list(results)) # [2.5, 6.0, 9.5, 0.0]
```

# Reducing Iterables

A _reducing_ function combines all the values of an iterable and produces a
single value out of them as a result.

## Built-in Reducing Functions

the `len()` function is one of the most common reducing functions. It returns
the number of elements contained in an sequence:

```python
print(len([7, 3, 5, 2])) # 4
print(len([]))           # 0
```

The `sum()` function adds up the items of an iterable and returns their sum:

```python
print(sum([7, 3, 5, 2])) # 17
print(sum([]))           #  0
```

An optional start value can be provided for the second argument if the summation
should start from a different value than 0 (default):

```python
print(sum([7, 3, 5, 2], -10)) # 7
print(sum([], 0))             # 0
```

Even though the `sum()` function applies the `+` operator to the elements of the
given iterable, it cannot be used to concatenate strings. Use the string's
`join()` method instead:

```python
letters = ['abc', 'de', 'f', 'ghi']
print(sum(letters, '')) # wrong: TypeError
print(''.join(letters)) # right: abcdefghi
```

The `min()` and `max()` function return the smallest or biggest element of an
iterable, respectively:

```python
numbers = [7, 3, 1, 9, 5]
print(min(numbers)) # 1
print(max(numbers)) # 9
```

If the elements are list themselves, those sub-lists are compared element-wise:

```python
numbers = [[3, 1, 2], [9, 1, 3], [1, 9, 8]]
print(min(numbers)) # [1, 9, 8]
print(max(numbers)) # [9, 1, 3]
```

Calling `min()` or `max()` on an empty iterable causes a `ValueError`, which can
be prevented by setting a `default` argument, which is used as a fallback, and
ignored for non-empty iterables:

```python
numbers = [9, 1, 5]
nothing = []

print(min(numbers))            # 1
print(min(numbers, default=0)) # 1
print(min(nothing))            # ValueError
print(min(nothing, default=0)) # 0

print(max(numbers))            # 9
print(max(numbers, default=0)) # 9
print(max(nothing))            # ValueError
print(max(nothing, default=0)) # 0
```

The optional `key` argument can be used to speficy the criterion being used for
comparison—like for the `sorted()` function or `sort()` method:

```python
employees = [
    ('Dilbert', 42, 120000),
    ('Alice', 39, 110000),
    ('Wally', 53, 130000),
    ('Ashok', 23, 36000),
]

youngest = min(employees, key=lambda e: e[1])
oldest = max(employees, key=lambda e: e[1])

lowest_salary = min(employees, key=lambda e: e[2])
highest_salary = max(employees, key=lambda e: e[2])

print(f'age: {youngest} (youngest), {oldest} (oldest)')
print(f'earns: {lowest_salary} (least), {highest_salary} (most)')
```

    age: ('Ashok', 23, 36000) (youngest), ('Wally', 53, 130000) (oldest)
    earns: ('Ashok', 23, 36000) (least), ('Wally', 53, 130000) (most)

The `any()` function returns `True` if _at least one element_ of the given
iterable evaluates to `True`:

```python
print(any([False, False, False])) # False
print(any([False, False, True]))  # True
print(any([0, 0, 0, 0, 0]))       # False
print(any([0, 0, 0, 1, 0]))       # True
print(any(['', '', '', '']))      # False
print(any(['', 'x', '', 'y']))    # True
print(any([False, '', 0, []]))    # False
print(any([]))                    # False
```

For an empty iterable (last example), `any()` returns False—unlike the `all()`
function, which returns `True` if _all elements_ evaluate to `True`, and
`False`, if an element evaluates to `False`:

```python
print(all([True, False, True]))  # False
print(all([True, True, True]))   # True
print(all([2, 8, 0, 3, 8]))      # False
print(all([2, 8, 4, 3, 8]))      # True
print(all(['a', 'b', '', 'd']))  # False
print(all(['u', 'v', 'x', 'y'])) # True
print(all([True, 'a', 1, []]))   # False
print(all([]))                   # True
```

## The `reduce()` Function

The `functools` module provides a `reduce()` function, which allows for custom
definitions of reducing operations. Its first argument is a function accepting
_two_ parameters (the elements `n-1` and `n` to be combined), and its second
argument is the iterable to be reduced. This example implements factorials using
the `operator` module's `mul()` and the `functool` module's `reduce()` function:

```python
from functools import reduce
from operator import mul

def factorial(x):
    numbers = range(1, x+1)
    return reduce(mul, numbers)

print(factorial(4)) #  24
print(factorial(5)) # 120
print(factorial(6)) # 720
```

As an optional third argument, an `initializer` can be provided:

```python
from functools import reduce
from operator import mul

numbers = range(1, 6)
half_the_fac = reduce(mul, numbers, 0.5)
print(half_the_fac) # 60.0
```

This is especially useful when dealing with empty iterables, which result in a
`TypeError` when reduced without an initializer, which serves as a fallback
value:

```python
from functools import reduce
from operator import mul

print(reduce(mul, []))      # TypeError
print(reduce(mul, [], 0.5)) # 0.5
```

## The `filter()`, `map()`, `reduce()` Pattern

Even though they work completely different, the functions `filter()`, `map()`,
and `reduce()` have some pair-wise commonalities:

- Both `filter()` and `map()` process the elements of an iterable one by one.
- Both `map()` and `reduce()` transform values.
- Both `filter()` and `reduce()` decrease the number of elements.

Those three functions are often used together to process iterables, resulting in
a single value. Consider the following list containing employees, their hourly
rates, and the amount of hours worked by each for a project:

```python
efforts = [
    # (name, rate, hours)
    ('Dilbert', 220, 13.5),
    ('Alice', 180, 16.0),
    ('Wally', 150, 0.0),
    ('Ashok', 80, 42.5),
    ('Dogbert', 250, 3.5),
    ('Pointy Haired Boss', 500, 0.0),
]
```

In order to produce the total labor costs of the project, this list of tuples
can be processed in three steps:

1. _filter_: Only entries with actual working hours (&gt; 0.0) are retained.
2. _map_: Compute the cost for each employee (rate multiplied by hours).
3. _reduce_: Sum up all the individual costs of each employee.

```python
from functools import reduce
from operator import add

efforts = [
    # (name, rate, hours)
    ('Dilbert', 220, 13.5),
    # ...
]

involved = filter(lambda e: e[2] &gt; 0.0, efforts)
cost_per_employee = map(lambda e: e[1] * e[2], involved)
total_costs = reduce(add, cost_per_employee)
print(f'total costs: {total_costs}') # 10125.0
```

In this particular example, the `filter` step is redundant, because employees
with zero hours would not affect the total cost at all. The `reduce` step could
also be simplified using the `sum()` function:

```python
efforts = [
    # (name, rate, hours)
    ('Dilbert', 220, 13.5),
    # ...
]

cost_per_employee = map(lambda e: e[1] * e[2], efforts)
total_costs = sum(cost_per_employee)
print(f'total costs: {total_costs}') # 10125.0
```

Consider another example: a list of exam submissions consisting of the name, the
submission date, and the score achieved:

```python
submissions = [
    # name, submission date, score
    ('Alice', '2021-07-03', 73),
    ('Bob', '2021-07-18', 81),
    ('Charles', '2021-07-12', 57),
    ('Deborah', '2021-07-10', 96),
    ('Ernest', '2021-07-19', 89),
    ('Fanny', '2021-07-06', 61),
]
```

The average grade of submissions within deadline should be computed as follows:

1. _filter_: Submissions after the deadline (`2021-07-10`) are ignored.
2. _map_: A grade from 1 (worst) to 6 (best) is computed based on a maximum
   score of 100.
3. _reduce_: The grade average of all submissions is calculated.

```python
from datetime import datetime

submissions = [
    # name, submission date, score
    ('Alice', '2021-07-03', 73),
    # ...
]

max_score = 100

def is_within_deadline(submission):
    deadline = datetime.fromisoformat('2021-07-10')
    submitted = datetime.fromisoformat(submission[1])
    return submitted &lt; deadline

def swiss_grade(score, max_score):
    return score / max_score * 5 + 1

within_deadline = filter(is_within_deadline, submissions)
grades = map(lambda s: swiss_grade(s[2], max_score), within_deadline)
grades = list(grades)
average = sum(grades) / len(grades)
print(f'average: {average}') # 4.35
```

# Comprehensions

Creating an iterable based on another iterable, say, building the squares of a
list of numbers, can be done in various ways.

The structured approach uses a `for` loop:

```python
numbers = range(1, 10)

squares = []
for number in numbers:
    squares.append(number ** 2)

print(squares) # [1, 4, 9, 16, 25, 36, 49, 64, 81]
```

This approach is perfectly valid, but requires _operational reasoning_ to
understand.

A more declarative approach uses the higher-order `map` function, which requires
less code to be written:

```python
numbers = range(1, 10)

squares = list(map(lambda x: x ** 2, numbers))

print(squares) # [1, 4, 9, 16, 25, 36, 49, 64, 81]
```

However, the best tool for this purpose—building a list based on an iterable—is
a list comprehension:

```python
numbers = range(1, 10)

squares = [x ** 2 for x in numbers]

print(squares) # [1, 4, 9, 16, 25, 36, 49, 64, 81]
```

No lambda expression is required, the expression can be stated directly.

The comprehension has the following structure:

    [{expression} for {item} in {iterable}]

The above example can be read in English as:

&gt; make a list of `x ** 2` for all values of `x` in `numbers`

## Conditions

The higher-order functinos `filter` and `map` are often used toghether: first,
the items to be processed are fitered, second, the remaining items are mapped.

Consider this example turning a list of empty and non-empty strings into
title-cased strings, ignoring the empty ones:

```python
strings = ['', '', 'john', '', 'alice', '', 'bob']
non_empty = filter(len, strings)
names = list(map(lambda s: s.title(), non_empty))
print(names) # ['John', 'Alice', 'Bob']
```

A comprehension has an optional `if` statements; only items passing this test
end up in the resulting sequence:

```python
strings = ['', '', 'john', '', 'alice', '', 'bob']
names = [s.title() for s in strings if s]
print(names) # ['John', 'Alice', 'Bob']
```

This code is shorter and clearer. Consider a comprehension as an alternative of
combining `filter` and `map`.

## Nesting

Comprehensions can be nested, which can be used to create multi-dimensional
lists:

```python
def field_2d(rows, cols):
    return [[(x, y) for x in range(cols)] for y in range(rows)]

field = field_2d(6, 7)
for row in field:
    print(row)
```

    [(0, 0), (1, 0), (2, 0), (3, 0), (4, 0), (5, 0), (6, 0)]
    [(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 1), (6, 1)]
    [(0, 2), (1, 2), (2, 2), (3, 2), (4, 2), (5, 2), (6, 2)]
    [(0, 3), (1, 3), (2, 3), (3, 3), (4, 3), (5, 3), (6, 3)]
    [(0, 4), (1, 4), (2, 4), (3, 4), (4, 4), (5, 4), (6, 4)]
    [(0, 5), (1, 5), (2, 5), (3, 5), (4, 5), (5, 5), (6, 5)]

Again, this is much shorter than using the structured approach:

```python
def field_2d(rows, cols):
    field = []
    for y in range(rows):
        row = []
        for x in range(cols):
            row.append((x, y))
        field.append(row)
    return field
```

Notice that comprehensions can be nested without creating multi-dimensional
sequences as a result:

```python
coords = [x + y for x in range(0, 40, 10) for y in range(4)]
print(coords)
```

    [0, 1, 2, 3, 10, 11, 12, 13, 20, 21, 22, 23, 30, 31, 32, 33]

This translates to structured code as follows:

```python
coords = []
for x in range(0, 40, 10):
    for y in range(4):
        coords.append(x + y)
```

In the comprehension expression, the inner loop is on the right, the outer loop
on the left.

## Dictionaries, Sets, Tuples

Comprehensions can be used for the other sequence types—dictionaries, sets,
and tuples—too:

```python
squares = {x: x ** 2 for x in range(1, 6)}
print(squares) # {1: 1, 2: 4, 3: 9, 4: 16, 5: 25}

additions = [(3, 4), (4, 3), (5, 2), (3, 1), (4, 2)]
sums = {x + y for (x, y) in additions}
print(sums) # {4, 6, 7}

strings = ['', '', 'john', '', 'alice', '', 'bob']
names = tuple(s.title() for s in strings if s)
print(names) # ('John', 'Alice', 'Bob')
```

Notice that the last example creates a _generator object_, which must explicitly
be converted to a tuple.

# Generators

Unlike comprehensions, _generators_ use lazy evaluation. Unlike iterators,
generators do not require implementing a class implementing the `next()` and
`iter()` method (less boilerplate).

Generators are implemented using functions that yield a different result every
time they are called:

```python
def squares(n):
    for i in range(n):
        yield i ** 2

print(list(squares(6))) # [0, 1, 4, 9, 16, 25]
```

After a value is returned using the `yield` keyword, the generator function
stops its execution, but its state is remembered. The execution is continued for
the next iteration. This makes it possible for generator functions to hold a
state (without using an explicit closure):

```python
def factorials(n):
    current = 1
    for i in range(n):
        if i != 0:
            current *= i
        yield current

print(list(factorials(6))) # [1, 1, 2, 6, 24, 120]
```

As seen in the last example of the previous chapter, a generator can be created
as a comprehension using parentheses:

```python
square_gen = (x ** 2 for x in range(2, 6))
print(next(square_gen)) #  4
print(next(square_gen)) #  9
print(next(square_gen)) # 16
print(next(square_gen)) # 25
print(next(square_gen)) # StopIteration
```

Generators combine the advantages of comprehensions with lazy evaluation. If a
sequence is hard to express in terms of `filter` and `map`, and if the task is
memory-critical, consider a generator.

# Partial Application and Currying

_Partial application_ of functions and _currying_ are both ways to create new
functions based on existing functions. Those techniques are based on closures. (A
closure is an inner function returned from a surrounding function, with the
inner function having references to the surrounding function.)

## Partial Application

With partial application, only a subset of a function's parameters are set on
the first function call. The rest of the parameters are filled in a later call
to the partially applied function.

Consider the function `inc_x`, which requires a parameter `x`, and returns a
function that increases its parameter by `x`:

```python
def inc_x(x):
    def inc(y):
        return x + y
    return inc

numbers = [1, 2, 3]

inc_1 = inc_x(1)
inc_3 = inc_x(3)

print(list(map(inc_1, numbers))) # [2, 3, 4]
print(list(map(inc_3, numbers))) # [4, 5, 6]
```

Partial application is especially helpful if a function has a lot of parameters,
like a quadratic function:

    y = ax² + bx + c

Such a function is usually defined in terms of the parameters `a`, `b`, and
`c`—and applied multiple times using different values for `x`:

```python
def quad(a, b, c, x):
    return a*x**2 + b*x + c

def quad_abc(a, b, c):
    def f(x):
        return quad(a, b, c, x)
    return f

xs = range(5)

f = quad_abc(1, 2, 3)
g = quad_abc(2, 0, 1)

print(list(map(f, xs))) # [3, 6, 11, 18, 27]
print(list(map(g, xs))) # [1, 3, 9, 19, 33]
```

The `partial()` functions from the `functools` module provides a more flexible
approach that doesn't require defining closures for specific partial
applications. The code above can be simplified using `partial()`:

```python
from functools import partial

def quad(a, b, c, x):
    return a*x**2 + b*x + c

xs = range(5)

f = partial(quad, 1, 2, 3)
g = partial(quad, 2, 0, 1)

print(list(map(f, xs))) # [3, 6, 11, 18, 27]
print(list(map(g, xs))) # [1, 3, 9, 19, 33]
```

It is possible to apply a function partially multiple times, until every
parameter was filled in:

```python
from functools import partial

def quad(a, b, c, x):
    return a*x**2 + b*x + c

xs = range(5)

quad_a = partial(quad, 1)
quad_ab = partial(quad_a, 2)
quad_abc = partial(quad_ab, 3)

print(list(map(quad_abc, xs))) # [3, 6, 11, 18, 27]
```

However, using partial application, the parameters have to be filled in the
order as they are defined in the function. It's not possible to just define
the `quad` function's parameter `b` and `x`, and leave `a` and `c` undefined.

It is possible though to partially apply a function by setting keyword
arguments:

```python
from functools import partial

print_csv = partial(print, sep=',')
print_space = partial(print, sep=' ')

names = ['Dilbert', 'Alice', 'Wally']
print_csv(*names)   # Dilbert,Alice,Wally
print_space(*names) # Dilbert Alice Wally
```

## Currying

Python supports currying using third-party libraries such as PyMonad (version
2.4.0):

    $ pip install --user PyMonad==2.4.0

The `pymonad` module includes the `curry` decorator, which can be used to define
functions that can be partially applied without explicit use of
`functools.partial`. The number of arguments to be curried needs to be passed to
the `curry` decorator:

```python
from pymonad.tools import curry

@curry(4)
def quad(a, b, c, x):
    return a*x**2 + b*x + c

xs = range(5)

quad_abc = quad(1, 2, 3)
print(list(map(quad_abc, xs))) # [3, 6, 11, 18, 27]

quad_a = quad(1)
quad_ab = quad_a(2)
quad_abc = quad_ab(3)
print(list(map(quad_abc, xs))) # [3, 6, 11, 18, 27]
```

Notice that curried functions don't come for free; a lot of functions with
different argument lists are defined automatically in the background. Calling a
curried function is less explicit than using partial application. Make sure the
curried nature of a function is made clear to its users by the means of naming,
documentation, or convention (writing a module where functions are curried in
general).

## Advanced Composition

Functions with a single argument can be composed using a closure:

```python
def compose(f, g):
    def fn(x):
        return f(g(x))
    return fn

def increment(x):
    return x + 1

def twice(x):
    return x * 2

f = compose(twice, increment)

print(f(1)) # 4
print(f(2)) # 6
print(f(3)) # 8
```

Functions with multiple arguments can only be composed as above if partially
applied before to turn them into functions accepting a single argument:

```python
from functools import partial

def compose(f, g):
    def fn(x):
        return f(g(x))
    return fn

def add(x, y):
    return x + y

def mul(x, y):
    return x * y

increment = partial(add, 1)
twice  = partial(mul, 2)
f = compose(twice, increment)

print(f(1)) # 4
print(f(2)) # 6
print(f(3)) # 8
```

### Composing Multiple Functions

Consider the following set of functions `f()`, `g()`, `h()`, and `i()`, which
perform the following computations:

- `f(x)`: adds 1 to `x`
- `g(x)`: multiplies `x` by 2
- `h(x)`: computes `x` to the power of 3
- `i(x)`: subtracts 1 from `x`

Thus, `i(h(g(f(x)))) = (((x + 1) * 2) ^ 3) - 1`. Composing those functions one
by one is cumbersome:

```python
def compose(f, g):
    def fn(x):
        return f(g(x))
    return fn

def f(x):
    return x + 1

def g(x):
    return x * 2

def h(x):
    return x ** 3

def i(x):
    return x - 1

fn = compose(i, h)
fn = compose(fn, g)
fn = compose(fn, f)

print(fn(1)) # (((1 + 1) * 2) ^ 3) - 1 = 63
```

The composition can be generalized as a reducing operation. The `compose()`
function accepts a list of functions to be reduced by composing them pair-wise:

```python
from functools import reduce

def compose(*fns):
    def compose_pair(f, g):
        def fn(x):
            return f(g(x))
        return fn
    return reduce(compose_pair, fns)

def f(x):
    return x + 1

def g(x):
    return x * 2

def h(x):
    return x ** 3

def i(x):
    return x - 1

fn = compose(i, h, g, f)

print(fn(1)) # (((1 + 1) * 2) ^ 3) - 1 = 63
```

Unfortunately, this implementation doesn't work if _no_ functions are passed as
arguments. The `initializer` argument of `reduce()` can be used to define a
default value. A sensible default value, however, depends on the operation to be
performed. (For an addition or subtraction, the neutral element is 0, for a
multiplicationor a division, the neutral element is 1.) The _identity value_
provided by an _identity function_ (`lambda x: x`) is the right choice for all
cases:

```python
from functools import reduce

def compose(*fns):
    def compose_pair(f, g):
        def fn(x):
            return f(g(x))
        return fn
    return reduce(compose_pair, fns, lambda x: x)

fn = compose()
print(fn(37)) # 37
```

# Functors and Monads

Functors wrap a value and control how functions are applied to that wrapped
value. Such wrappers are useful when dealing with values that might be missing,
or add new capabilities to existing functions, such as making a function
that can only deal with scalar values capable of handling lists of scalar
values.

The `oslash` library provides Haskell-style _functors_, _applicatives_, and
_monads_:

    $ pip install oslash==0.6.3

- A **functor** wraps a value and controls how function is applied to that
  wrapped value using the `map()` method or the `%` operator.
- An **applicative** is a special kind of a functor that wraps a function, which
  can be called by the `apply()` method.
- A **monad** is a special kind of an applicative that also wraps the value
  returned from a function using its `bind()` method.

Those constructs are crucial in a pure functional programming language like
Haskell, where they are needed to deal with errors or side-effects. In Python,
those constructs are optional—hence available by third-party libraries such as
`oslash`—and can be left away in favour of procedural code.

## Functors

The `Just` functor, which technically is also an applicative and a monad (of
which more later), is a wrapper around a value:

```python
from oslash import Just

x = Just(3)
print(x) # Just 3
```

Functions cannot be called directly with an instance of `Just` as an argument.
Instead, the functor's `map()` method or the `%` operator can be used:

```python
from oslash import Just

def twice(x):
    return x * 2

x = Just(3)

y = twice(x) # illegal

y = x.map(twice) # correct
print(y) # Just 6

y = twice % x # correct, but shorter
print(y) # Just 6
```

Notice that the function stands at the left of the `%` operator, and the functor
to its right.

The `Nothing` functor does not wrap a value. It is the functional brother of
Python's `None` with well-defined behaviour—a function being applied to
`Nothing` always returns `Nothing` instead of throwing an exception:

```python
from oslash import Nothing

def twice(x):
    return x * 2

x = Nothing()
print(x) # Nothing

y = twice % x
print(y) # Nothing
```

The `List` functor wraps a list of values and makes it possible that a function
that only deals with scalar values can be applied to an entire list—a lot like
the higher-order `map()` function (notice that the `twice()` function has to be
wrapped by a `Just` functor, of which more in the next section):

```python
from oslash import Just, List

def twice(x):
    return x * 2

xs = List.from_iterable([1, 2, 4, 8]) # [1, 2, 4, 8]
print(xs)

f = Just(twice)

ys = f.apply(xs) # [2, 4, 8, 16]
print(ys)
```

## Applicatives

The `Just` functor is, in fact, also an _applicative functor_ that wraps a
function as a value, or short: an applicative, which provides an `apply()`
method:

```python
from oslash import Just

def twice(x):
    return x * 2

x = Just(3)
f = Just(twice)

b = f.apply(x)
print(b) # Just 6
```

Notice that both the value `3` and the function `twice()` have been wrapped by a
`Just` applicative.

An applicative wrapping a function with more than one parameter returns a
partially applied function if the `apply()` method is called on it. The
arguments can be filled in one by one:

```python
from oslash import Just

def quad(a, b, c, x):
    return (a * x ** 2) + b * x + c

f = Just(quad)
f_a = f.apply(Just(1))
f_ab = f_a.apply(Just(2))
f_abc = f_ab.apply(Just(3))

x = Just(4)

y = f_abc.apply(x)
print(y) # Just 27

y = Just(quad).apply(Just(1)).apply(Just(2)).apply(Just(3)).apply(x)
print(y) # Just 27
```

## Monads

An applicative that also wraps the return value resulting from a call to its 
wrapped function is called a monad. Its `bind()` method accepts a single
parameter—a function returning another monad:

```python
from oslash import Just, Nothing

def safe_reciprocal(x):
    if x == 0:
        return Nothing()
    return Just(1/x)

x = Just(4)
y = x.bind(safe_reciprocal)
print(y) # Just 0.25

x = Just(0)
y = x.bind(safe_reciprocal)
print(y) # Nothing
```

# Useful Libraries

Python's standard library offers a lot of capabilities that support a functional
programming style. The `functools` (treated above) and `itertools` (treated in
the following section) modules are especially useful for that purpose.

## The `itertools` Module

The `itertools` module provides useful functions to create iterators.

Infinite series of incrementing values can be created using the `count()`
function, which requires a `start` value and an optional `step` size:

```python
from itertools import count

to_infinite = count(0) # 0, 1, 2, 3, ...
to_infinite = count(0, 10) # 0, 10, 20, 30, ...

```

Infinite or finite repetitions of values can be created using the `repeat()`
function, which requires a value `x` to be repeated and an optional limit `n`:

```python
from itertools import repeat

infinite_ones = repeat(1) # 1, 1, 1, 1, ...
limited_ones = repeat(1, 3) # 1, 1, 1
```

Series of numbers can be repeated using the `cycle()` function that accepts an
iterator to be repeated:

```python
from itertools import cycle

one_two_three_ad_nauseam = cycle([1, 2, 3]) # 1, 2, 3, 1, 2, 3
```

Like `zip`, the `zip_longest` function zips together two iterables. Unlike
`zip`, it doesn't stop when the shorter iterable is exhausted, but fills in
values until the longer iterable is exhausted, too:

```python
from itertools import zip_longest

names = ['Dilbert', 'Alice', 'Wally']
ranks = range(1, 6)
ranking = zip_longest(ranks, names, fillvalue='fired')
for rank in ranking:
    print(rank)
```

    (1, 'Dilbert')
    (2, 'Alice')
    (3, 'Wally')
    (4, 'fired')
    (5, 'fired')

If a function with `n` parameters is given to the higher-order `map()` function,
it expects `n` iterables, too. The higher-order `starmap()` requires a single
iterable consisting of `n` tuples instead:

```python
from itertools import starmap

inventory = [
    (17, 0.99),
    (32, 0.49),
    (12, 5.49),
    (97, 0.19),
    (13, 2.95),
]

positions = starmap(lambda n, p: n * p, inventory)

for position in positions:
    print(position)
```

    16.83
    15.68
    65.88
    18.43
    38.35

The `filterfalse()` higher-order function works like `filter()`, except that it
returns the values for which the predicate function returns `False`:

```python
from itertools import filterfalse

def is_even(x):
    return x % 2 == 0

numbers = range(10)
even = filter(is_even, numbers)
odd = filterfalse(is_even, numbers)

print(list(even)) # [0, 2, 4, 6, 8]
print(list(odd))  # [1, 3, 5, 7, 9]
```

The `accumulate()` function works like `sum()`, but keeps a running total:

```python
from itertools import accumulate

xs = range(5)
sums = accumulate(xs)

print(list(xs))   # [0, 1, 2, 3, 4]
print(list(sums)) # [0, 1, 3, 6, 10]
```

Two or more iterables can be joined together using the `chain()` function:

```python
from itertools import chain

xs = range(3)
ys = range(3, 6)
zs = range(6, 9)

print(list(chain(xs, ys, zs))) # [0, 1, 2, 3, 4, 5, 6, 7, 8]
```

An iterable can be turned into `n` iterables with the same underlying values
using the `tee()` function:

```python
from itertools import tee

xs = range(5)

a, b, c = tee(xs, 3)
print(list(a)) # [0, 1, 2, 3, 4]
print(list(b)) # [0, 1, 2, 3, 4]
print(list(c)) # [0, 1, 2, 3, 4]
```

The `takewhile()` function works like `filter()`, but stops after the first item
fails the predicate function. The `dropwhile()` function ignores values until
the first item matches the predicate function:

```python
from itertools import takewhile, dropwhile

def is_even(x):
    return x % 2 == 0

numbers = [0, 2, 4, 6, 7, 8, 10, 11]

left = takewhile(is_even, numbers)
right = dropwhile(is_even, numbers)

print(list(left))  # [0, 2, 4, 6]
print(list(right)) # [7, 8, 10, 11]
```

Notice that the value `11` is included in the `right` list, even though it
wouldn't match the `is_even()` predicate function.

See the [`itertools`](https://docs.python.org/3/library/itertools.html) and
[`functools`](https://docs.python.org/3/library/functools.html) documentation
pages for more details and additional useful functions.

## Third-Party Libraries

The following third-party libraries have been introduced in this text:

- [PyMonad](https://pypi.org/project/PyMonad/) providing functional programming
  techniques the Python standard library doesn't.
- [OSlash](https://pypi.org/project/OSlash/) providing functors, applicatives,
  and monads.
</content>
    </entry>
    <entry>
        <title>Russ Olsen: “Getting Clojure”</title>
        <link href="https://paedubucher.ch/articles/2021-06-06-russ-olsen-getting-clojure.html"/>
        <author>
            <name>Patrick Bucher</name>
        </author>
        <id>https://paedubucher.ch/articles/2021-06-06-russ-olsen-getting-clojure.html</id>
        <updated>2021-06-06T13:00:00Z</updated>
        <content type="text/markdown; charset=UTF-8">
_This is a rough summary of [Getting
Clojure](https://pragprog.com/titles/roclojure/getting-clojure/) by Russ Olsen.
Some examples have been taken from the original, some have been modified, and
some have been made up._

# Hello, Clojure

Install Clojure and Leiningen on Arch Linux:

    # pacman -S clojure leiningen

Start a REPL:

    $ lein repl

Write a &quot;Hello World&quot; program on the REPL:

    &gt; (println &quot;Hello, World!&quot;)
    Hello, World!
    nil

Write the same program with comments to `hello.clj`:

    ;; Hello World program in Clojure.

    (println &quot;Hello, World!&quot;) ; Say hi.

Comments start with a semicolon and end with the line. Comments that take up a
whole line start with two semicolons by convention.

Run the program:

    $ clojure hello.clj
    Hello, World!

Use basic arithmetic functions:

    &gt; (+ 3 2)
    5
    &gt; (- 100 3 7)
    90
    &gt; (+ (* 3 6) (/ 12 4))
    21
    &gt; (/ 8 3) ; produces a ratio
    8/3
    &gt; (quot 8 3) ; truncates
    2

Bind a _symbol_ to a value:

    &gt; (def result (* 13 12))
    &gt; (println result)
    156

Concatenate multiple values as a string:

    &gt; (str 1 &quot;to&quot; 2)
    &quot;1to2&quot;

Write and call the &quot;Hello World&quot; program as a function:

    &gt; (defn hello-world [] (println &quot;Hello, World!&quot;))
    &gt; (hello-world)
    Hello, World!

Use a single function parameter:

    (defn greet [whom]
      (println &quot;Hello,&quot; whom))

    &gt; (greet &quot;John&quot;)
    Hello, John

Use multiple function parameters:

    (defn average [a b]
      (/ (+ a b) 2))

    &gt; (average 10 4)
    7

Use multiple expressions in the function body:

    (defn average [a b]
      (def a-plus-b (+ a b))
      (def half-the-sum (/ a-plus-b 2))
      half-the-sum)

    &gt; (average 24 6)
    15

The last expression of the function body (here: `half-the-sum`) is returned.

Create and run a proper application using Leiningen:

    $ lein new app hello-world
    $ cd hello-world
    $ lein run
    Hello, World!

Extend the example (`src/hello_world/core.clj`) as follows:

    (ns hello-world.core
      (:gen-class))

    (defn greet [app whom]
      (println app &quot;greets&quot; whom))

    (defn -main
      [&amp; args]
      (greet &quot;Hello World&quot; &quot;the user&quot;))

And run it again:

    $ lein run
    Hello World greets the user

# Vectors and Lists

Create a vector of numbers:

    &gt; [1 2 3 4]
    [1 2 3 4]

A vector can hold values of different types:

    &gt; [1 &quot;two&quot; 3.0 &quot;four&quot;]
    [1 &quot;two&quot; 3.0 &quot;four&quot;]

Vectors can be nested:

    &gt; [1 [&quot;foo&quot; &quot;bar&quot;] 2 [&quot;qux&quot; &quot;baz&quot;]]
    [1 [&quot;foo&quot; &quot;bar&quot;] 2 [&quot;qux&quot; &quot;baz&quot;]]

Vectors can also be created using the `vector` function:

    &gt; (vector 1 2 3 4)
    [1 2 3 4]
    &gt; (vector (vector &quot;one&quot; &quot;two&quot;) &quot;three&quot; (vector &quot;four&quot; &quot;five&quot;))
    [[&quot;one&quot; &quot;two&quot;] &quot;three&quot; [&quot;four&quot; &quot;five&quot;]]

`count` returns the number of elements in a vector:

    &gt; (def numbers [1 3 9 27])
    &gt; (count numbers)
    4

`first` returns the first element of a vector:

    &gt; (first [1 2 3 4])
    1
    &gt; (first [])
    nil

`rest` returns all the elements of a vector but the first as a _sequence_:

    &gt; (rest [1 2 3 4])
    &gt; (rest [1])
    ()
    &gt; (rest [])
    ()

`nth` returns the element at position `n` (zero-based index):

    &gt; (nth [1 2 3 4] 2)
    3

The nth element can also be accessed using the vector's name and an index:

    &gt; (def numbers [1 2 3 4 5])
    &gt; (numbers 3)
    4

`conj` adds an element _to the end_ of a vector:

    &gt; (def cities [&quot;London&quot;, &quot;New York&quot;, &quot;Berlin&quot;])
    &gt; (conj cities &quot;Moscow&quot;)
    [&quot;London&quot; &quot;New York&quot; &quot;Berlin&quot; &quot;Moscow&quot;]

`cons` adds an element _to the front_ of a vector:

    &gt; (def countries [&quot;USA&quot;, &quot;Germany&quot;, &quot;Turkey&quot;])
    &gt; (cons &quot;Russia&quot; countries)
    (&quot;Russia&quot; &quot;USA&quot; &quot;Germany&quot; &quot;Turkey&quot;)

Actually, a new vector or sequence, respectively, is created, holding the
additional element.

A list can be created as follows:

    &gt; '(&quot;New York&quot; &quot;London&quot; &quot;Berlin&quot;)
    (&quot;New York&quot; &quot;London&quot; &quot;Berlin&quot;)

Or using the `list` function:

    &gt; (list &quot;New York&quot; &quot;London&quot; &quot;Berlin&quot;)
    (&quot;New York&quot; &quot;London&quot; &quot;Berlin&quot;)

The functions `count`, `first`, `rest`, and `nth` can be applied to lists, too:

    &gt; (def countries '(&quot;USA&quot; &quot;Russia&quot; &quot;Germany&quot; &quot;France&quot;))
    &gt; (count countries)
    4
    &gt; (first countries)
    &quot;USA&quot;
    &gt; (rest countries)
    (&quot;Russia&quot; &quot;Germany&quot; &quot;France&quot;)
    &gt; (nth countries 3)
    &quot;France&quot;

Unlike vectors, a list can _not_ be used like a function:

    &gt; (countries 3)
    Execution error (ClassCastException) at user/eval2092 (REPL:1).
    clojure.lang.PersistentList cannot be cast to clojure.lang.IFn

Vectors are implemented as arrays, lists are implemented as linked lists. This
has some implications:

- Appending to the front is fast for lists and slow for vectors.
- Appending to the end is fast for vectors and slow for lists.

The `conj` function therefore adds elements to the front of lists and to the end
of vectors:

    &gt; (conj [1 2 3] &quot;what&quot;)
    [1 2 3 &quot;what&quot;]
    &gt; (conj '(1 2 3) &quot;what&quot;)
    (&quot;what&quot; 1 2 3)

# Maps, Keywords, and Sets

Maps are created using pairs within curly braces:

    &gt; {&quot;title&quot; &quot;War and Peace&quot; &quot;author&quot; &quot;Lev Tolstoy&quot; &quot;year&quot; 1869}
    {&quot;title&quot; &quot;War and Peace&quot;, &quot;author&quot; &quot;Lev Tolstoy&quot;, &quot;year&quot; 1869}

Commas between key-value pairs can be used for better readability, but are
optional:

    &gt; {&quot;title&quot; &quot;War and Peace&quot;, &quot;author&quot; &quot;Lev Tolstoy&quot;, &quot;year&quot; 1869}
    {&quot;title&quot; &quot;War and Peace&quot;, &quot;author&quot; &quot;Lev Tolstoy&quot;, &quot;year&quot; 1869}

Maps can also be created using the `hash-map` function:

    &gt; (hash-map &quot;title&quot; &quot;War and Peace&quot; &quot;author&quot; &quot;Lev Tolstoy&quot; &quot;year&quot; 1869)
    {&quot;author&quot; &quot;Lev Tolstoy&quot;, &quot;title&quot; &quot;War and Peace&quot;, &quot;year&quot; 1869}

The left part of the pair is the _key_, the right part the _value_ of the entry.

`get` looks up the value of an entry by its key:

    &gt; (def book {&quot;title&quot; &quot;War and Peace&quot; &quot;author&quot; &quot;Lev Tolstoy&quot; &quot;year&quot; 1869})
    &gt; (get book &quot;author&quot;)
    &quot;Lev Tolstoy&quot;

Like vectors, elements can be accessed without an explicit function call:

    &gt; (book &quot;title&quot;)
    &quot;War and Peace&quot;
    &gt; (book &quot;year&quot;)
    1869
    &gt; (book &quot;publisher&quot;)
    nil

Idiomatically, _keywords_ starting with a colon are used as map keys:

    &gt; (def book {:title &quot;War and Peace&quot; :author &quot;Lev Tolstoy&quot; :year 1869})
    &gt; book
    {:title &quot;War and Peace&quot;, :author &quot;Lev Tolstoy&quot;, :year 1869}
    &gt; (book :title)
    &quot;War and Peace&quot;

Keywords can also be used for map lookups:

    &gt; (:title book)
    &quot;War and Peace&quot;

`assoc` returns a map with an element either overwritten or added:

    &gt; (def book {:title &quot;War and Peace&quot; :author &quot;Lev Tolstoy&quot; :year 1869})
    {:title &quot;War and Peace&quot;, :author &quot;Lev Tolstoy&quot;, :year 1869}
    &gt; (assoc book :pages 2000)
    {:title &quot;War and Peace&quot;, :author &quot;Lev Tolstoy&quot;, :year 1869, :pages 2000}
    &gt; (assoc book :pages 1987)
    {:title &quot;War and Peace&quot;, :author &quot;Lev Tolstoy&quot;, :year 1869, :pages 1987}

Using `assoc`, it's possible to add/modify multiple key-value pairs at once:

    &gt; (def employee {:name &quot;Dilbert&quot;})
    &gt; (assoc employee :job &quot;Engineer&quot; :salary 120000)
    {:name &quot;Dilbert&quot;, :job &quot;Engineer&quot;, :salary 120000}

`dissoc` removes a map's entry by its key:

    &gt; (def employee {:name &quot;Dilbert&quot; :note &quot;smelly&quot;})
    &gt; (dissoc employee :note)
    {:name &quot;Dilbert&quot;}

Like `assoc`, multiple keys can be used at once with `dissoc`:

    &gt; (def employee {:name &quot;Dilbert&quot; :note &quot;smelly&quot; :terminate &quot;Jan 2023&quot;})
    &gt; (dissoc employee :note :terminate)
    {:name &quot;Dilbert&quot;}

Keys not found in the map will be ignored silently:

    &gt; (dissoc employee :sex-appeal :girlfriend :hobbies)
    {:name &quot;Dilbert&quot;, :note &quot;smelly&quot;, :terminate &quot;Jan 2023&quot;}

`keys` returns the map's keys (in unspecified order:

    &gt; (def book {:title &quot;War and Peace&quot; :author &quot;Lev Tolstoy&quot; :year 1869})
    &gt; (keys book)
    (:title :author :year)

Use a `sorted-map` for specified key ordering:

    &gt; (def book (sorted-map :title &quot;War and Peace&quot; :author &quot;Lev Tolstoy&quot; :year 1869))
    &gt; (keys book)
    (:author :title :year)

`keys` returns the map's values in arbitrary, but matching the key's order:

    &gt; (vals book)
    (&quot;War and Peace&quot; &quot;Lev Tolstoy&quot; 1869)
    &gt; (keys book)
    (:title :author :year)

A _set_ can be created as follows (commas being optional):

    &gt; #{&quot;Dilbert&quot;, &quot;Alice&quot;, &quot;Wally&quot;}
    #{&quot;Alice&quot; &quot;Wally&quot; &quot;Dilbert&quot;}

An element must not occur more than once:

    &gt; #{&quot;Dilbert&quot;, &quot;Alice&quot;, &quot;Wally&quot;, &quot;Dilbert&quot;}
    Syntax error reading source at (REPL:1:42).
    Duplicate key: Dilbert

`contains?` checks if an element is contained in a set:

    &gt; (def employees #{&quot;Dilbert&quot;, &quot;Alice&quot;, &quot;Wally&quot;, &quot;Ashok&quot;})
    &gt; (contains? employees &quot;Dilbert&quot;)
    true
    &gt; (contains? employees &quot;Pointy Haired Boss&quot;)
    false

This lookup can be done without calling a function, returning the element if it
is contained, or `nil` if the element is missing:

    &gt; (employees &quot;Dilbert&quot;)
    &quot;Dilbert&quot;
    &gt; (employees &quot;Ratbert&quot;)
    nil

When working with keywords, the order can be switched:

    &gt; (def genres #{:scifi :action :drama :love})
    &gt; (genres :scifi)
    :scifi
    &gt; (:scifi genres)
    :scifi

Functions like `count`, `first`, and `rest` handle map entries as two-element vectors:

    &gt; (def employee {:name &quot;Dilbert&quot; :age 42 :job &quot;Engineer&quot;})
    &gt; (count employee)
    3
    &gt; (first employee)
    [:name &quot;Dilbert&quot;]
    &gt; (rest employee)
    ([:age 42] [:job &quot;Engineer&quot;])

A set can be extendes using `conj`:

    &gt; (conj genres :western)
    #{:western :scifi :drama :action :love}

An element won't be added a second time _without_) error:

    &gt; (conj genres :western)
    #{:western :scifi :drama :action :love}
    &gt; (conj genres :western)
    #{:western :scifi :drama :action :love}

`disj` returns a set without the specified element:

    &gt; (disj genres :western)
    #{:scifi :drama :action :love}

No error occurs if the element is missing:

    &gt; (disj genres :comedy)
    #{:scifi :drama :action :love}

Be aware taht `nil` is a valid set entry and map key:

    &gt; (contains? #{:foo :bar nil} nil)
    true

# Logic

Conditional code can be executed using `if`:

    (if (= guess secret-number)
      (println &quot;You guessed the secret number.&quot;)
      (println &quot;Sorry, wrong number guessed...&quot;))

If the boolean expression (first argument) holds true, the second argument is
evaluated; otherwise, the (optional) third argument is evaluated.

Being an expression, `if` returns a value:

    (defn yield-rate [balance]
      (if (&gt;= balance 0) 0.125 12.5))

    &gt; (yield-rate 300)
    0.125
    &gt; (yield-rate -150)
    12.5

`nil` will be returned if the condition evaluates to `false` and if there's no
`else` branch.

Comparison operators like `=`, `not=`, and `&gt;=` are actually functions, which
can take two or more arguments:

    &gt; (= 2 2 2 2 3)
    false
    &gt; (= 2 2 2 2 2 2)
    true

    &gt; (not= 1 1 1 1)
    false
    &gt; (not= 1 1 2 1)
    true

    &gt; (&gt;= 9 6 4 4 1)
    true
    &gt; (&gt;= 9 6 4 5 1)
    false

Predicate functions return whether or not an expression is of some specific type:

    &gt; (number? 1987)
    true
    &gt; (string? &quot;Dilbert&quot;)
    true
    &gt; (keyword? :title)
    true
    &gt; (map? {:born 1987})
    true
    &gt; (vector? [1 2 3])
    true

Multiple conditions can be combined using `and`, `or`, and `not`:

    &gt; (or (and (&gt; 5 3) (&lt; 1 6)) (not (= 3 1)))
    true

Both `or` and `and` are _short-circuit_ operations (nothing is printed here):

    &gt; (or (= 1 1) (println &quot;strange&quot;))
    true
    &gt; (and (not= 1 1) (println &quot;strange&quot;))
    false

Every value besides `false` and `nil` is treated as _truthy_ (i.e. will be
evaluated to `true`), even empty collections and the number 0:

    &gt; (if [] (println &quot;[] is truthy&quot;))
    [] is truthy
    nil

    &gt; (if 0 (println &quot;0 is truthy&quot;))
    0 is truthy
    nil

Multiple expressions can be grouped together using `do`:

    (if (= guess secret-number)
      (do
        (println &quot;You guessed the secret number.&quot;)
        (println &quot;A winner is you.&quot;)
        {:points 100})
      (do
        (println &quot;You guessed the wrong number.&quot;)
        (println &quot;Shame on you.&quot;)
        {:points 0}))

The `do` expression evaluates to its last argument.

If no `else` branch is required, `when` can be used instead of `if`, which
allows for multiple expressions without using `do`:

    (when (= guess secret-number)
      (println &quot;You guessed the secret number.&quot;)
      (println &quot;A winner is you.&quot;)
      {:points 100})

If the condition doesn't hold `true`, `nil` is returned (like `if`).

Instead of nesting multiple `if`s, `cond` can be used for handling multiple
conditions:

    (defn check [guess secret-number]
      (cond
        (= guess secret-number) (println &quot;correct&quot;)
        (&lt; guess secret-number) (println &quot;too low&quot;)
        (&gt; guess secret-number) (println &quot;too high&quot;)))

    &gt; (check 3 3)
    correct
    nil

    &gt; (check 4 3)
    too high
    nil

    &gt; (check 3 4)
    too low
    nil

Idiomatically, a catch-all `:else` clause is added to make sure that every
condition is handled:

    (defn check [guess secret-number]
      (cond
        (= guess secret-number) (println &quot;correct&quot;)
        (&lt; guess secret-number) (println &quot;too low&quot;)
        (&gt; guess secret-number) (println &quot;too high&quot;)
        :else (println &quot;You broke the universe&quot;)))

Since `:else` is truthy, its branch will be evaluated unless any other branch
was evaluated before. (Any truthy value could be used instead of `:else`.)

For multiple equality comparisons against _constants_, `case` can be used
instead of `cond`:

    (defn color-hex-code [color]
      (case color
        :red &quot;#ff0000&quot;
        :green &quot;#00ff00&quot;
        :blue &quot;#0000ff&quot;
        &quot;unknown&quot;))

    &gt; (color-hex-code :red)
    &quot;#ff0000&quot;
    &gt; (color-hex-code :black)
    &quot;unknown&quot;

Exceptions can be handled using `try`/`catch`:

    (defn safe-divide [dividend divisor]
      (try
        (/ dividend divisor)
        (catch ArithmeticException e
          (println &quot;One does not simply divide by zero.&quot;))))

    &gt; (safe-divide 9 3)
    3

    &gt; (safe-divide 9 0)
    One does not simply divide by zero.
    nil.

Exceptions can be thrown using `throw` and `ex-info`:

    (defn publish [book]
      (when (&lt; (:pages book) 50)
        (throw
          (ex-info &quot;A book needs fifty pages or more!&quot; book))))

    &gt; (publish {:title &quot;Hello&quot; :pages 30})
    Execution error (ExceptionInfo) at user/publish (REPL:4).
    A book needs fifty pages or more!

`ex-info` expects a string message and a map argument, and can be caught as
`clojure.lang.ExceptionInfo`.

# More Capable Functions

_Multi-arity_ functions accept different sets of parameters:

    (defn greet
      ([to-whom] (println &quot;Hello&quot; to-whom))
      ([message to-whom] (println message to-whom)))

    &gt; (greet &quot;John&quot;)
    Hello John
    &gt; (greet &quot;Hi&quot; &quot;John&quot;)
    Hi John

In order to reduce the amount of duplicated code, it's common that lower arity
functions call the function with the highest arity by filling in the missing
parameters with default values:

    (defn greet
      ([to-whom] (greet &quot;Hello&quot; to-whom))
      ([message to-whom] (println message to-whom)))

_Variadic_ functions accept a variable number of arguments:

    (defn output-all [&amp; args]
      (println &quot;args&quot; args))

    &gt; (output-all &quot;one&quot;)
    args (one)
    &gt; (output-all &quot;one&quot; 2 &quot;three&quot; 4.0)
    args (one 2 three 4.0)

The arguments left of the ampersand are regular arguments:

    (defn output-all [x y &amp; args]
      (println &quot;x&quot; x &quot;y&quot; y &quot;args&quot; args))

    &gt; (output-all &quot;one&quot; 2 &quot;three&quot; 4.0)
    x one y 2 args (three 4.0)

Multi-arity and variadic functions are good at dealing with a _different number_
of arguments. _Multimethods_ are useful to deal with _different characteristics_
of arguments. They consist of:

1. A dispatch function (`defn`) that assigns a keyword to an argument.
2. A multimethod (`defmulti`) that groups the implementations and refers to the
   dispatch function.
3. Multiple methods (`defmethod`), of which each handles one type of argument.

Consider employees being stored in different formats:

    ;; implicit fields: first name, job
    (def dogbert [&quot;Dogbert&quot; &quot;Head of Abuse&quot;])
    (def ashok [&quot;Ashok&quot; &quot;Technical Intern&quot;])

    ;; relevant fields: name, position
    (def alice {:name &quot;Alice&quot; :position &quot;Engineer&quot; })
    (def wally {:name &quot;Wally&quot; :position &quot;Engineer&quot; })

    ;; relevant fields: first-name, job
    (def catbert {:first-name &quot;Catbert&quot; :job &quot;HR Manager&quot;})
    (def dilbert {:first-name &quot;Dilbert&quot; :job &quot;Engineer&quot; :department &quot;IT&quot;})

The dispatch function figures out which format such an entry has:

    (defn dispatch-employee-format [employee]
      (cond
        (vector? employee) :vector-employee
        (and (contains? employee :name)
             (contains? employee :position)) :min-employee
        (and (contains? employee :first-name)
             (contains? employee :job)) :max-employee))

The multimethod defines a method name and connects it to the dispatcher:

    (defmulti normalize-employee dispatch-employee-format)

The implementations all have the same name, but handle a different keyword, as
mapped by the dispatcher function:

    (defmethod normalize-employee :vector-employee [employee]
      {:first-name (nth employee 0) :role (nth employee 1)})

    (defmethod normalize-employee :min-employee [employee]
      {:first-name (:name employee) :role (:position employee)})

    (defmethod normalize-employee :max-employee [employee]
      {:first-name (:first-name employee) :role (:job employee)})

The different kind of employee data structures are converted to a common format:

    &gt; (normalize-employee dogbert)
    {:first-name &quot;Dogbert&quot;, :role &quot;Head of Abuse&quot;}
    &gt; (normalize-employee alice)
    {:first-name &quot;Alice&quot;, :role &quot;Engineer&quot;}
    &gt; (normalize-employee dilbert)
    {:first-name &quot;Dilbert&quot;, :role &quot;Engineer&quot;}

If the dispatch method cannot match the argument submitted, an exception will be
thrown:

    &gt; (normalize-employee {:first-name &quot;Topper&quot; :position &quot;Head of Annoyance&quot;})
    Execution error (IllegalArgumentException) at user/eval2108 (REPL:1).
    No method in multimethod 'normalize-employee' for dispatch value: null

This condition can be handled properly by defining a `:default` branch in the
dispatcher function.

Implementations for multimethods can be defined in different files, which allows
for extensibility. This allows for polymorphism not just based on type, but also
based on values.

Some functions are best implemented recursively:

    (def employees [{:name &quot;Dilbert&quot; :salary 120000}
                    {:name &quot;Wally&quot; :salary 130000}
                    {:name &quot;Alice&quot; :salary 110000}
                    {:name &quot;Boss&quot; :salary 380000}
                    {:name &quot;Ashok&quot; :salary 54000}])

    (defn sum-payroll
      ([employees] (sum-payroll employees 0))
      ([employees total]
        (if (empty? employees)
          total
          (sum-payroll
            (rest employees)
            (+ total (:salary (first employees)))))))

    &gt; (sum-payroll employees)
    794000

The `sum-payroll` function could run out of stack space if the `employee` vector
gets too big. Therefore, Clojure supports _tail call optimization_, by simply
replacing the function call with `recur`:

    (defn sum-payroll
      ([employees] (sum-payroll employees 0))
      ([employees total]
        (if (empty? employees)
          total
          (recur
            (rest employees)
            (+ total (:salary (first employees)))))))

The multi-arity function can be simplified to a single-arity function using a
`loop` expression:

    (defn sum-payroll [employees]
      (loop [employees employees total 0]
        (if (empty? employees)
          total
          (recur
            (rest employees)
            (+ total (:salary (first employees)))))))

This construct defines and invokes a pseudo-function, where the `employees`
parameter is initialized with the `employees` argument of the `sum-payroll`
function; and `total` is initialized to `0`. This values will be re-initialized
by `recur` (`employees` to `(rest employees)` and `total` to itself plus the
current employees salary).

In practice, higher-ordered functions such as `map` are preferred over
`loop`/`recur` constructs.

Since comments are dropped upon compilation, _docstrings_ provide a way of
documenting code that will be preserved. They are accessible via the `doc`
macro:

    (defn average
      &quot;Computes the average of a and b.&quot;
      [a b]
      (/ (+ a b) 2.0))

    &gt; (average 3 2)
    2.5

    &gt; (doc average)
    -------------------------
    user/average
    ([a b])
      Computes the average of a and b.
    nil

Docstrings can also be used for other constructs than functions:

    &gt; (def dilbert &quot;The smelly IT guy...&quot; {:name &quot;Dilbert&quot; :job &quot;Engineer&quot;})
    &gt; (doc dilbert)
    -------------------------
    user/dilbert
      The smelly IT guy...
    nil

A map containing `:pre` and `:post` entries can be used to enforce pre- and
post-conditions:

    (defn give-raise [employee amount]
      {:pre  [(&lt;= amount 10000) (not= (:name employee) &quot;Ashok&quot;)]
       :post [(&lt;= (:salary %) 180000)]}
      (assoc employee :salary (+ (:salary employee) amount)))

The `:pre` condition makes sure that a raise must not exceed 100000, and that
an employee named Ashok will never get a raise.

The `:post` condition makes sure that after a raise, no employee will have a
salary of more than 180000. The return value is referred by `%`.

    &gt; (give-raise {:name &quot;Dilbert&quot; :salary 120000} 5000)
    {:name &quot;Dilbert&quot;, :salary 125000}

    &gt; (give-raise {:name &quot;Wally&quot; :salary 110000} 15000)
    Execution error (AssertionError) at user/give-raise (REPL:1).
    Assert failed: (&lt;= amount 10000)

    &gt; (give-raise {:name &quot;Ashok&quot; :salary 45000} 1000)
    Execution error (AssertionError) at user/give-raise (REPL:1).
    Assert failed: (not= (:name employee) &quot;Ashok&quot;)

    &gt; (give-raise {:name &quot;Ted&quot; :salary 175000} 8000)
    Execution error (AssertionError) at user/give-raise (REPL:1).
    Assert failed: (&lt;= (:salary %) 180000)

# Functional Things

Functions are values, which can be passed to other functions:

    (def dilbert {:name &quot;Dilbert&quot; :job &quot;Engineer&quot; :salary 120000})
    (def ashok {:name &quot;Ashok&quot; :job &quot;Intern&quot; :salary 45000})

    (defn well-paid? [employee]
      (&gt; (:salary employee) 100000))

    (defn nerd? [employee]
      (= (:job employee) &quot;Engineer&quot;))

    (defn both? [employee pf1 pf2]
      (and (pf1 employee)
           (pf2 employee)))

    &gt; (both? dilbert well-paid? nerd?)
    true

    &gt; (both? ashok well-paid? nerd?)
    false

Anonymous functions can be defined using `fn`:

    (both? dilbert
      (fn [e] (&gt; (:salary e) 100000))
      (fn [e] (= (:name e) &quot;Dilbert&quot;)))

This can be used to created parametrized functions using a _lexical closure_:

    (defn cheaper-func [max-salary]
      (fn [employee]
        (&lt; (:salary employee) max-salary)))

    (def working-poor? (cheaper-func 50000))
    (def cheap-hire? (cheaper-func 100000))

    &gt; (working-poor? ashok)
    true
    
    &gt; (cheap-hire? dilbert)
    false

The `apply` function applies a function for each argument:

    &gt; (apply + [1 2 3])
    6

`partial` creates a new function by _partially_ filling in the arguments for an
existing function. Here, the plus function is partially applied with a single
number:

    &gt; (def increment (partial + 1))
    &gt; (increment 1)
    2
    &gt; (increment 10)
    11

And here, the `give-raise` function is partially applied to define the `amount`
parameter:

    (def dilbert {:name &quot;Dilbert&quot; :salary 120000 :job &quot;Engineer&quot;})

    (defn give-raise [amount employee]
      (assoc employee :salary (+ amount (:salary employee))))

    (def small-raise (partial give-raise 1000))

    &gt; (small-raise dilbert)
    {:name &quot;Dilbert&quot;, :salary 121000, :job &quot;Engineer&quot;}

`complement` produces a new function by wrapping a function with a `not` call:

    (defn is-cheap? [employee]
      (&lt;= (:salary employee) 100000))

    (def is-expensive? (complement is-cheap?))

    &gt; (is-cheap? dilbert)
    false
    &gt; (is-expensive? dilbert)
    true

`every-pred` combines multiple predicate function with `and`:

    (defn cheap? [employee]
      (&lt;= (:salary employee) 100000))

    (defn engineer? [employee]
      (= &quot;Engineer&quot; (:job employee)))

    (defn smelly? [employee]
      (= &quot;Dilbert&quot; (:name employee)))

    (def fire? (every-pred (complement cheap?) engineer? smelly?))

    &gt; (fire? {:name &quot;Dilbert&quot; :salary 120000 :job &quot;Engineer&quot;})
    true

    &gt; (fire? {:name &quot;Ted&quot; :salary 180000 :job &quot;Marketing&quot;})
    false

Function literals or _lambdas_ can be defined using `#`:

    &gt; (apply #(+ %1 %2 %3) [1 2 3])
    6

Since there is no argument list, the arguments are referred to using `%1`, `%2`,
etc. The highest-numbered argument defines the number of arguments:

    &gt; (apply #(+ %5 %6) [1 2 3 4 5 6])
    11

The arguments one to four (i.e. `[1 2 3 4]`) are ignored.

If only a single argument is needed, it can be referred by `%` instead of `%1`:

    &gt; (apply #(* 2 %) [123])
    246

Use lambdas for very short and simple functions. Use `fn` if named arguments are
useful. Use `defn` for lengthy functions with a proper name.

`defn` can be defined in terms of `def` and `fn`:

    (defn hello [to-whom]
      (println &quot;Hello&quot; to-whom))

Has the same effect as:

    (def hello
      (fn [to-whom]
        (println &quot;Hello&quot; to-whom)))

`update` works on a map by applying a function to a map's entry:

    (def dilbert {:name &quot;Dilbert&quot; :salary 120000 :job &quot;Engineer&quot;})

    (defn promote [employee raise-func]
      (update employee :salary raise-func))

    &gt; (promote dilbert #(+ % 1000))
    {:name &quot;Dilbert&quot;, :salary 121000, :job &quot;Engineer&quot;}

`update-in` accepts an additional path to locate the field in a nested map to be
updated:

    (def dogbertix {:name &quot;Dogbertix&quot; :ceo {:name &quot;Dogbert&quot; :salary 250000}})

    (defn give-bonus [company]
      (update-in company [:ceo :salary] #(* 2 %)))

    &gt; (give-bonus dogbertix)
    {:name &quot;Dogbertix&quot;, :ceo {:name &quot;Dogbert&quot;, :salary 500000}}

# Let

`compute-bonus` needs to calculate the same value twice; once for the `if`
condition, and once for the return value of the function:

    (defn compute-bonus [employee bonus-rate max-bonus]
      (if (&lt;= (* (:salary employee) bonus-rate) max-bonus)
        (* (:salary employee) bonus-rate)
        max-bonus))

    (def dilbert {:name &quot;Dilbert&quot; :salary 120000})

    &gt; (compute-bonus dilbert 0.1 5000)
    5000

    &gt; (compute-bonus dilbert 0.1 25000)
    12000.0

`let` defines re-usable local bindings:

    (defn compute-bonus [employee bonus-rate max-bonus]
      (let [bonus (* (:salary employee) bonus-rate)]
        (if (&lt;= bonus max-bonus)
          bonus
          max-bonus)))

The expression on the right-hand side is assigned to the symbol on the left-hand
side of the vector. Multiple local bindings can be created at once:

    &gt; (let [a 1 b 2 c 3] (println (+ a b c)))
    6

Later bindings have access to earlier bindings to their left:

    &gt; (let [a 1 b (* 2 a) c (* 2 b)] (println a b c))
    1 2 4

The function `compute-bonus` calculates the employee's bonus by looking up their
bonus rate in a map:

    (def employee-bonus-rates
      {&quot;Dilbert&quot; 0.05 &quot;Dogbert&quot; 0.25 &quot;Pointy-Haired Boss&quot; 1.0})

    (defn compute-bonus [salary employee-name bonus-rates min-bonus]
      (let [bonus-rate (bonus-rates employee-name)
            bonus (* salary bonus-rate)]
        (if (&lt; bonus min-bonus)
          min-bonus
          bonus)))

    &gt; (compute-bonus 120000 &quot;Dilbert&quot; employee-bonus-rates 1000)
    6000.0

    &gt; (compute-bonus 200000 &quot;Dogbert&quot; employee-bonus-rates 10000)
    50000.0

The map of `employee-bonus-rates` has to be carried away wherever a bonus has to
be calculated. A better approach is to return individual functions by employee
that have their bonus rate parametrized:

    (defn mk-compute-bonus-func [employee-name bonus-rates min-bonus]
      (let [bonus-rate (bonus-rates employee-name)]
        (fn [salary]
          (let [bonus (* bonus-rate salary)]
            (if (&lt; bonus min-bonus)
              min-bonus
              bonus)))))

    (def calc-dilbert-bonus
      (mk-compute-bonus-func &quot;Dilbert&quot; employee-bonus-rates 1000))

    (def calc-dogbert-bonus
      (mk-compute-bonus-func &quot;Dogbert&quot; employee-bonus-rates 10000))

    &gt; (calc-dilbert-bonus 120000)
    6000.0

    &gt; (calc-dogbert-bonus 200000)
    50000.0

`let` is used to bind local variables that are referred to by another function
(lexical closure).

The following function outputs book entries with their authors, if available:

    (def books [{:title &quot;War and Peace&quot; :author &quot;Lev Tolstoy&quot;}
                {:title &quot;Beowulf&quot;}
                {:title &quot;The Name of the Rose&quot; :author &quot;Umberto Eco&quot;}
                {:title &quot;Till Eulenspiegel&quot;}])

    (defn output [book]
      (let [author (:author book)]
        (if author
          (str (:title book) &quot; by &quot; author)
          (:title book))))

    &gt; (map output books)
    (&quot;War and Peace by Lev Tolstoy&quot; &quot;Beowulf&quot; &quot;The Name of the Rose by Umberto Eco&quot; &quot;Till Eulenspiegel&quot;)

`if` and `let` can be combined to `if-let`, making the function shorter:

    (defn output [book]
      (if-let [author (:author book)]
        (str (:title book) &quot; by &quot; author)
        (:title book)))

First, the binding with `let` is created; second, the bound value is evaluated
using `if` (yielding `true` for any truthy value).

`when-let` combines `when` with `let` in the same way:

    (defn writtey-by [book]
      (when-let [author (:author book)]
        (str (:title book) &quot; was written by &quot; author)))
    
    &gt; (map writtey-by books)
    (&quot;War and Peace was written by Lev Tolstoy&quot; nil &quot;The Name of the Rose was written by Umberto Eco&quot; nil)

# Def, Symbols, and Vars

Like keywords, symbols are values. Whereas keywords evaluate to themselves,
symbols created with `def` are bound to other values. The symbol itself can be
accessed programmatically using the single quote:

    &gt; (def first-name &quot;Dilbert&quot;)
    &gt; first-name
    &quot;Dilbert&quot;
    &gt; 'first-name
    first-name
    &gt; (str 'first-name)
    &quot;first-name&quot;
    &gt; (= 'first-name 'last-name)
    false
    &gt; (= 'first-name 'first-name)
    true

A symbol and a value are bound together using a _var_, which is accessible
through the pound character and the symbol:

    &gt; (def first-name &quot;Dilbert&quot;)
    #'user/first-name
    &gt; (def the-name #'first-name)

Symbol and value then can be accessed as follows:

    &gt; (.-sym the-name)
    first-name
    &gt; (.get the-name)
    &quot;Dilbert&quot;

Vars are _mutable_, so that bindings can be re-defined during development, say,
in a REPL session.

_Dynamic bindigs_ can be changed using `binding` and are, by convention,
surrounded by asterisks (`*`) or &quot;earmuffs&quot;:

    &gt; (def ^:dynamic *debug-enabled* false)
    &gt; *debug-enabled*
    false

    &gt; (binding [*debug-enabled* true]
        (println *debug-enabled*))
    true

Vars are _not_ supposed to be used like variables in other programming
languages. Use `^:dynamic` vars and `binding` sparingly.

The REPL provides some dynamic vars `*[n]` where `[n]` denotes the n-last
result:

    &gt; (+ 2 1)
    3
    &gt; (+ 5 4)
    9
    (- *1 *2) ; 9 - 3
    6

Dynamic vars can be changed using the `set!` function:

    &gt; (def employees [&quot;Dilbert&quot; &quot;Wally&quot; &quot;Alice&quot; &quot;Ted&quot; &quot;Ashok&quot;])
    &gt; employees
    [&quot;Dilbert&quot; &quot;Wally&quot; &quot;Alice&quot; &quot;Ted&quot; &quot;Ashok&quot;]
    &gt; (set! *print-length* 2)
    &gt; employees
    [&quot;Dilbert&quot; &quot;Wally&quot; ...]

`*e` denotes the last exception thrown:

    &gt; (/ 3 0)
    Execution error (ArithmeticException) at user/eval2038 (REPL:1).
    Divide by zero
    &gt; *e
    #error {
     :cause &quot;Divide by zero&quot;
     ...

# Namespaces

Vars, which represent the binding between a symbol and a value, live in
_namespaces_. There is always one _current_ namespace, affected by calls of
`def`.

The REPL creates and uses a namespace called `user`:

    &gt; (def employee &quot;Dilbert&quot;)
    #'user/employee

The `employee` symbol is bound to the value `&quot;Dilbert&quot;` within the `user`
namespace.

A new namespace can be created and made the current namespace using `ns`:

    &gt; (ns dilbertix)
    &gt; (def employees [:dilbert :alice :wally])
    #'dilbertix/employees

Calling `ns` with an existing namespace makes that namespace the current,
without changing it:

    &gt; (ns user)
    &gt; (def today &quot;Sunday&quot;)
    #'user/today

After switching back, the bindings are still available:

    &gt; (ns dilbertix)
    &gt; employees
    [:dilbert :alice :wally]

Symbols from other namespaces can be referred to using a _fully qualified
symbol_:

    &gt; (ns user)
    &gt; dilbertix/employees

Namespaces defined in other files need to be loaded before they can be used. The
function `clojure.data/diff` is unavailable by default:

    &gt; (def engineers [:alice :dilbert :wally])
    &gt; (def high-performers [:alice :dilbert :topper])
    &gt; (clojure.data/diff engineers high-performers)
    Execution error (ClassNotFoundException) at java.net.URLClassLoader/findClass (URLClassLoader.java:382).

The `clojure.data` namespace can be made available using `require`:

    &gt; (require 'clojure.data)
    &gt; (clojure.data/diff engineers high-performers)
    [[nil nil :wally] [nil nil :topper] [:alice :dilbert]]

Given a new project skeleton:

    $ lein new app dilbertix

Containing the file `src/dilbertix/core.clj`:

    (ns dilbertix.core
      (:gen-class))

    (defn -main
      &quot;I don't do a whole lot ... yet.&quot;
      [&amp; args]
      (println &quot;Hello, World!&quot;))

The namespace `dilbertix.core` and the file's location `dilbertix/core.clj`
match together: A namespace `foo.bar` is to be found in a file `foo/bar.clj`.

Dashes need to be converted to underscores: The namespace `foo-bar.qux` is to be
found in the file `foo_bar/qux.clj`.

Thus, a new namespace `dilbertix.employees` is to be defined within the project
folder in the file `src/dilbertix/employees.clj`:

    (ns dilbertix.employees)

    (def job-satisfaction 0.0021)

    (def employees [:dilbert :alice :wally])

The namespace definition can be supplied with `:require` expressions:

    (ns dilbertix.core
      (:require dilbertix.employees)
      (:gen-class))

    (defn -main
      [&amp; args]
      (println &quot;Our Employees:&quot; dilbertix.employees/employees))

Notice the difference between the stand-alone `require`:

    (require 'dilbertix.employees) ; symbol, quoted

And the expression within the `ns` definition:

    (:require dilbertix.employees) ; keyword, unquoted

Aliases can be defined to make imported names shorter:

    (require '[dilbertix.employees :as employees])

Or within the namespace definition:

    (ns dilbertix.core
        (:require [dilbertix.employees :as employees]))

Which allows for shorter references:

    (println employees/job-satisfaction)

Instead of:

    (println dilbertix.employees/job-satisfaction)

Aliases don't mask ordinary bindings, so an `employees` var would still be
visible.

Using `:refer`, vars from anothe rnamespace are pulled into the current
namespace:

    (require '[dilbertix.employees :refer [employees job-satisfaction])

Which would mask an existing `employees` binding in the current namespace.
Therefore, `:refer` should be used sparingly.

The current namespace is available through the symbol `*ns*`:

    &gt; (println *ns*)
    #object[clojure.lang.Namespace 0x38158523 user]

Existing namespaces can be looked up by their name:

    &gt; (find-ns 'user)
    #object[clojure.lang.Namespace 0x38158523 &quot;user&quot;]

Namespaces can be discovered:

    &gt; (ns-map *ns*)
    {primitives-classnames #'clojure.core/primitives-classnames, +' #'clojure.core/+' ...
    ;; omitted

The namespace is part of the symbol and can be extracted using the `namespace`
function:

    &gt; (def hello &quot;world&quot;)
    &gt; (namespace 'user/hello)
    &quot;user&quot;

The namespace `clojure.core` provides functions such as `println` or `first` and
is made ready automatically:

    (require '[clojure.core :refer :all])

There is _no_ hierarchy of namespaces. The dots in `clojure.core.data` are just
part of the name.

Using `require`, a namespace only gets loaded once, which is sensible for code
within files. In a REPL session, reloading modified code is common, and the
`:reload` keyword can be used:

    (require :reload '[dilbertix.employees :as employees])

Symbols no longer needed can be removed using `ns-unmap`:

    (ns-unmap 'dilbertix.employees)

`defonce` makes sure a symbol is only bound to a value _once_, even if the
definition is required using `:reload`:

    (defonce answer-to-everything (summarize-all-books))

If the symbol is supposed to be rebound nonetheless, it can be unmapped:

    (ns-unmap *ns* 'answer-to-everything)

# Sequences

The different collection types (maps, vectors, lists, sets) share a common
wrapper interface called a _sequence_. That's why the `count` function (and many
others) work on different kinds of collections:

    &gt; count [1 2 3]) ; vector: number of items
    3
    &gt; (count {:name &quot;John&quot; :age 29}) ; map: number of key-value pairs
    2

The `seq` function wraps any collection in a sequence:

    &gt; (seq {:age 42 :name &quot;Dilbert&quot;})
    ([:age 42] [:name &quot;Dilbert&quot;]) ; sequence of key-value pairs

    &gt; (seq [&quot;Alice&quot;, &quot;Dilbert&quot;, &quot;Wally&quot;])
    (&quot;Alice&quot; &quot;Dilbert&quot; &quot;Wally&quot;) ; looks like a list, is a sequence

`seq` returns `nil` when invoked on an empty collection:

    &gt; (seq '()) ; empty list
    nil
    &gt; (seq [])  ; empty vector
    nil
    &gt; (seq {})  ; empty map
    nil
    &gt; (seq #{}) ; empty set
    nil

Like `rest`, `next` returns all but the first elements of a sequence:

    &gt; (rest [1 2 3])
    (2 3)
    &gt; (next [1 2 3])
    (2 3)

_Unlike_ `rest`, `next` returns `nil` the remainder is an empty sequence:

    &gt; (rest [1])
    ()
    &gt; (next [1])
    nil

Always rely on `rest` and `next` returning an empty collection or `nil`,
respectively; _never_ compare the result of `first` against `nil` for this
purpuse:

    ;; bad idea!
    (defn is-empty [collection]
      (= (first (seq collection)) nil))

    &gt; (is-empty [1 2 3])   ; correct
    false
    &gt; (is-empty [])        ; correct
    true
    &gt; (is-empty [nil 1 2]) ; wrong!
    true

    ;; better approach
    (defn is-empty [collection]
        (= (next (seq collection)) nil))

    &gt; (is-empty [1 2 3])   ; correct
    false
    &gt; (is-empty [])        ; correct
    true
    &gt; (is-empty [nil 1 2]) ; correct
    false

New elements can be added to the front of a sequence using `cons`:

    &gt; (cons 0 (seq [1 2 3]))
    (0 1 2 3)
    &gt; (cons [:job &quot;Engineer&quot;] (seq {:name &quot;Dilbert&quot; :age 42}))
    ([:job &quot;Engineer&quot;] [:name &quot;Dilbert&quot;] [:age 42])

`rest`, `next`, `cons` (but _not_ `conj`), `sort`, `reverse` all return
sequences.

A _seqable_ is something that `seq` can turn into a sequence.

`partition` chops a sequence (or _seqable_) into a sequence of smaller junks:

    &gt; (partition 2 [1 2 3 4 5])
    ((1 2) (3 4))

`interleave` zips two sequences together:

    &gt; (interleave [1 3 5 7 9] [2 4 6 8])
    (1 2 3 4 5 6 7 8)

`interpose` adds a separator value in between the elements:

    &gt; (interpose &quot;and&quot; [&quot;Dilbert&quot; &quot;Wally&quot; &quot;Alice&quot;])
    (&quot;Dilbert&quot; &quot;and&quot; &quot;Wally&quot; &quot;and&quot; &quot;Alice&quot;)
    &gt; (interpose '+ [1 2 3])
    (1 + 2 + 3)

`filter` accepts a predicate function and a sequence, and returns a new sequence
holding the elements for which the predicate holds true:

    &gt; (defn negative? [x] (&lt; x 0))
    &gt; (filter negative? [5 -5 10 -10])
    (-5 -10)

    (defn useful? [employee]
        (not= (:job employee) &quot;Manager&quot;))

    &gt; (filter useful? [{:name &quot;Pointy Haired Boss&quot; :job &quot;Manager&quot;}
                       {:name &quot;Dilbert&quot; :job &quot;Engineer&quot;}])
    ({:name &quot;Dilbert&quot;, :job &quot;Engineer&quot;})

Like `filter`, `some` applies a predicate to the elements of a sequence. Unlike
`filter`, it returns the first truthy value returned by the predicate:

    &gt; (some neg? [1 2 3])
    nil
    &gt; (some neg? [1 2 -3])
    true

    (defn useful-name [employee]
      (when
        (not= (:job employee) &quot;Manager&quot;)
        (:name employee)))

    &gt; (some useful-name [{:name &quot;Pointy Haired Boss&quot; :job &quot;Manager&quot;}
                         {:name &quot;Dilbert&quot; :job &quot;Engineer&quot;}])
    &quot;Dilbert&quot;

`map` transforms the elements of a sequence using a function:

    (defn raise-salary [employee]
      (assoc employee :salary (* 1.2 (:salary employee))))

    &gt; (map raise-salary [{:name &quot;Dilbert&quot; :salary 120000}
                         {:name &quot;Ashok&quot; :salary 10000}])
    ({:name &quot;Dilbert&quot;, :salary 144000.0} {:name &quot;Ashok&quot;, :salary 12000.0})

    &gt; (map :name [{:name &quot;Dilbert&quot; :salary 120000}
                  {:name &quot;Ashok&quot; :salary 10000}])
    (&quot;Dilbert&quot; &quot;Ashok&quot;)

`comp` (for &quot;compose&quot;) produces a function by applying the argument functions
from right to left (first, the salary is raised; second, the `:name` is
extracted):

    &gt; (map (comp :name raise-salary)
        [{:name &quot;Dilbert&quot; :salary 120000}
         {:name &quot;Ashok&quot; :salary 10000}])
    (144000.0 12000.0)

`for` processes a sequence element by element:

    &gt; (def employees [{:name &quot;Dilbert&quot; :salary 120000}
                      {:name &quot;Ashok&quot; :salary 10000}])
    &gt; (for [e employees] (:name e))
    (&quot;Dilbert&quot; &quot;Ashok&quot;)

`reduce` combines the elements of a sequence into a single value. It works with
a function that requires _two_ values: an accumulator and the current element:

    &gt; (reduce (fn [acc x] (+ acc x)) [1 2 3 4])
    10
    &gt; (reduce (fn [acc x] (* acc x)) [1 2 3 4])
    24

The starting value of the accumulator can be defined, too:

    &gt; (reduce (fn [acc x] (+ acc x)) 100 [1 2 3 4])
    110

If the start value is left out, the first element will be used for it.

Since arithmetic operators are functions, this can be simplified:

    &gt; (reduce + [1 2 3 4])
    10
    &gt; (reduce * [1 2 3 4])
    24

Those higher-order functions can be used to compose elegant solutions:

    (def employees [{:name &quot;Dilbert&quot; :salary 120000}
                    {:name &quot;Wally&quot; :salary 130000}
                    {:name &quot;Alice&quot; :salary 11000}
                    {:name &quot;Dogbert&quot; :salary 180000}
                    {:name &quot;Topper&quot; :salary 150000}])

    (defn top-earners [n employees]
      (apply
        str
        (interpose
          &quot; &gt;= &quot;
          (map :name (take n (reverse (sort-by :salary employees)))))))

    &gt; (top-earners 3 employees)
    &quot;Dogbert &gt;= Topper &gt;= Wally&quot;

    &gt; (top-earners 5 employees)
    &quot;Dogbert &gt;= Topper &gt;= Wally &gt;= Dilbert &gt;= Alice&quot;

    &gt; (top-earners 1 employees)
    &quot;Dogbert&quot;

The definition of `top-earners` needs to be read from the inside out. The pointy
arrow function `-&gt;&gt;` allows for an easier to read syntax without any runtime
performance overhead:

    (defn top-earners [n employees]
      (-&gt;&gt;
        employees
        (sort-by :salary)
        reverse
        (take n)
        (map :name)
        (interpose &quot; &gt;= &quot;)
        (apply str)))

The `-&gt;&gt;` function uses the result of a function as the _last_ argument for the
next function all; `-&gt;` as the _first_ argument for the subsequent function
call.

Since Clojure provides so many ways of processing sequences, turning something
into a sequence can be a big step to solving that problem.

`line-seq` turns the lines of a text file into a (lazy) sequence. Given this CSV
employee data base (`employees.txt`):

    Pointy Haired Boss;Manager;58;250000
    Dilbert;Engineer;42;120000
    Alice;Engineer;39;110000
    Wally;Engineer;52;130000
    Dogbert;Consultant;7;390000
    Topper;Salesman;35;850000
    Ted;Project Manager;45;280000

`read-employee-db` turns it into a sequence of maps:

    (require '[clojure.java.io :as io])
    (require '[clojure.string :as str])

    (defn split-by [sep]
      (fn [line]
        (str/split line sep)))

    (defn to-employee [values]
      (zipmap [:name :job :age :salary] values))

    (defn read-employee-db [filename]
      (with-open [r (io/reader filename)]
        (map
          (comp to-employee (split-by #&quot;;&quot;))
          (doall (line-seq r)))))

    &gt; (read-employee-db &quot;employees.txt&quot;)
    ((:name &quot;Pointy Haired Boss&quot; :job &quot;Manager&quot; :age &quot;58&quot; :salary &quot;250000&quot;)
     (:name &quot;Dilbert&quot; :job &quot;Engineer&quot; :age &quot;42&quot; :salary &quot;120000&quot;)
     (:name &quot;Alice&quot; :job &quot;Engineer&quot; :age &quot;39&quot; :salary &quot;110000&quot;)
     (:name &quot;Wally&quot; :job &quot;Engineer&quot; :age &quot;52&quot; :salary &quot;130000&quot;)
     (:name &quot;Dogbert&quot; :job &quot;Consultant&quot; :age &quot;7&quot; :salary &quot;390000&quot;)
     (:name &quot;Topper&quot; :job &quot;Salesman&quot; :age &quot;35&quot; :salary &quot;850000&quot;)
     (:name &quot;Ted&quot; :job &quot;Project Manager&quot; :age &quot;45&quot; :salary &quot;280000&quot;))

Which then can be processed using the `top-earners` function from before:

    &gt; (top-earners 3 (read-employee-db &quot;employees.txt&quot;))
    &quot;Topper &gt;= Dogbert &gt;= Ted&quot;

Using regular expressions, strings can be turned into sequences (e.g. of words):

    &gt; (re-seq #&quot;\w+&quot; &quot;this is some sentence to be split&quot;) ; split by whitespace
    (&quot;this&quot; &quot;is&quot; &quot;some&quot; &quot;sentence&quot; &quot;to&quot; &quot;be&quot; &quot;split&quot;)

Even though sequences are extremely useful, data structures like maps and
vectors loose _some_ of their power when wrapped as a sequence.

# Lazy Sequences

Unlike sequences, _lazy sequences_ only make up their values as they are
actually needed.

`repeat` creates a lazy sequence that contains the given element unlimited
times:

    &gt; (def words (repeat &quot;duck&quot;))

There are, of course, not unlimited instances of the string `&quot;duck&quot;` put into
memory, which would be impossible to do. The elements of the lazy sequence are
only created when used:

    &gt; (nth words 3)
    &quot;duck&quot;
    &gt; (nth words 123456)
    &quot;duck&quot;

    &gt; (take 3 words)
    (&quot;duck&quot; &quot;duck&quot; &quot;duck&quot;)

`cycle` creates a lazy sequence by repeating a given sequence endlessly:

    &gt; (take 7 (cycle [1 2 3]))
    (1 2 3 1 2 3 1)

`iterate` creates a lazy sequence based on a function. The first argument is a
function to be called for each subsequent iteration; the secund argument is a
starting value:

    &gt; (def counter (iterate inc 1))
    &gt; (take 3 counter)
    (1 2 3)
    &gt; (take 12 counter)
    (1 2 3 4 5 6 7 8 9 10 11 12)

Note that the lazy sequence is _not consumed_ like an iterator in other
programming languages.

This `counter` sequence can be used to enumerate items of a sequence:

    &gt; (interleave [&quot;Alive&quot; &quot;Dilbert&quot; &quot;Wally&quot;] counter)
    (&quot;Alive&quot; 1 &quot;Dilbert&quot; 2 &quot;Wally&quot; 3)

Or:

    &gt; (zipmap counter [&quot;Alive&quot; &quot;Dilbert&quot; &quot;Wally&quot;])
    {1 &quot;Alive&quot;, 2 &quot;Dilbert&quot;, 3 &quot;Wally&quot;}

`map` is lazy, so it can be applied to unbound lazy sequences:

    &gt; (defn twice [x] (* 2 x))
    &gt; (def doubled (map twice counter))
    &gt; (take 7 doubled)
    (2 4 6 8 10 12 14)

Combined with `cycle`, `map` can be used to combine existing elements in all
possible ways (permutations):

    (def names [&quot;Alice&quot; &quot;Dilbert&quot; &quot;Wally&quot; &quot;Ashok&quot; &quot;Dogbert&quot;])
    (def adjectives [&quot;great&quot; &quot;lazy&quot; &quot;nerdy&quot; &quot;evil&quot;])
    (def professions [&quot;engineer&quot; &quot;manager&quot; &quot;consultant&quot;])

    (defn combine-employees [name adjective profession]
      (str name &quot; the &quot; adjective &quot; &quot; profession))

    (def employees
      (map combine-employees
        (cycle names)
        (cycle adjectives)
        (cycle professions)))

    &gt; (take 8 employees)
    (&quot;Alice the great engineer&quot; &quot;Dilbert the lazy manager&quot; &quot;Wally the nerdy consultant&quot;
     &quot;Ashok the evil engineer&quot; &quot;Dogbert the great manager&quot; &quot;Alice the lazy consultant&quot;
     &quot;Dilbert the nerdy engineer&quot; &quot;Wally the evil manager&quot;)

`lazy-seq` creates a lazy sequence from an existing sequence:

    &gt; (def numbers (lazy-seq [1 2 3]))
    (take 2 numbers)
    (1 2)

Never output lazy sequences as if they were finite:

    &gt; (def counter (iterate inc 1))
    &gt; counter ; bad idea

Or at least be sure to set `*print-length*` before doing so:

    &gt; (set! *print-length* 10)
    &gt; counter
    (1 2 3 4 5 6 7 8 9 10 ...)

`doall` realizes a lazy sequence, which should only be done for _finite_ lazy
sequences:

    &gt; (doall counter) ; bad idea, again...

`doseq` is similar to `for` and useful if the iteratio step is more interesting
than the result:

    &gt; (def numbers (take 5 counter))
    &gt; (doseq [n numbers]
        (println &quot;Current iteration&quot; n))
    Current iteration 1
    Current iteration 2
    Current iteration 3
    Current iteration 4
    Current iteration 5

Infinite sequences should not be sorted or reduced.

Many functions are lazy, such as `take`.

Notice that working with lazy sequences opens a timely gap between when the
instruction to do something is given and when it is actyally done. This can
cause troubles when working with side-effects (e.g. files read/written with
`slurp`/`spit` whose content changes in the meantime).

# Destructuring

_Destructuring_ is a tool for unpacking data structures with little syntax:

    &gt; (def employees [&quot;Dilbert&quot; &quot;Wally&quot;])
    &gt; (let [[nerd lazybone] employees]
        (println nerd &quot;is a nerd&quot;)
        (println lazybone &quot;is a lazybone&quot;))
    Dilbert is a nerd
    Wally is a lazybone

The left side vector of `let` describes the data to be extracted. The right side
expression is the data structure to be unpacked.

The unpacking need not be exhaustive. Values can be simply dropped on the right
side:

    &gt; (let [[a b c] [&quot;foo&quot; &quot;bar&quot; &quot;baz&quot; &quot;qux&quot;]] ; &quot;qux&quot; ignored
        (println a b c))
    foo bar baz

And the dummy symbol `_` can be used to drop values from the left side:

    &gt; (let [[_ a _ b] [&quot;foo&quot; &quot;bar&quot; &quot;baz&quot; &quot;qux&quot;]] ; &quot;foo&quot; and &quot;qux&quot; ignored
        (println a b))
    bar qux

Nested structures can be destructured using a nested pattern on the left side:

    &gt; (def teams [[&quot;Dilbert&quot; &quot;Alice&quot; &quot;Wally&quot;] [&quot;Dogbert&quot; &quot;Ratbert&quot; &quot;Catbert&quot;]])
    &gt; (let [[[dilbert _ wally] [dogbert _ catbert]] teams]
        (println dilbert wally dogbert catbert))

Anything that can be turned into a sequence can be destructured:

    &gt; (let [[one _ _ four] '(1 2 3 4)]
        (println one four))
    1 4

    &gt; (let [[b a r] &quot;bar&quot;]
        (println b a r))
    b a r

    &gt; (let [[a _ b _ c] (iterate inc 1)]
        (println a b c))
    1 3 5

Destructuring can not only be used with `let`, but also when calling functions:

    &gt; (defn fire [[scapegoat-one scapegoat-two]]
        (println scapegoat-one &quot;and&quot; scapegoat-two &quot;are fired!&quot;))
    &gt; (fire [&quot;Dilbert&quot; &quot;Alice&quot; &quot;Wally&quot; &quot;Dogbert&quot;])
    Dilbert and Alice are fired!

When destructuring maps, the variable to be bound stands on the left, and the
keyword for the value to be extracted stands on the right:

    &gt; (def employees {:engineer &quot;Dilbert&quot; :consultant &quot;Dogbert&quot; :slacker &quot;Wally&quot;})
    &gt; (let [{scapegoat :engineer} employees]
        (println scapegoat))
    Dilbert

Nested data structures can be destructured, too:

    &gt; (def company {:name &quot;Random Inc.&quot;
                    :employees [{:name &quot;Dilbert&quot; :role &quot;Engineer&quot;}
                                {:name &quot;Dogbert&quot; :role &quot;Consultant&quot;}]})
    &gt; (let [{[{looser-job :role} {leech :name}] :employees} company]
        (println looser-job &quot;is the worst job and&quot; leech &quot;is a leech&quot;))
    Engineer is the worst job and Dogbert is a leech

If all of a map's values are to be bound, listing all the keys is cumbersome:

    &gt; (def employees [{:name &quot;Dilbert&quot; :role &quot;Engineer&quot; :age 42}
                      {:name &quot;Dogbert&quot; :role &quot;Consultant&quot; :age 7}])
    &gt; (defn describe [{name :name role :role age :age}]
        (println name &quot;is a&quot; age &quot;year old&quot; role))
    &gt; (map describe employees)
    Dilbert is a 42 year old Engineer
    Dogbert is a 7 year old Consultant

The `:keys` keyword allows for a shorter mapping:

    &gt; (defn describe [{:keys [name role age]}]
        (println name &quot;is a&quot; age &quot;year old&quot; role))

If the passed value should not only be destructured, but also retained in its
entirety, the `:as` keyword can be used.

    &gt; (defn add-greeting [{:keys [name role age] :as employee}]
        (assoc employee
               :greeting
               (str &quot;I'm a &quot; age &quot; year old &quot; role &quot; called &quot; name)))
    &gt; (map add-greeting employees)
    ({:name &quot;Dilbert&quot;, :role &quot;Engineer&quot;, :age 42,
      :greeting &quot;I'm a 42 year old Engineer called Dilbert&quot;}
     {:name &quot;Dogbert&quot;, :role &quot;Consultant&quot;, :age 7,
      :greeting &quot;I'm a 7 year old Consultant called Dogbert&quot;})

Default values can be provided using the `:or` keyword:

    &gt; (defn email [{:keys [user host domain]
                    :or {user &quot;root&quot;, host &quot;localhost&quot;, domain &quot;local&quot;} :as parts}]
        (str user &quot;@&quot; host &quot;.&quot; domain))
    &gt; (email {:user &quot;john&quot;})
    &quot;john@localhost.local&quot;
    &gt; (email {:host &quot;dilbertix&quot; :domain &quot;com&quot;})
    &quot;root@dilbertix.com&quot;

Destructuring can't be used directly with `def`, only within `let`.

# Records and Protocols

A common tradeoff in programming is between _generic_ and _specialized_
solutions. Maps are _generic_ and very flexible. They can deal with arbitrary
keys, which comes with a runtime penalty when dealing with huge amounts of data.

_Records_ are specialized data structures dealing only a set of predefined keys:

    &gt; (defrecord Employee [name age job salary])

`defrecord` creates three `var`s: one for the record type, and two factory
functions `-&gt;Employee` and `map-&gt;Employee` to create instances of the record
type:

    &gt; (def dilbert (-&gt;Employee &quot;Dilbert&quot; 42 &quot;Engineer&quot; 120000))

    &gt; (def alice (map-&gt;Employee
        {:name &quot;Alice&quot;
         :age 37
         :job &quot;Engineer&quot;
         :salary 110000}))

An instance of a record can be used like a map:

    &gt; (:name dilbert)
    &quot;Dilbert&quot;

    &gt; (:job alice)
    &quot;Engineer&quot;

    &gt; (keys dilbert)
    (:name :age :job :salary)

    &gt; (def alice-promoted (assoc alice :salary 120000 :job &quot;Head of Engineering&quot;))
    &gt; alice-promoted
    #user.Employee{:name &quot;Alice&quot;, :age 37, :job &quot;Head of Engineering&quot;, :salary 120000}

It's also possible to associate _extra fields_ with a record; however, those
don't get the speed optimization of the record's defined fields:

    &gt; (def dilbert-secret (assoc dilbert :note &quot;Smelly and ugly guy&quot;))
    &gt; dilbert-secret
    #user.Employee{:name &quot;Dilbert&quot;, :age 42, :job &quot;Engineer&quot;,
                   :salary 120000, :note &quot;Smelly and ugly guy&quot;}

While the speed advantage of records is only noticable for large amounts of
data, the documentation provided by records is always helpful:

    &gt; (defrecord Poet [name century works])
    &gt; (defrecord FictionalCharacter [name show traits])

    &gt; (def homer-1
        (-&gt;Poet &quot;Homer&quot; &quot;8th/7th B.C.&quot; [&quot;Iliad&quot; &quot;Odyssey&quot;]))
    &gt; (def homer-2
        (-&gt;FictionalCharacter &quot;Homer&quot; &quot;The Simpsons&quot; [&quot;lazy&quot; &quot;stupid&quot; &quot;impulsive&quot;]))

`class` returns the underlying type of a record instance:

    &gt; (class homer-1)
    user.Poet

    &gt; (class homer-2)
    user.FictionalCharacter

`instance?` (like Java's `instanceof`) checks if an instance if of a specific record type:

    &gt; (instance? FictionalCharacter homer-1)
    false

    &gt; (instance? FictionalCharacter homer-2)
    true

This offers one primitive way to create polymorphic functions:

    (defn output [x]
      (if (instance? FictionalCharacter x)
        (println (:name x) &quot;the&quot; (:traits x) &quot;character from&quot; (:show x))
        (println (:name x) &quot;the author of&quot; (:works x) &quot;who lived&quot; (:century x))))
    
    &gt; (output homer-1)
    Homer the author of [Iliad Odyssey] who lived 8th/7th B.C.

    &gt; (output homer-2)
    Homer the [lazy stupid impulsive] character from The Simpsons

However, _protocols_ are a better alternative for this purpose. A protocol
(here: `Person`) defines a set of functions (here: `describe`, `greet`) that can
be performed on different kinds of records implementing that protocol:

    (defprotocol Person
      (describe [this])
      (greet [this msg]))

The functions of the protocol need to be implemented by the records. The
protocol name (here: `Person`) is followed by the _method definitions_:

    (defrecord Poet [name century works]
      Person
      (describe [this]
        (str
          (:name this)
          &quot;, the author of &quot; (clojure.string/join &quot;, &quot; (:works this))
          &quot; who lived in &quot; (:century this)))
      (greet [this msg]
        (str
          msg &quot; &quot; (:name this)
          &quot;, author of &quot; (clojure.string/join &quot;, &quot; (:works this)))))

    (defrecord FictionalCharacter [name show traits]
      Person
      (describe [this]
        (str
          (:name this)
          &quot;, the &quot; (clojure.string/join &quot;, &quot; (:traits this))
          &quot; character from &quot; (:show this)))
      (greet [this msg]
        (str
          msg &quot; &quot; (:name this)
          &quot;, you &quot; (clojure.string/join &quot;, &quot; (:traits this))
          &quot; character from &quot; (:show this))))

The first argument (conventionally called `this`) refers to the instance the
function is called on:

    &gt; (def homer-1
        (-&gt;Poet &quot;Homer&quot; &quot;8th/7th B.C.&quot; [&quot;Iliad&quot; &quot;Odyssey&quot;]))
    &gt; (describe homer-1)
    &quot;Homer, the author of Iliad, Odyssey who lived in 8th/7th B.C.&quot;
    &gt; (greet homer-1 &quot;Greetings&quot;)
    &quot;Greetings Homer, author of Iliad, Odyssey&quot;

    &gt; (def homer-2
        (-&gt;FictionalCharacter &quot;Homer&quot; &quot;The Simpsons&quot; [&quot;lazy&quot; &quot;stupid&quot; &quot;impulsive&quot;]))
    &gt; (describe homer-2)
    &quot;Homer, the lazy, stupid, impulsive character from The Simpsons&quot;
    &gt; (greet homer-2 &quot;Hi&quot;)
    &quot;Hi Homer, you lazy, stupid, impulsive character from The Simpsons&quot;

New protocols can be created and implemented for existing types using
`extend-protocol`:

    (defprotocol Greetable
      (say-hi [this]))

    (extend-protocol Greetable
      Employee
      (say-hi [employee]
        (str &quot;Hello, I'm &quot; (:name employee) &quot;. I work as a &quot; (:job employee) &quot;.&quot;))
      Poet
      (say-hi [poet]
        (str &quot;Greetings, I'm &quot; (:name poet) &quot;, author of &quot;
             (clojure.string/join &quot;, &quot; (:works poet)) &quot;.&quot;))
      FictionalCharacter
      (say-hi [character]
        (str &quot;Hi, I'm &quot; (:name character) &quot; from &quot; (:show character) &quot;.&quot;)))

    &gt; (say-hi dilbert)
    &quot;Hello, I'm Dilbert. I work as a Engineer.&quot;

    &gt; (say-hi homer-1)
    &quot;Greetings, I'm Homer, author of Iliad, Odyssey&quot;

    &gt; (say-hi homer-2)
    &quot;Hi, I'm Homer from The Simpsons.&quot;

It is also possible to implement protocols for existing data types:

    (extend-protocol Greetable
      String
      (say-hi [string]
        (str &quot;Hi, I'm the String '&quot; string &quot;'.&quot;))
      Boolean
      (say-hi [bool]
        (str &quot;Hi, I'm the Boolean '&quot; bool &quot;'.&quot;)))
    
    &gt; (say-hi &quot;foobar&quot;)
    &quot;Hi, I'm the String 'foobar'.&quot;

    &gt; (say-hi false)
    &quot;Hi, I'm the Boolean 'false'.&quot;

Records and their instances resemble classes and objects, but they don't have
hierarchies and are immutable. Protocols are similar to abstract classes or
interfaces, but more flexible: They can be implemented without touching the
definition of the type the methods are implemented for; and, again, they don't
come in hierarchies. Also, most object-oriented programming languages require
the programmer to use classes, objects, and interfaces. Records and protocols,
however, are optional: Better start without them, and only use them if they
bring some tangible benefit.

Protocols and multimethods have a lot in common, but also some differences:

- Multimethods define single, stand-alone operations. Protocols group related
  operations together.
- Multimethods support an arbitrary dispatch mechanism. Protocols dispatch based
  on a type.

Simple one-off implementations of a protocol as a single instance (say, for test
doubles) can be created using `reify`:

    (def dirty-harry
      (reify Person
        (describe [_] &quot;Lieutenant Harry Callahan, San Francisco Police Department&quot;)
        (greet [_ msg] (str msg &quot;, punk. Feeling lucky today?&quot;))))

    &gt; (describe dirty-harry)
    &quot;Lieutenant Harry Callahan, San Francisco Police Department&quot;

    &gt; (greet dirty-harry &quot;Hi, there&quot;)
    &quot;Hi there, punk. Feeling lucky today?&quot;

Since `this` wasn't used in any of the method bodies, it was replaced by `_`.

The methods defined for a protocol could pollute the namespace:

    &gt; (defprotocol Items (count [this]))
    Warning: protocol #'user/Items is overwriting function count

When in doubt, put protocols in their own namespace.

`deftype` is a more generic version of `defrecord` and requires the programmer
to provide all of the behaviour of a new type. This is more work than defining a
new record, but allows for more flexibility.

# Tests

For the following example, a new project `company` is created:

    $ lein new company

A fixed set of employees is defined in `src/company/employees.clj`:

    (ns company.employees)

    (def employees [{:name &quot;Dilbert&quot; :age 42 :job &quot;Engineer&quot; :salary 120000}
                    {:name &quot;Alice&quot; :age 37 :job &quot;Engineer&quot; :salary 115000}
                    {:name &quot;Wally&quot; :age 47 :job &quot;Engineer&quot; :salary 130000}
                    {:name &quot;Pointy Haired Boss&quot; :age 57 :job &quot;Manager&quot; :salary 250000}
                    {:name &quot;Ashok&quot; :age 22 :job &quot;Intern&quot; :salary 18000}
                    {:name &quot;Dogbert&quot; :age 7 :job &quot;Consultant&quot; :salary 470000}
                    {:name &quot;Catbert&quot; :age 9 :job &quot;Head of HR&quot; :salary 190000}])

Some functions to operate on a set of employees are provided in `src/company/core.clj`:

    (ns company.core)

    (defn find-by-name
      &quot;Search for an employee by name (unique result)&quot;
      [employees by-name]
      (first (filter #(= (:name %1) by-name) employees)))

    (defn sum-salaries
      &quot;Sum up the salaries of all given employees&quot;
      [employees]
      (reduce #(+ %1 %2) (map #(:salary %1) employees)))

Unit tests can be created using the `clojure.test` library. In
`src/company/core_test.clj`, test cases for the functions in `company.core` are
defined. The test functions from `clojure.core`, the functions to be tested from
`company.core`, and the static set of employees in `company.employees` are
required:

    (ns company.core-test
      (:require [clojure.test :refer :all])
      (:require [company.core :as cc])
      (:require [company.employees :as ce]))

A simple test is defined using `deftest` and the `is` assertion function:

    (deftest test-finding-employee-by-name
      (is (not (nil? (cc/find-by-name ce/employees &quot;Dilbert&quot;)))))

The test can be executed with Leiningen:

    $ lein test
    lein test company.core-test

    Ran 1 tests containing 1 assertions.
    0 failures, 0 errors.

To test another function, another `deftest` is created:

    (deftest test-sum-up-employee-salaries
      (is (= 1293000 (cc/sum-salaries ce/employees))))

Multiple test cases can be grouped together and described using `testing` as
so-called _subtests_ or _contexts_:

    (deftest test-finding-employee-by-name
      (testing &quot;Finding employees&quot;
        (is (not (nil? (cc/find-by-name ce/employees &quot;Dilbert&quot;))))
        (is (not (nil? (cc/find-by-name ce/employees &quot;Catbert&quot;)))))
      (testing &quot;Not finding employees&quot;
        (is (nil? (cc/find-by-name ce/employees &quot;Sharkbert&quot;)))
        (is (nil? (cc/find-by-name ce/employees &quot;Competent Boss&quot;)))))

In order to test a property of the code, one needs to work with examples that
_exercise that property_. Instead of making up examples manually, the
[`test.check`](https://github.com/clojure/test.check) library can be used for
_Property-Based Testing_. First, `test.check` needs to be added as a dependency
(`project.clj`):

    :dependencies [[org.clojure/clojure &quot;1.10.1&quot;]
                   [org.clojure/test.check &quot;1.1.0&quot;]]

Random data can then be created using _generators_:

    &gt; (require '[clojure.test.check.generators :as gen])
    &gt; (gen/sample gen/string-alphanumeric)
    (&quot;&quot; &quot;M&quot; &quot;Lw&quot; &quot;cs9&quot; &quot;CtQU&quot; &quot;yOg95&quot; &quot;YE&quot; &quot;3XTEL&quot; &quot;001qEF3w&quot; &quot;ZTwzwZ&quot;)

`gen/string-alphanumeric` generates an endless stream of alphanumeric strings
(including empty ones), `gen/sample` takes a sample of that stream. In order to
test the functions for the employee data base, the constrained generators for
the following map keywords are needed:

- `:name`: alphanumeric, non-empty
- `:age`: numeric, positive, non-zero
- `:job`: alphanumeric, non-empty
- `:salary`: numeric, positive, non-zero

The constraints can be modeled using `such-that` predicates:

    (def text-gen
      (gen/such-that not-empty gen/string-alphanumeric))

    &gt; (gen/sample text-gen)
    (&quot;1JB&quot; &quot;91&quot; &quot;t&quot; &quot;FJyD&quot; &quot;34eM&quot; &quot;3h0a5&quot; &quot;9fhzo&quot; &quot;8v31R&quot; &quot;00O83F&quot; &quot;PNEEsZQMe&quot;)

    (def num-gen
      (gen/such-that (complement zero?) gen/pos-int))

    &gt; (gen/sample num-gen)
    (1 3 1 3 2 4 1 3 5 5)

A single employee map can be created using those functions:

    (def employee-gen
      (gen/hash-map :name text-gen
                    :age num-gen
                    :job text-gen
                    :salary num-gen))

    &gt; (gen/sample employee-gen)
    ({:name &quot;wlf&quot;, :age 3, :job &quot;8&quot;, :salary 1}
     {:name &quot;5&quot;, :age 1, :job &quot;ZR&quot;, :salary 1}
     {:name &quot;Hd&quot;, :age 2, :job &quot;L3&quot;, :salary 3}) ; output shortened

An endless supply of non-empty employee databases (vectors) can be generated:

    (def payroll-gen
      (gen/not-empty (gen/vector employee-gen)))

    &gt; (gen/sample payroll-gen)
    ([{:name &quot;uQ&quot;, :age 2, :job &quot;Zl&quot;, :salary 2}]
     [{:name &quot;VR&quot;, :age 2, :job &quot;ME&quot;, :salary 2}
      {:name &quot;vj&quot;, :age 3, :job &quot;8&quot;, :salary 1}]
     [{:name &quot;BYsO&quot;, :age 2, :job &quot;p7&quot;, :salary 2}]) ; output shortened

In order to conduct tests, a random example has to be plucked from the test
data. Here, `gen/let` is used to create a map containing a payroll together with
one single employee taken from that payroll (from the `inventory` built by
`inventory-gen`, assign a random, single element to `book`):

    (def payroll-and-employee-gen
      (gen/let [payroll payroll-gen
                employee (gen/elements payroll)]
        {:payroll payroll :employee employee}))

    &gt; (gen/smaple payroll-and-employee-gen)
    ({:payroll [{:name &quot;Y0&quot;, :age 2, :job &quot;K0&quot;, :salary 1}],
      :employee {:name &quot;Y0&quot;, :age 2, :job &quot;K0&quot;, :salary 1}}
      :employee {:name &quot;Q&quot;, :age 3, :job &quot;T&quot;, :salary 1}}
     {:payroll [{:name &quot;qv&quot;, :age 3, :job &quot;2ys&quot;, :salary 3}
                {:name &quot;n&quot;, :age 2, :job &quot;893&quot;, :salary 1}
                {:name &quot;2g&quot;, :age 3, :job &quot;cLx&quot;, :salary 2}],
      :employee {:name &quot;2g&quot;, :age 3, :job &quot;cLx&quot;, :salary 2}}) ; output shortened

Once the functions to generate the test data are ready, the property needs to be
expressed with property test functions, provided by the `test.check` library:

    &gt; (require '[clojure.test.check.properties :as prop])

A _theorem_—e.g. the increment of a number is bigger than that number—can be
expressed using `prop/for-all`:

    (prop/for-all [i gen/pos-int]
      (&lt; i (inc i)))

Computers can't prove theorems, but only execute tests, for which `test.check`
provides functions:

    &gt; (require '[clojure.test.check :as tc])

To perform a test, a limit (e.g. 50) of examples to be considered needs to be
supplied using `tc/quick-check`, which wraps the theorem from above:

    (tc/quick-check 50    
      (prop/for-all [i gen/pos-int]
        (&lt; i (inc i))))

This function produces an output describing the tests conducted:

    {:result true, :pass? true, :num-tests 50, :time-elapsed-ms 4,
     :seed 1622715897727}

(The `seed` value could be used to reproduce the random data that was
generated.)

Finally, these tools can be combined to write property test functions:

    (def tc/quick-check 50
      (prop/for-all [p-and-e payroll-and-employee-gen]
        (= (cc/find-by-name (:payroll p-and-e) (-&gt; p-and-e :employee :name))
           (:employee i-and-e))))

For each payroll/employee sample, the function to be tested `cc/find-by-name` is
called with the whole payroll and the randomly picked employee's name. The
result is than compared to random employee extracted by the generator before.

In order to integrate this property-based test into the native test runner,
`defspec` from `test.check` can be used:

    &gt; (require '[clojure.test.check.clojure-test :as ctest])

The test can then be defined using `ctest/decspec` as follows:

    (ctest/defspec find-by-name-finds-employee 50
      (prop/for-all [p-and-e payroll-and-employee-gen]
        (= (cc/find-by-name (:payroll p-and-e) (-&gt; p-and-e :employee :name))
           (:employee p-and-e))))

Which then can be tested using Leiningen:

    $ lein test

    lein test company.core-property-test
    {:result true, :num-tests 50, :seed 1622716789601, :time-elapsed-ms 92,
     :test-var &quot;find-by-name-finds-employee&quot;}

    lein test company.core-test

    Ran 3 tests containing 6 assertions.
    0 failures, 0 errors.

Here's the whole property-based test for the employee database
(`test/company/core_property_test.clj`):

    (ns company.core-property-test
      (:require [clojure.test :refer :all])
      (:require [company.core :as cc])
      (:require [clojure.test.check :as tc])
      (:require [clojure.test.check.clojure-test :as ctest])
      (:require [clojure.test.check.generators :as gen])
      (:require [clojure.test.check.properties :as prop]))

    (def text-gen
      (gen/such-that not-empty gen/string-alphanumeric))

    (def num-gen
      (gen/such-that (complement zero?) gen/pos-int))

    (def employee-gen
      (gen/hash-map :name text-gen
                    :age num-gen
                    :job text-gen
                    :salary num-gen))

    (def payroll-gen
      (gen/not-empty (gen/vector employee-gen)))

    (def payroll-and-employee-gen
      (gen/let [payroll payroll-gen
                employee (gen/elements payroll)]
        {:payroll payroll :employee employee}))

    (ctest/defspec find-by-name-finds-employee 50
      (prop/for-all [p-and-e payroll-and-employee-gen]
        (= (cc/find-by-name (:payroll p-and-e) (-&gt; p-and-e :employee :name))
           (:employee p-and-e))))

While unit tests are easier to implement and understand, they often cover only a
few hand-picked examples, leaving much of the input space untested.
Property-based testing is harder to implement and understand, but covers a much
wider space of possible inputs. However, one might jump to the wrong conclusion
that _all_ possibilities are covered, where generators probably miss some basic
but crucial cases (e.g. picking 0 as a random number for testing division by
zero).

Therefore, it's a good idea to start with some unit tests covering the basic
cases (say, all combinations of a devision with positive and negative numbers,
and zero). Property-based tests can then be introduced to cover more of the
input space.

Notice that property-based tests are non-deterministic. Re-starting a failed
test pipeline might yield a test run without errors, but the underlying error
remains.

Even having only one single trivial test case is way better than having no test
cases at all:

    (require '[clojure.test :refer :all])
    (require '[company.core :as cc])
    (require '[company.employees :as ce])

    (deftest test-sum-up-employee-salaries
      (is (= 1293000 (cc/sum-salaries ce/employees))))

This test case reveals a lot about the code base being tested:

- There is a namespace called `company.core`, providing more or less useful
  functions.
- There is a namespace called `company.employees`, providing an actual database
  of employee records.
- There is a function `sum-salaries` that calculates the total salaries of an
  employee database, returning a positive integer.

Simple unit tests can be made more powerful by using parameters with `are`:

    (deftest test-finding-employee-by-name-parametrized
      (testing &quot;Finding employees by their name, checking their roles&quot;
        (are [actual expected] (= (:job actual) expected)
             (cc/find-by-name ce/employees &quot;Dilbert&quot;) &quot;Engineer&quot;
             (cc/find-by-name ce/employees &quot;Ashok&quot;) &quot;Intern&quot;
             (cc/find-by-name ce/employees &quot;Dogbert&quot;) &quot;Consultant&quot;)))

Here, three examples are provided, each consisting of two expressions: The left
(the actual function call) being mapped to `actual`, the right (the expected
result) being mapped to `expected`. The test consists of comparing a property of
`actual` (the job of the employee) to the `expected` value. This helps keeping
the test definition separate from the test examples, which makes it less
effortful to add more test cases.

# Spec

Clojure programmers are often more concerned with the _shape_ of data rather
than its _type_. The question is rather &quot;is this a vector of maps each providing a
`:name` key?&quot; than &quot;is this a `NamedItemsVector`?&quot;.

The shape of data can be verified by providing functions such as this:

    (defn employee? [x]
      (and
        (map? x)
        (string? (:name x))
        (pos-int? (:age x))
        (string? (:job x))
        (pos-int? (:salary x))))

    &gt; (employee? {:name &quot;Dilbert&quot; :age 42 :job &quot;Engineer&quot; :salary 120000})
    true
    &gt; (employee? {:name &quot;Clint Eastwood&quot; :age 82 :role &quot;Dirty Harry&quot;})
    false

This manual approach doesn't scale well. Consider matching the shape of strings:
Writing a state machine manually for every pattern neither scales well. Instead,
_regular expressions_ are used to describe those patterns. The
[`clojure.spec`](https://clojure.org/about/spec) library provides facilities to
define and check the shape of data; it works like regular expressions for data
structures:

    &gt; (require '[clojure.spec.alpha :as s])

`clojure.spec` is about to be finished, and therefore is used under the
namespace `.alpha` for the time being.

The `s/valid?` function expects a predicate function and a value and returns
whether or not the value passes the validation:

    &gt; (s/valid? number? 44)
    true
    &gt; (s/valid? string? 44)
    false

Multiple predicates can be combined using `clojure.spec/and`:

    (def n-leq-100
      (s/and number? #(&lt;= % 100)))

    &gt; (s/valid? n-leq-100 99)
    true

    &gt; (s/valid? n-leq-100 101)
    false

A predicate function `n-leq-100` is called a _spec_. The whole library and
concept is called `clojure.spec`.

Predicates can also be combined using `clojure.spec/or`, which requires
additional keywords describing the checks:

    (def far-from-zero?
      (s/or :positive #(&gt; % +10)
            :negative #(&lt; % -10)))

    &gt; (s/valid? far-from-zero? 5)
    false
    &gt; (s/valid? far-from-zero? 15)
    true
    &gt; (s/valid? far-from-zero? -5)
    false
    &gt; (s/valid? far-from-zero? -15)
    true

The keywords `:positive` and `:negative` are useful for providing feedback in
case the value fails to match the spec (more of which later).

Multiple predicates can be combined to build new predicates:

    (def n-pos? #(&gt;= % 0))
    (def n-leq-100? #(&lt;= % 100))
    (def n-even? #(= (mod % 2) 0))
    (def n-pos-even-leq-100
      (s/and
        n-pos?
        n-leq-100?
        n-even?))

    &gt; (s/valid? n-pos-even-leq-100 99)
    false
    &gt; (s/valid? n-pos-even-leq-100 98)
    true

`spec/coll-of` can be used to check if something is a collection of something:

    (def coll-of-strings? (s/coll-of string?))

    &gt; (s/valid? coll-of-strings? [&quot;John&quot; &quot;Doe&quot;])
    true
    &gt; (s/valid? coll-of-strings? [&quot;one&quot; &quot;two&quot; &quot;three&quot; 4 &quot;five&quot;])
    false

`spec/cat` can be used to create _this_ should follow _that_ specs (descriptive
keywords are needed):

    (def s-n-s-n? (s/cat :s1 string? :n1 number? :s2 string? :n2 number?))

    &gt; (s/valid? s-n-s-n? [&quot;Dilbert&quot; 42 &quot;Ashok&quot; 21])
    true
    &gt; (s/valid? s-n-s-n? [&quot;Dilbert&quot; &quot;Alice&quot; &quot;Dogbert&quot; &quot;Wally&quot;])
    false

Specs for maps can be written using the `keys` function (using the `employees`
database from the last chapter `src/company/employees.clj`):

    (ns company.employees
      (:require [clojure.spec.alpha :as s]))

    (def employee-s?
      (s/keys :req-un [:company.employees/name
                       :company.employees/age
                       :company.employees/job
                       :company.employees/salary]))

    &gt; (require '[clojure.spec.alpha :as s])
    &gt; (require '[company.employees :as ce])
    &gt; (s/valid? ce/employee-s? {:name &quot;Dilbert&quot; :age 42 :job &quot;Engineer&quot; :salary 120000})
    true
    &gt; (s/valid? ce/employee-s? {:name &quot;Ashok&quot; :age 27 :job &quot;Intern&quot;})
    false

Here, namespace-qualified keys (`:company.employees/name`) have been used.
However, the `-un` part of `:req-un` means _unqualified_, so the keys of a map
value don't have to be qualified in order to match.

In order to use specs in different locations, they can be stored in a _global
registry_ (&quot;global&quot; means JVM-wide) using `clojure.spec/def`. This registers an
employee record as `:company.employees/employee`:

    (s/def :company.employees/employee
      (s/keys :req-un [:company.employees/name
                       :company.employees/age
                       :company.employees/job
                       :company.employees/salary]))

The spec can be used globally under its global name:

    &gt; (s/valid? :company.employees/employee
        {:name &quot;Dilbert&quot; :age 42 :job &quot;Engineer&quot; :salary 120000})
    true

Keywords must be fully qualified in the spec definition because of the global
registry, otherwise they could collide with other keywords. However, from within
the namespace `company.employees`, the shortcut `::name` can be used instead of
`:company.employees.name`. Thus, the above spec can be simplified:

    (s/def :company.employees/employee
      (s/keys :req-un [::name
                       ::age
                       ::job
                       ::salary]))

`clojure.spec` tries to look up the fully qualified keys in the registry. If a
spec is found, the value associated with that key is validated against it.
(Otherwise, no validation takes place.)

Let's create additional specs for the map keywords:

    (s/def ::name string?)

    (s/def ::age int?)

    (s/def ::job string?)

    (s/def ::salary int?)

    (s/def ::employee
      (s/keys :req-un [::name
                       ::age
                       ::job
                       ::salary]))

    (s/def ::employees (s/coll-of ::employee))

    &gt; (s/valid? :company.employees/employee
        {:name &quot;Dilbert&quot; :age 42 :job &quot;Engineer&quot; :salary 120000})
    true
    &gt; (s/valid? :company.employees/employee
        {:name &quot;Ashok&quot; :age 27 :job &quot;Intern&quot; :salary &quot;nothing&quot;})
    false

When using heavily nested specs, it's often unclear _why_ a particular value
failed to match a spec. In this case, `clojure.spec/explain` can be used (just
like `valid`):

    &gt; (s/explain :company.employees/employee
        {:name &quot;Ashok&quot; :age 27 :job &quot;Intern&quot; :salary &quot;nothing&quot;})
    &quot;nothing&quot; - failed: int? in: [:salary] at: [:salary] spec: :company.employees/salary

`explain` always returns `nil` and prints its result. The related function
`clojure.spec/conform`, on the other side, returns the (positively) matching
value, or `:clojure.spec.alpha/invalid` in case of a mismatch:

    &gt; (s/conform :company.employees/employee
        {:name &quot;Dilbert&quot; :age 42 :job &quot;Engineer&quot; :salary 120000})
    {:name &quot;Dilbert&quot;, :age 42, :job &quot;Engineer&quot;, :salary 120000}
    &gt; (s/conform :company.employees/employee
        {:name &quot;Ashok&quot; :age 27 :job &quot;Intern&quot; :salary &quot;nothing&quot;})
    :clojure.spec.alpha/invalid

Specs can also be used to validate the arguments of a function. One way is to
use `:pre` and `:post` conditions with functions (`src/company/core.clj`):

    (ns company.core
      (:require [company.employees])
      (:require [clojure.spec.alpha :as s]))

    (defn find-by-name
      &quot;Search for an employee by name (unique result)&quot;
      [employees by-name]
      {:pre [(s/valid? :company.employees/employees employees)
             (s/valid? :company.employees/name by-name)]}
      (first (filter #(= (:name %1) by-name) employees)))

A more convenient way is to define those conditions separately from the function
using `clojure.spec/fdef`:

    (s/fdef find-by-name
            :args (:by-name :company.employees/name))

Those checks come with a significant performance penalty and are therefore
deactivated by default. They can be activated by explicitly instrumenting a
function:

    &gt; (require '[company.employees])
    &gt; (require '[clojure.spec.alpha :as s])
    &gt; (require '[clojure.spec.test.alpha :as st])

    &gt; (st/instrument 'company.core/find-by-name)
    &gt; (find-by-name company.employees/employees &quot;Dilbert&quot;)
    {:name &quot;Dilbert&quot;, :age 42, :job &quot;Engineer&quot;, :salary 120000}

    &gt; (find-by-name company.employees/employees :dilbert)
    Execution error - invalid arguments to company.core/find-by-name at ...
    :dilbert - failed: string? at: [:by-name] spec: :company.employees/name

This should only be used during development and testing.

Creating specs provides much information that can be used for generating test
data. Consider the function `introduce` (`src/company/employees.clj`) and its
spec:

    (defn introduce [employee]
      (str &quot;Hello, my name is &quot;
           (:name employee)
           &quot;, I'm &quot;
           (:age employee)
           &quot; years old.&quot;))

    (s/fdef introduce :args (s/cat :employee :company.employees/employee))

    &gt; (st/instrument 'company.core/introduce)
    &gt; (introduce (find-by-name company.employees/employees &quot;Dilbert&quot;))
    &quot;Hello, my name is Dilbert, I'm 42 years old.&quot;

Using the `:ret` keyword, the return value of the function can be checked if it
contains the static portion of the text:

    (s/fdef introduce
            :args (s/cat :employee :company.employees/employee)
            :ret (s/and string?
                        (partial re-find #&quot;Hello, my name is &quot;)
                        (partial re-find #&quot;I'm &quot;)
                        (partial re-find #&quot; years old.&quot;)))

The function must be instrumented for testing:

    &gt; (require '[clojure.spec.test.alpha :as stest])
    &gt; (stest/check 'company.core/introduce)
    ({:spec #object[clojure.spec.alpha$fspec_impl$reify__2524 0x58782ed6
     &quot;clojure.spec.alpha$fspec_impl$reify__2524@58782ed6&quot;],
     :clojure.spec.test.check/ret {:result true, :pass? true, :num-tests 1000,
                                   :time-elapsed-ms 431, :seed 1622747931886},
     :sym company.core/introduce})

The `:fn` keyword can be used to provide a function for performing additional
checks. The `employee-exists` function gets both the arguments (`args`) and the
return value (`ret`) of the instrumented function as arguments. The employee's
name is extracted, and it is checked, if that name is contained in the return
value:

    (defn employee-exists [{:keys [args ret]}]
      (let [employee (-&gt; args :employee :name)]
        (not (neg? (.indexOf ret employee)))))

    (s/fdef introduce
            :args (s/cat :employee :company.employees/employee)
            :ret (s/and string?
                        (partial re-find #&quot;Hello, my name is &quot;)
                        (partial re-find #&quot;I'm &quot;)
                        (partial re-find #&quot; years old.&quot;))
            :fn employee-exists)

When dealing with keyword specs, double-check that there are no typos. Misnamed
keywords won't be validated by a spec.

# Interoperating with Java

Since Clojure is based on the Java Virtual Machine, Java code can be used
directly from Clojure using its interoperation facilities (_interop_). The
Clojure REPL is also a great tool to explore Java APIs.

Java offers the class `java.io.File`, which abstracts the concept of a file.
The [JavaDoc](https://docs.oracle.com/javase/8/docs/api/java/io/File.html) shows
that there is a constructor expecting a `pathname`. A file can, thus, created as
follows (note the additional dot after `java.io.File`).

    &gt; (def employees-file (java.io.File. &quot;employees.txt&quot;))

Methods of the created `File` instance can be called with a dot in front of the
method name:

    &gt; (.exists employees-file)
    false
    &gt; (.getAbsolutePath employees-file)
    &quot;/home/patrick/employees.txt&quot;

Some classes, such as `java.awt.Rectangle`, offer public fields, which can be
accessed by prepending a dot and a minus:

    &gt; (def rect (java.awt.Rectangle. 0 0 15 25))
    &gt; (.-width rect)
    15
    &gt; (.-height rect)
    25

When refering to a class repeatedly, typing out the fully qualified class name
becomes tedious. Therefore, classes can be imported. Use `import` from the REPL:

    &gt; (import java.io.File)

Or `:import` of `ns` within a `.clj` source file:

    (:ns company.core
      (:import java.io.File))

Once imported, `File` can be used as follows:

    (def backup (File. &quot;backup.txt&quot;))

To import multiple classes from the same package, just put them into a list
separated by space (which must be quoted in the REPL):

    &gt; (import '(java.io File InputStream))

And without quotation from a `.clj` source file:

    (:ns company.core
      (:import (java.io File InputStream))

Classes from the package `java.lang` are automatically imported.

Static fields and methods can be accessed using a forward slash:

    &gt; (def dump (File. (str &quot;data&quot; File/separator &quot;dump.txt&quot;)))
    &gt; (.getAbsolutePath dump)
    &quot;/home/patrick/data/dump.txt&quot;

    &gt; (def temp (File/createTempFile &quot;employees&quot; &quot;.txt&quot;))
    &gt; (.getAbsolutePath temp)
    &quot;/tmp/employees6472489788961466629.txt&quot;

Java libraries can be used just like Clojure libraries. Let's import the Gson
library for reading and writing JSON to the `company` project (`project.clj`):

    :dependencies [[org.clojure/clojure &quot;1.10.1&quot;]
                   [org.clojure/test.check &quot;1.1.0&quot;]
                   [com.google.code.gson/gson &quot;2.8.0&quot;]] ; new import

The library can be used—and explored—from the REPL:

    $ lein repl

First, the `Gson` class needs to be imported (here, tab completion comes in
handy to figure out the package structure):

    &gt; (import com.google.gson.Gson)

Second, a `Gson` object needs to be initialized:

    &gt; (def gson-obj  (Gson.))

Now Clojure values can be turned into JSON strings:

    &gt; (.toJson gson-obj 42)
    &quot;42&quot;

    &gt; (.toJson gson-obj [7 14 21 28])
    &quot;[7,14,21,28]&quot;

    &gt; (def employees [{:name &quot;Dilbert&quot; :age 42 :job &quot;Engineer&quot; :salary 120000}
                      {:name &quot;Alice&quot; :age 37 :job &quot;Engineer&quot; :salary 115000}
                      {:name &quot;Wally&quot; :age 47 :job &quot;Engineer&quot; :salary 130000}])
    &gt; (.toJson gson-obj employees)
    &quot;[{\&quot;:name\&quot;:\&quot;Dilbert\&quot;,\&quot;:age\&quot;:42,\&quot;:job\&quot;:\&quot;Engineer\&quot;,\&quot;:salary\&quot;:120000},
      {\&quot;:name\&quot;:\&quot;Alice\&quot;,\&quot;:age\&quot;:37,\&quot;:job\&quot;:\&quot;Engineer\&quot;,\&quot;:salary\&quot;:115000},
      {\&quot;:name\&quot;:\&quot;Wally\&quot;,\&quot;:age\&quot;:47,\&quot;:job\&quot;:\&quot;Engineer\&quot;,\&quot;:salary\&quot;:130000}]&quot;

Even though interoperability between Clojure and Java works almost seamlessly, a
few differences have to be considered.

First, a Java method is not a function, and, thus, cannot be bound like a
Clojure functon to a symbol:

    &gt; (.count [1 2 3])
    3
    &gt; (def count-method .count)
    Syntax error compiling at (/tmp/form-init2910488673606898294.clj:1:1).
    Unable to resolve symbol: .count in this context

However, it is possible to turn a method into a function using `memfn`:

    &gt; (def count-method (memfn count))
    &gt; (count-method [1 2 3])
    3

This is helpful when dealing with higher-order functions, such as `map`:

    &gt; (def files [(File. &quot;source.txt&quot;) (File. &quot;target.txt&quot;)]) 
    &gt; (map (memfn exists) files)
    (false false)

Second, many Java objects are mutable, which might be surprising after dealing
with Clojure's immutable collections:

    &gt; (import java.util.Vector)
    &gt; (def employees (java.util.Vector.))
    &gt; (.addElement employees &quot;Dilbert&quot;)
    &gt; (.addElement employees &quot;Alice&quot;)
    &gt; (.addElement employees &quot;Wally&quot;)
    &gt; employees
    [&quot;Dilbert&quot; &quot;Alice&quot; &quot;Wally&quot;]

Rather than returning a vector with the element added, `nil` is returned, and
the element added as a side effect. Use Clojure's collections instead, unless
mutability is needed.

# Threads, Promises, and Futures

Threads are a very powerful, but also dangerous tool, especially when used in
the context of mutable values. At runtime, a Clojure program is a Java program,
and every Java program runs a single main thread. Additional threads can be
started by using Java's `Thread` class:

    &gt; (defn say-hello [] (println &quot;Hello!&quot;))
    &gt; (def thread (Thread. say-hello))
    &gt; (.start thread)
    Hello!

A Clojure function `say-hello` is defined, and a thread is created based on that
function (which is a `Runnable`). Once the thread is started, the function is
executed.

The effect of multiple threads running can be shown by putting one thread to
sleep for a while:

    &gt; (defn say-hello []
        (println &quot;Hello, once!&quot;)
        (Thread/sleep 1000)
        (println &quot;Hello, again!&quot;))
    &gt; (def thread (Thread. say-hello))
    &gt; (.start thread)
    Hello, once!
    ;; waiting for a second, the REPL is not blocking...
    Hello, again!

When multiple threads are running, program execution gets non-deterministic,
unless control mechanisms are put in place. Let's consider these two functions
working on a shared variable:

    (def employee-of-the-month &quot;Dilbert&quot;)
    (defn make-alice-eom []
      (def employee-of-the-month &quot;Alice&quot;))
    (defn make-wally-eom []
      (def employee-of-the-month &quot;Wally&quot;))

The behaviour is completely deterministic when the functions are executed one
after another:

    &gt; (make-alice-eom)
    &gt; (make-wally-eom)
    employee-of-the-month
    &quot;Wally&quot;

However, when the functions are executed in separate threads, the result depends
on mere chance:

    (defn race-condition []
        (let [thread-alice (Thread. make-alice-eom)
              thread-wally (Thread. make-wally-eom)]
          (.start thread-alice)
          (.start thread-wally)))

    &gt; (race-condition)
    &gt; employee-of-the-month
    &quot;Wally&quot;

Even though the thread started later will usually finish later in this example,
no such guarantee can be given. Not modifying shared variables and using
immutable data structures helps to avoid such race conditions. Notice that
dynamic vars live in thread local storage, and, thus, can be used safely from
different threads.

Some threads do their work in the background, but we need to make sure that they
can finish their work before the main thread finishes using `join`:

    (defn delete-cache []
      (.delete (java.io.File. &quot;/tmp/cache.txt&quot;)))

    &gt; (def delete-thread (Thread. delete-cache))
    &gt; (.start delete-thread)
    &gt; (.join delete-thread)

`join` waits until the thread is finished and then returns `nil`.

It is often useful to get back a result from a computation performed in a
thread. A _promise_ is some kind of a value trap that will deliver a value when
demanded. A promise is created using the `promise` function:

    &gt; (def the-result (promise))

A value is delivered using the `deliver` function:

    &gt; (deliver the-result &quot;Dilbert&quot;)

The value then can be grabbed using `deref` or using `@`:

    &gt; (println &quot;The result is:&quot; (deref the-result))
    The result is: Dilbert
    &gt; (println &quot;The result is:&quot; @the-result)
    The result is: Dilbert

Once set, the value of a promise can't be changed again; the value trap was shut!

Promises are useful in the context of multiple threads. Consider this employee
database, to which two functions shall be applied concurrently:

    (def employees [{:name &quot;Dilbert&quot; :age 42 :salary 120000}
                    {:name &quot;Ashok&quot;   :age 27 :salary  18000}
                    {:name &quot;Topper&quot;  :age 37 :salary 250000}])

    (defn average-age [employees]
      (float (/ (reduce + (map :age employees)) (count employees))))

    (defn average-salary [employees]
      (float (/ (reduce + (map :salary employees)) (count employees))))

The two calculations can be executed in separate threads and deliver their
results using a promise. The parallel processing (given multiple CPUs) might
come in handy as the database grows:

    (defn perform-calculations [employees]
      (let [age-prom (promise)
            pay-prom (promise)]
        (.start (Thread. #(deliver age-prom (average-age employees))))
        (.start (Thread. #(deliver pay-prom (average-salary employees))))
        (println &quot;Average age:&quot; @age-prom)
        (println &quot;Average salary:&quot; @pay-prom)))

    &gt; (perform-calculations employees)
    Average age: 35.333332
    Average salary: 129333.336

No `join` is needed, since the threads are done after delivering their values.

This whole process can be simplified by using a _future_, which is a promise
that brings its own thread along:

    (defn perform-calculations [employees]
      (let [age-future (future (average-age employees))
            pay-future (future (average-salary employees))]
        (println &quot;Average age:&quot; @age-future)
        (println &quot;Average salary:&quot; @pay-future)))

    &gt; (perform-calculations employees)
    Average age: 35.333332
    Average salary: 129333.336

A function call wrapped using `future` will be executed in its own thread and
deliver its return value to be grabbed using `deref` or `@`.

In general, prefer futures over promises, because they don't require dealign
with threads directly. For more fine-grained control, consider using Java's
thread-pool facilities (`java.util.concurrent.Executors`).

In practice, it is often a sensible approach to provide a timeout (here: 500
milliseconds) and a fallback value (here: the keyword `:timeout` with a
promise):

    (deref calc-prom 500 :timeout)

`pmap` is a function that, from the outside, works like `map`, but, on the
inside, uses multiple threads to process the elements in parallel (hence the
name; _parallel_ map). Parallel processing comes with some performance overhead
and should only be used if it brings a net performance win.

# State

The less mutable state a program has, the easier it is to understand. However,
modeling things that do change over time requires state.

Consider this employee database with a `hire` to add new employees to it:

    (def employees [{:name &quot;Dilbert&quot; :job &quot;Engineer&quot; :salary 120000}
                    {:name &quot;Alice&quot; :job &quot;Engineer&quot; :salary 110000}
                    {:name &quot;Dogbert&quot; :job &quot;Consultant&quot; :salary 250000}])

    (defn hire [employee]
      (conj employees employee))

    &gt; (hire {:name &quot;Wally&quot; :job &quot;Engineer&quot; :salary 130000})
    [{:name &quot;Dilbert&quot;, :job &quot;Engineer&quot;, :salary 120000}
     {:name &quot;Alice&quot;, :job &quot;Engineer&quot;, :salary 110000}
     {:name &quot;Dogbert&quot;, :job &quot;Consultant&quot;, :salary 250000}
     {:name &quot;Wally&quot;, :job &quot;Engineer&quot;, :salary 130000}]

    &gt; employees
    [{:name &quot;Dilbert&quot;, :job &quot;Engineer&quot;, :salary 120000}
     {:name &quot;Alice&quot;, :job &quot;Engineer&quot;, :salary 110000}
     {:name &quot;Dogbert&quot;, :job &quot;Consultant&quot;, :salary 250000}]

The function produces a new list with the additional employee, but the original
list of employees remains the same.

The state of the `employees` vector _could_ be changed using `def` (notice the
exclamation mark in `hire!` to denote the side-effect):

    (defn hire! [employee]
      (def employees (conj employees employee)))

    &gt; (hire! {:name &quot;Wally&quot; :job &quot;Engineer&quot; :salary 130000})
    &gt; employees
    [{:name &quot;Dilbert&quot;, :job &quot;Engineer&quot;, :salary 120000}
     {:name &quot;Alice&quot;, :job &quot;Engineer&quot;, :salary 110000}
     {:name &quot;Dogbert&quot;, :job &quot;Consultant&quot;, :salary 250000}
     {:name &quot;Wally&quot;, :job &quot;Engineer&quot;, :salary 130000}]

The `employees` vector was updated, however, _not in a thread-safe manner_ (see
previous chapter).

Thread-safe state change can be achieved by using _atoms_, which wraps the value
to be changed:

    (def employees 
      (atom [{:name &quot;Dilbert&quot; :job &quot;Engineer&quot; :salary 120000}
             {:name &quot;Alice&quot; :job &quot;Engineer&quot; :salary 110000}
             {:name &quot;Dogbert&quot; :job &quot;Consultant&quot; :salary 250000}]))

    (defn hire! [employee]
      (swap! employees #(conj % employee)))

The `atom` function wraps the `employees` vector, which then can be updated in a
thread-safe manner using the `swap!`, which takes two arguments: first, the atom
to be updated (`employees`), second, a function to be applied to produce the new
value to be stored in the atom. Note that `employees` is no longer a vector, but
a vector wrapped in an atom.

The employees database can now be updated thread-safely in place:

    &gt; (hire! {:name &quot;Wally&quot; :job &quot;Engineer&quot; :salary 130000})

To access the value wrapped in the atom, use `deref` or `@` (as with promises
and futures):

    &gt; (deref employees)
    [{:name &quot;Dilbert&quot;, :job &quot;Engineer&quot;, :salary 120000}
     {:name &quot;Alice&quot;, :job &quot;Engineer&quot;, :salary 110000}
     {:name &quot;Dogbert&quot;, :job &quot;Consultant&quot;, :salary 250000}
     {:name &quot;Wally&quot;, :job &quot;Engineer&quot;, :salary 130000}] ; new entry

    &gt; @employees
    [{:name &quot;Dilbert&quot;, :job &quot;Engineer&quot;, :salary 120000}
     {:name &quot;Alice&quot;, :job &quot;Engineer&quot;, :salary 110000}
     {:name &quot;Dogbert&quot;, :job &quot;Consultant&quot;, :salary 250000}
     {:name &quot;Wally&quot;, :job &quot;Engineer&quot;, :salary 130000}] ; new entry

Any value can be wrapped in an atom, consider this counter:

    (def counter (atom 0))

    (defn increase-counter! [amount]
      (swap! counter + amount))

The `swap!` function applies the `+` function to the `counter` atom. The
`amount` value is handed over to the `+` function by `swap!`:

    &gt; (increase-counter! 1)
    &gt; @counter
    1
    &gt; (increase-counter! 5)
    &gt; @counter
    6

When an atom is updated using `swap!`, it performs the following steps to
guarantee thread safety:

1. The current value of the atom is read.
2. The update function is called to produce the new value.
3. The current value of the atom is read _again_, and compared to the value read
   previously.
    - If the value _did not change_ in the meantime, the value is updated.
    - If the value _did change_ in the meantime, the whole process is repeated
      from the first step.

Note that the update function can be called multiple times for a single update!
Therefore it's important, that the update function has no side effects!

Sometimes, there are multiple values that need to be synchronized. Consider this
empty employee database, for which also the total number of employees, and the
total salary needs to be kept track of:

    (def employees (atom []))

    (def total-payroll (atom 0))

    (def total-staff (atom 0))

The `hire!` function tries to keep track of those three atoms:

    (defn hire! [employee]
      (swap! employees #(conj % employee))
      (swap! total-payroll #(+ (:salary employee)))
      (swap! total-staff inc))

    &gt; (hire! {:name &quot;Dogbert&quot; :salary 250000})
    &gt; (hire! {:name &quot;Dilbert&quot; :salary 120000})

    &gt; @employees
    [{:name &quot;Dogbert&quot;, :salary 250000}
     {:name &quot;Dilbert&quot;, :salary 120000}]
    &gt; @total-payroll
    370000
    &gt; @total-staff
    2

This seems to work fine, but the atoms are out of sync _between_ the calls to
`swap!`:

    (defn hire! [employee]
      (swap! employees #(conj % employee))
      ; out of sync
      (swap! total-payroll #(+ (:salary employee)))
      ; out of sync
      (swap! total-staff inc))

The three values must be updated either _all together_ or _not at all_, like an
_atomic_ database transaction.

Such groups of atoms can be managed as _refs_, which are a lot like atoms, but
use different functions. First, `ref` is used instead of `atom` for the
wrapping:

    (def employees (ref []))

    (def total-payroll (ref 0))

    (def total-staff (ref 0))

Second, `alter` is used instread of `swap!`. And, third, all the updates
belonging to the same transaction are grouped together using `dosync`:

    (defn hire! [employee]
      (dosync
        (alter employees #(conj % employee))
        (alter total-payroll #(+ % (:salary employee)))
        (alter total-staff inc)))

Now all the values are kept perfectly in sync. When read, all the three values
being altered within the same call to `dosync` will either yield all the old or
all the new values:

    &gt; (hire! {:name &quot;Catbert&quot; :salary 180000})
    &gt; (hire! {:name &quot;Alice&quot; :salary 120000})
    &gt; @employees
    [{:name &quot;Catbert&quot;, :salary 180000}
     {:name &quot;Alice&quot;, :salary 120000}]
    &gt; @total-payroll
    300000
    &gt; @total-staff
    2

If the old value of a `ref` is not of interest, use `ref-set` instead of
`alter`, providing just the new value.

Sometimes, modifications to values should be accompanied by some side effects,
say, writing changes to a file when new items are added. Consider the function
`notify-new-hire` together with the old version of `hire!` working on a single
atom. Every time a new employee is hired, `notify-new-hire` is called:

    (def employees (atom []))

    (defn notify-new-hire [employee]
      (println &quot;Watch out for&quot; (:name employee)))

    (defn hire! [employee]
      (notify-new-hire employee)
      (swap! employees #(conj % employee)))

    &gt; (hire! {:name &quot;Alice&quot; :salary 120000})
    Watch out for Alice

This works fine—until an update has to be retried because the values have been
modified in between. In this case, `notify-new-hire` would be executed multiple
times, which is not wanted.

An _agent_ is an atom that can be combined with side-effects. Instead of calling
`swap!`, the function `send` is used to both update the agent, and to produce
the side effect:

    (def employees (agent []))

    (defn notify-new-hire [employee]
      (println &quot;Watch out for&quot; (:name employee)))

    (defn hire! [employee]
      (send employees
            (fn [old-employees]
              (notify-new-hire employee)
              (conj old-employees employee))))

Notice that an anonymous function (`fn`) has been used instead of a lambda
expression. Every agent has its own queue of functions. When `hire!` gets
called, the call to the anonymous function gets queued up. `send` worls
asynchronously, i.e. it returns immediately after the function was put into the
queue. The agent pops an outstanding function call from the queue and executes
it. The side effect (calling `notify-new-hire`) and the update are then
performed:

    &gt; (hire! {:name &quot;Ratbert&quot; :salary 0})
    Watch out for Ratbert
    &gt; (hire! {:name &quot;Alice&quot; :salary 115000})
    Watch out for Alice
    &gt; @employees
    [{:name &quot;Ratbert&quot;, :salary 0}
     {:name &quot;Alice&quot;, :salary 115000}]

Since agents perform their updates in their own thread, exceptions caused by a
failed update are not reported immediately. Consider this agent for tracking
donations:

    (def donations (agent 0))

    (defn praise-donor [amount donor]
      (println &quot;Praise&quot; donor &quot;for donating&quot; amount &quot;coins!&quot;))

    (defn donate! [amount donor]
      (send donations
            (fn [old-donations]
              (praise-donor amount donor)
              (+ old-donations amount))))

    &gt; (donate! 100 &quot;John&quot;)
    Praise John for donating 100 coins!
    &gt; @donations
    0

If the arguments `amount` (integer) and `donor` (string) are swapped, the
operation cannot be completed:

    &gt; (donate! &quot;Jane&quot; 200)
    Praise 200 for donating Jane coins!
    &gt; @donations
    100

The message from the side effect looks suspicious, and the `donations` haven't
been increased. But the error can go unnoticed until the next update is done:

    &gt; (donate! 300 &quot;Jim&quot;)
    Execution error (ClassCastException) at user/donate!$fn (REPL:5).
    java.lang.String cannot be cast to java.lang.Number

In this case, the agent has to be re-started using `agent-restart`. The error
condition can be detected using `agent-error`:

    (if (agent-error donations)
      (restart-agent donations 0 :clear-actions true))

However, the value managed by the agent is reset to 0 in the process.

Make sure to call `shutdown-agents` at the end of the `-main` function of your
program, so that the agent's threads get properly terminated:

    (defn -main []
      ;; working with agents
      (shutdown-agents))

Use the following guidelines to pick the proper mechanism for your state
changes:

1. If a value remains _mostly stable_, put it into a `var`.
2. If a number of values need to be updated together without side effects, use
   `ref`s.
3. If the update of values is to be accompanied by side effects, or if the
   update function takes a lot of time, use an `agent`.
4. If you have a single value that changes without additional side effects, us
   an `atom`.

Instead of using `ref`s, the values to be updated could also be grouped into a
single data structure, which is then wrapped in an `atom`:

    (def payroll (atom {:total-staff 0
                        :total-payroll 0
                        :employees []}))

    (defn hire! [employee]
      (swap! payroll
             (fn [old]
               (assoc old
                      :total-staff (inc (:total-staff old))
                      :total-payroll (+ (:total-payroll old) (:salary employee))
                      :employees (conj (:employees old) employee)))))

    &gt; (hire! {:name &quot;Alice&quot; :salary 115000})
    &gt; (hire! {:name &quot;Dogbert&quot; :salary 250000})
    &gt; @payroll
    {:total-staff 2,
     :total-payroll 365000,
     :employees [{:name &quot;Alice&quot;, :salary 115000}
                 {:name &quot;Dogbert&quot;, :salary 250000}]}

There are various functions in Clojure making use of atoms, e.g. `memoize`,
which wraps a function with a cache that maps the arguments to the computed
return values. Consider this Fibonacci function:

    (defn fib [n]
      (println &quot;Computing Fibonacci number for&quot; n)
      (cond
        (= n 0) 1
        (= n 1) 1
        (&gt; n 1) (+ (fib (- n 1)) (fib (- n 2)))
        :else (throw (ex-info &quot;fib(n) only defined for n &gt;= 0&quot;))))

    &gt; (fib 1)
    Computing Fibonacci number for 1
    1

    &gt; (fib 2)
    Computing Fibonacci number for 2
    Computing Fibonacci number for 1
    Computing Fibonacci number for 0
    2

    &gt; (fib 3)
    Computing Fibonacci number for 4
    Computing Fibonacci number for 3
    Computing Fibonacci number for 2
    Computing Fibonacci number for 1
    Computing Fibonacci number for 0
    Computing Fibonacci number for 1
    Computing Fibonacci number for 2
    Computing Fibonacci number for 1
    Computing Fibonacci number for 0
    5

The function is called with the same argument multiple times, which slows down
the computation for bigger `n`.

`memoize` wraps the function so that it caches its return values by argument:

    &gt; (def fib (memoize fib))
    &gt; (fib 1)
    Computing Fibonacci number for 1
    1
    &gt; (fib 2)
    Computing Fibonacci number for 2
    Computing Fibonacci number for 0
    2
    &gt; (fib 3)
    Computing Fibonacci number for 3
    3
    &gt; (fib 4)
    Computing Fibonacci number for 4
    5
    &gt; (fib 10)
    Computing Fibonacci number for 10
    Computing Fibonacci number for 9
    Computing Fibonacci number for 8
    Computing Fibonacci number for 7
    Computing Fibonacci number for 6
    Computing Fibonacci number for 5
    89

The function returned by `memoize` has its own atom, which serves as a cache for
the results having been computed.

# Read and Eval

Clojure's syntax might look strange on the first sight, but is crucial for how
the language works. There are two critical functions in Clojure—`read` and
`eval`—that relate to Clojure's syntax.

Clojure code looks a lot like data literals:

    '(dilbert pointy-haired-boss [alice]
       (wally ratbert
         (dogbert &quot;some characters from dilbert...&quot;)))

Just replace the Dilbert character names by some other symbols, and almost have
Clojure code:

    '(defn say-hello [friendly]
       (if friendly
         (println &quot;Hello, my dear!&quot;)))

Just remove the quote, and you have an executable Clojure function.

    (defn say-hello [friendly]
      (if friendly
        (println &quot;Hello, my dear!&quot;)))

    &gt; (say-hello true)
    Hello, my dear!

Clojure code _is_ just Clojure data, and Clojure function calls _are_ lists.
Clojure is _homoiconic_, which means, code and data are the same thing.

The `read` function reads data, by default from the REPL (`stdin`), until
`[Return]` is entered:

    &gt; (read)
    1
    1
    &gt; (read)
    &quot;hello&quot;
    &quot;hello&quot;
    &gt; (read)
    (defn say-hi []
      (println &quot;Hi!&quot;))
    (defn say-hi [] (println &quot;Hi!&quot;))

The `read` function just returns returns the expression it read.

`read-string` reads a string and turns it into a Clojure value (notice the
escaped double quotes around the text `Hi`):

    &gt; (read-string &quot;(defn say-hi [] (println \&quot;Hi!\&quot;))&quot;
    (defn say-hi [] (println &quot;Hi!&quot;))

Once data has been read (from whatever source), it can be evaluated as Clojure
code using the `eval` function:

    &gt; (def some-function-call '(+ 2 3))
    &gt; (eval some-function-call)
    5

The code in the quoted list `(+ 2 3)` is compiled and run as Clojure code.

Some data types, like numbers, strings, and keywords, just evaluate to
themselves:

    &gt; (eval 42)
    42
    &gt; (eval &quot;Dilbert&quot;)
    &quot;Dilbert&quot;
    &gt; (eval :salary)
    :salary

A Clojure function consists of different data structures—lists, vectors—which
can all be made up as data, and then be combined to a list, which makes up an
actual function that can be evaluated:

    &gt; (def function-name 'say-hi)
    &gt; (def args (vector 'to-whom))
    &gt; (def output (list 'println &quot;Hi,&quot; 'to-whom))
    &gt; (def whole-function (list 'defn function-name args output))

    &gt; (eval whole-function) ; creates function say-hi
    &gt; (say-hi &quot;Joe&quot;)
    Hi, Joe

The two functions `read` and `eval` combined can be used to read Clojure code
from an external source, say, user-specific settings from a config file. No
extra language or syntax has to be made up: The full power of Clojure is
supported out of the box.

It's also possible to define a very simple REPL using `read` and `eval`:

    (defn my-repl []
      (loop []
        (println (eval (read)))
        (recur)))

    &gt; (my-repl)
    (println &quot;Hello&quot;)
    Hello
    (println (+ 3 2))
    5
    ; Hit Ctrl-D to finish

A toy version of the function `eval` can be implemented as an ordinary Clojure
function:

    (defn my-eval [expr]
      (cond
        (string? expr) expr
        (keyword? expr) expr
        (number? expr) expr
        (symbol? expr) (my-eval-symbol expr)
        (vector? expr) (my-eval-vector expr)
        (list? expr) (my-eval-list expr)
        :else :unknown-expression))

Strings, keywords, and numbers just evaluate to themselves, so the `expr` is
just returned.

Symbols need to be looked up in the current environment:

    (defn my-eval-symbol [expr]
      (.get (ns-resolve *ns* expr)))

Vectors needs to be processed recursively:

    (defn my-eval-vector [expr]
      (vec (map my-eval expr)))

And lists, which can be function calls, need to be separated into the function
name `f` and the argument list `args`, which are then applied using `apply`:

    (defn my-eval-list [expr]
      (let [evaled-items (map my-eval expr)
            f (first evaled-items)
            args (rest evaled-items)]
        (apply f args)))

    &gt; (my-eval '(println &quot;Hello&quot;))
    Hello

A real `eval` function _compiles_ the given expression to Clojure code, which
then can be executed a lot faster. Therefore, `eval` is not to be used as an
everyday tool, but only for special cases: it is just _too powerful_ and
therefore _too dangerous_. It also is slower than regular Clojure code, due to
the additional compilation step.

Reading in code from arbitrary sources can also be very dangerous. Use the
`read` function from `clojure.edn` if you don't trust the source. But `read` is
also not a tool for everyday use, so only use it if really needed.

# Macros

Code is both useful and painful: it solves problems, but also creates new
problems of its own. Writing more _expressive_ code leads to _less_ code. The
usefulnes of code is kept, but the pain is reduced; _less code, less pain_.

_Macros_ are a powerful tool in LISP-like languages (such as Clojure) to
automate some part of code writing.

Consider a rating system in which numbers are interpreted in three categories:

- positive numbers: good
- zero: indifferent
- negative numbers: bad

This rating system could be used to print a depiction of the rating in English:

    (defn print-rating [rating]
      (cond
        (pos? rating) (println &quot;good&quot;)
        (zero? rating) (println &quot;indifferent&quot;)
        :else (println &quot;bad&quot;)))

    &gt; (print-rating 3)
    good
    &gt; (print-rating 0)
    indifferent
    &gt; (print-rating -5)
    bad

Another implementation could be required to turn the rating number into a
keyword for further programmatic processing:

    (defn evaluate-rating [rating]
      (cond
        (pos? rating) :good
        (zero? rating) :indifferent
        :else :bad))

    &gt; (evaluate-rating 3)
    :good
    &gt; (evaluate-rating 0)
    :indifferent
    &gt; (evaluate-rating -1)
    :bad

Structurally, the two usages of `cond` are identical, they just have different
consequences. The commonalities of the two functions could be factored out into
a new function, `arithmetic-if`:

    (defn arithmetic-if [n pos zero neg]
      (cond
        (pos? n) pos
        (zero? n) zero
        (neg? n) neg))

The function accepts both the number `n` to be categorized, and the three
possible _consequences_ of a match: `pos`, `zero`, and `neg`.

This works great, if a value has to be returned, as in `evaluate-rating`, which
can be refactored in terms of `arithmetic-if`:

    (defn evaluate-rating [rating]
      (arithmetic-if rating :good :indifferent :bad))

    &gt; (evaluate-rating 5)
    :good
    &gt; (evaluate-rating 0)
    :indifferent
    &gt; (evaluate-rating -7)
    :bad

The same refactoring applied to `print-rating`, however, produces surprising
results:

    (defn print-rating [rating]
      (arithmetic-if rating
                     (println &quot;good&quot;)
                     (println &quot;indifferent&quot;)
                     (println &quot;bad&quot;)))

    &gt; (print-rating 4)
    good
    indifferent
    bad
    &gt; (print-rating 0)
    good
    indifferent
    bad
    &gt; (print-rating -2)
    good
    indifferent
    bad

All three `println` function calls are executed! When `arithmetic-if` is
invoked, the arguments get evaluated. This is unproblematic for `n`, which is a
number and, therefore, evaluates to itself. Function calls like `println`,
however, are evaluated by their actual execution.

If `arithmetic-if` expected functions as its last three parameters,
`print-rating` could be implemented based on the former:

    (defn arithmetic-if [n pos-f zero-f neg-f]
      (cond
        (pos? n) (pos-f)
        (zero? n) (zero-f)
        (neg? n) (neg-f)))

    (defn print-rating [rating]
      (arithmetic-if rating
                     #(println &quot;good&quot;)
                     #(println &quot;indifferent&quot;)
                     #(println &quot;bad&quot;)))

    &gt; (print-rating 4)
    good
    &gt; (print-rating 0)
    indifferent
    &gt; (print-rating -2)
    bad

However, the implementation of `evaluate-rating` now becomes more complicated,
thwarting the gains made using `arithmetic-if` as an abstraction:

    (defn evaluate-rating [rating]
      (arithmetic-if rating
                     #(identity :good)
                     #(identity :indifferent)
                     #(identity :bad)))

    &gt; (evaluate-rating 7)
    :good
    &gt; (evaluate-rating 0)
    :indifferent
    &gt; (evaluate-rating -5)
    :bad

What `arithmetic-if` is really supposed to do is to transform code written like
this:

    ;; evaluate-rating
    (arithmetic-if rating
      :good
      :indifferent
      :bad)

    ;; print-rating
    (arithmetic-if rating
      (println &quot;good&quot;)
      (println &quot;indifferent&quot;)
      (println &quot;bad&quot;))

Into code being executed like this:

    ;; evaluate-rating
    (cond
      (pos? rating) :good
      (zero? rating) :indifferent
      :else :bad)

    ;; print-rating
    (cond
      (pos? rating) (println &quot;good&quot;)
      (zero? rating) (println &quot;indifferent&quot;)
      :else (println &quot;bad&quot;))

Since Clojure code _is_ just data, this transformation can be made using a
function building up another function:

    (defn arithmetic-if-to-cond [n pos zero neg]
      (list 'cond (list 'pos? n) pos
                  (list 'zero? n) zero
                  :else neg))

Fed with parameters protected with a single quote from evaluation, this function
produces the desired `cond` forms:

    &gt; (arithmetic-if-to-cond 'rating
                             '(println &quot;good&quot;)
                             '(println &quot;indifferent&quot;)
                             '(println &quot;bad&quot;))
    (cond
      (pos? rating) (println &quot;good&quot;)
      (zero? rating) (println &quot;indifferent&quot;)
      :else (println &quot;bad&quot;))

    &gt; (arithmetic-if-to-cond 'rating
                             ':good
                             ':indifferent
                             ':bad)
    (cond
      (pos? rating) :good
      (zero? rating) :indifferent
      :else :bad)

However, those `cond` forms are _still just data_, and not compiled code that
actually can be used. Here, macros come into play, which are defined using
`defmacro`:

    (defmacro arithmetic-if [n pos zero neg]
      (list 'cond (list 'pos? n) pos
                  (list 'zero? n) zero
                  :else neg))

`print-rating` can now be implemented without using lambdas or quotation:

    (defn print-rating [rating]
      (arithmetic-if rating
                     (println &quot;good&quot;)
                     (println &quot;indifferent&quot;)
                     (println &quot;bad&quot;)))

    &gt; (print-rating 4)
    good
    &gt; (print-rating 0)
    indifferent
    &gt; (print-rating -2)
    bad

Which is also the case for `evaluate-rating`:

    (defn evaluate-rating [rating]
      (arithmetic-if rating
                     :good
                     :indifferent
                     :bad))

    &gt; (evaluate-rating 7)
    :good
    &gt; (evaluate-rating 0)
    :indifferent
    &gt; (evaluate-rating -5)
    :bad

The Clojure compilation process works as follows: First, source code is read,
i.e. turned into data structures (lists, vectors, etc.). Second, _macro
expansion_ is performed, modifying those data structures. Third, those modified
data structures with expanded macros are turned into byte code by the actual
compilation step.

Unlike C, Clojure macros work on the code as a _data structures_, not on code as
mere _program text_, which makes Clojure macros more powerful and _less_
dangerous than C macros.

The `arithmetic-if` macro from above requires a lot of quotes to prevent the
expressions from being evaluated. Clojure provides a templating system called
_syntax quoting_, which makes macros more readable:

    (defmacro arithmetic-if [n pos zero neg]
      `(cond
         (pos? ~n) ~pos
         (zero? ~n) ~zero
         :else ~neg))

Syntax quoting starts with a backquote (before `cond`). Within the quoted form,
expressions from the outside are referred to using a tilde prefix, which
prevents them from being evalauted when the macro is expanded before compilation.

There are a few other syntax specialities when it comes to using macros with
syntax quoting. Consider the `conjunction` macro, which works like
`and`—&quot;conjunction&quot; just being a fancy word for &quot;and&quot;:

    (defmacro conjunction
      ([] true)
      ([x] x)
      ([x &amp; next]
       `(let [current# ~x]
          (if current# (conjunction ~@next) current#))))

The macro works as follows:

1. For an empty list of conditions, `true` is returned (first base case).
2. For a single condition, the evaluated condition is returned (second base
   case).
3. For a list of more than one condition (general case), the following logic is
   applied:
    1. The first condition is bound to `current#`; the suffix `#` being used to
      guarantee a unique symbol.
    2. If the `current#` condition evaluates to `true`, the `conjunction` macro
       is &quot;called&quot; recursively with the next condition in the list. This `next`
       symbol is prefixed both by `~` for syntax quoting, and an `@`—more of
       which later.
    3. Otherwise, if `current#` evaluates to `false`, `current#` itself is
       returned, which terminates the recursive process.

In order to understand the `@` prefix, consider this alternative implementation
of `defn` as a macro called `my-defn`:

    (defmacro my-defn [name args &amp; body]
      `(def ~name (fn ~args ~body)))

A function to add two numbers is created using `my-defn`:

    (my-defn add-two-numbers [a b] (+ a b))

Unfortunately, the function won't work:

    &gt; (add-two-numbers 3 4)
    Execution error (ClassCastException) at user/add-two-numbers (REPL:1).
    java.lang.Long cannot be cast to clojure.lang.IFn

Let's see what code the macro actually generated using `macroexpand-1`, the
first tool to be grabbed if macros not behave as intended:

    &gt; (macroexpand-1 '(my-defn add-two-numbers [a b] (+ a b)))
    (def add-two-numbers (clojure.core/fn [a b] ((+ a b))))

Notice the `((+ a b))` expression being wrapped in two sets of parentheses.
Expression was created by `~body` in the macro template, which is itself a
collection because it was declared as a variadic parameter (`&amp; body`). Thus,
`body` is a list of one element containing another list: `((+ a b))`.

The `@` prefix makes sure that the collection is expanded before being written
into the code. Here's a working version of `my-defn` with this expansion:

    (defmacro my-defn [name args &amp; body]
      `(def ~name (fn ~args ~@body)))

This creates the working code as intended:

    &gt; (macroexpand-1 '(my-defn add-two-numbers [a b] (+ a b)))
    (def add-two-numbers (clojure.core/fn [a b] (+ a b)))

    &gt; (my-defn add-two-numbers [a b] (+ a b))
    &gt; (add-two-numbers 1 2)
    3

Notice that macros are processed in a two-step process: First, they are
_expanded_, second, the generated code is _evaluated_:

    (defmacro two-step-process []
      (println &quot;This code is run upon macro expansion.&quot;)
      `(fn [] (println &quot;This code is run with the generated code.&quot;)))

    &gt; (def generated-code (two-step-process))
    This code is run upon macro expansion.

    &gt; (generated-code)
    This code is run with the generated code.

Also notice that macros do not exist at runtime, so their names can't be found
in a stack trace. Macros also can't be used like a function given to a
higher-order function such as `filter` or `map`. Only use macros if the code to
be expressed is at odds with Clojure's evaluation rules. Stick to functions
otherwise.
</content>
    </entry>
    <entry>
        <title>Gerald M. Weinberg: “How Software Is Built”</title>
        <link href="https://paedubucher.ch/articles/2021-06-04-gerald-m-weinberg-how-software-is-built.html"/>
        <author>
            <name>Patrick Bucher</name>
        </author>
        <id>https://paedubucher.ch/articles/2021-06-04-gerald-m-weinberg-how-software-is-built.html</id>
        <updated>2021-06-04T15:30:00Z</updated>
        <content type="text/markdown; charset=UTF-8">
_This is a rough summary of “How Software Is Built”, Volume 1/6 of the “Quality
Software Management” by Gerald M. Weinberg. Remarks that are not found in the
book are written within [square brackets]._

# Preface

Computers evolve, and in order to not lag farther and farther behind, there are
three fundamental abilities needed: First, one needs to be able to observe, and
to understand the significance of the observed. Second, one needs to act
congruently, especially in difficoult interpersonal situation. And third, one
needs to be able to understand complex situations, to make plans, and to modify
those plans if needed. The third point is the subject of this book

# What Is Quality? Why Is It Important?

Adequate quality to one person may be inadequate quality to another:

&gt; Quality is meeting _some person’s_ requirements.
&gt; Every statement about quality is a statement about some person(s).
&gt; Who is the person behind that statement about quality.

There are different ideas about software quality, held by different groups.
Users want zero defects. Some users and marketers like to have a lot of features.
Developers strive for elegant code. Users and sales value high performance.
Customers and managers like low development costs. Users and marketers prefer
rapid over slow development. And different kinds of users have different ideas
about user-friendliness.

&gt; More quality for one person may mean less quality for another.  Whose opinion
&gt; of quality is to count when making decisions?  Quality is value to some
&gt; person.

Decisions are often hidden from the conscious minds of the person who makes
them. A quality manager must bring those decisions into consciousness. Internal
software organizations have little competition and therefore tend to stagnate.

Improving quality is difficult, there is an up- or downward spiral, where each
stage reinforces the next: First, there is a motivation to measure the cost of
quality. Second, one strives to understand the value of quality. Third,
motivation builds up to achieve quality. And, fourth, and understanding of how
to achieve quality is developed. This circle continues with the first point.

A lock-on effect, e.g. caused by the choice of a programming language, causes
the cost of change to _increase_, but the motivation and knowledge to change to
_decrease_ over time. A lock-on effect for a programming language entails
software tools, hardware systems [though less common nowadays], people trained
and hired, specialized consultants, a user community, managers that grew up with
that particular language, books and trainings, and sometimes an entire
philosophy of software engineering and user interface design.

&gt; People will always choose the familiar over the cofortable.

No two software organizations are 1) exactly alike or 2) entirely different.

There is some common software culture; its properties can be found in the entire
world. Some characteristics cluster together as _patterns_. Organizations lock
in on one of those patterns due to conservatism manifested in:

1. satisfaction with current quality level
2. fear of losing that level when improvements fail
3. no understanding for other cultures
4. invisibility of their own culture

However, improving quality requires cultural change. Resistance can be overcome
by preserving the good that is already there.

# Software Subcultures

The critical factor to software quality is the people involved: their
motivations and reactions.

The “manufacturing” part in software is its duplication; a rather trivial task.
Ideas such as “Zero Defects” are only sensibly applied to the duplication part
of software. The parallel development of requirements software is the critical
part of software quality.

Most software development takes place in a “dirty” environment, where the
requirements cannot be assumed correct. An “economics of quality” (tradeoffs in
terms of correctnes) only exist if there’s a correct set of requirements.

The requirements process can destroy value, e.g. if it is figured out that the
wrong thing was built. Defective software, however, can also provide a lot of
value.

If the customers of a software organization are satisfied, there’s no point in
changing the way that organization works. Mild dissatisfaction is better tackled
using small, gradual improvements rather than cultural change. However, trying
to improve your way out of the wrong pattern using small steps is like creating
a more detailed map of the wrong trip.

&gt; Quality is the ability to consistently get what people need. That means
&gt; producing what people will value and not producing what people won’t value.

Quality patterns should not be denoted in terms of “maturity”, but in a more
neutral way. Any pattern can produce satisfying results. Maturity only works in
one way, but organizations can go back to a different pattern, too. Different
cultural patterns may be more or less _fitting_ to an organization and its
quality needs.

&gt; Things are the way they are because they got that way.

One can learn about processes by observing the products created by them.

Organizations can be classified by their _degree of congruence_ between what is
said and what is done in different parts of the organization. Those patterns are:

- 0 _Oblivious_: “We don’t even know that we’re performing a process.”
    - not a professional pattern, but useful as a baseline
    - most frequent source of new software
    - the software user is the organization that builds the software
    - no managers, no customers, no specified processes
    - no awareness of doing “software development”
    - produces satisfied users nonetheless
- 1 _Variable_: “We do whatever we feel like at the moment.”
    - awareness rises that pattern 0 doesn’t suffice
    - separation from developer and user begins; blaming, too
    - management is not understood as part of development
    - “superprogrammer” (maybe leading a team) as source of success
    - myths about heroic deeds as company history
    - sometimes a pool of developers serve the specialist programmers
    - one or few rockstar programmers do the projects
    - procedures are abondoned at first sign of a crisis
    - improving quality by hiring a star
    - performance, schedule, and costs mostly depend on individual efforts, not
      on teams
    - project results reinforce the belief system (projects succeed and fail
      depending on the individual programmers doing them)
- 2 _Routine_: “We follow our routines (except when we panic).”
    - when “leave the programmer” no longer yields satisfying results
    - “super manager” as the deciding factor (replaces the “super programmer“)
    - procedures are put in place to keep programmers under control
    - procedures are followed, but their reasons are not understood
    - works well until routines are bypassed in a “disaster” project (as if the
      procedures that were followed in successful projects aren’t really trusted)
    - lack of understanding the _why_ behind procedures, managers start issuing
      counterproductive orders (overtime, less training, cutting corners)
    - silver bullets: refined measurements (in unstable environments),
      sophisticated (but not helpful) tools
    - “name magic”: just say “structured programming” [or “agile”] to work magic
- 3 _Steering_: “We choose among our routines by the results they produce.”
    - managers are more skilled and experienced (not just promoted programmers
      with a lack of role-models that do “management by telling“)
    - magic is replaced by understanding
    - procedures not completely defined, but understood; also followed in crisis
    - very few project failures; they always deliver _at least some_ value
    - procedures are flexible, not rigid (“steering” instead of just following a
      plan)
    - programmers actually like to work in a well-managed (!) operation
    - tools are introduced later in the process, but used well
- 4 _Anticipating_: “We establish routines based on our past experience of
  them.”
    - pattern 3 (“Steering”) manager in the higher ranks of management
    - comprehensive process measurements and analysis initiated
- 5 _Congruent_: “Everyone is involved in improving everything all the time.”
    - quality managers on highest level (CEO)
    - procedures are understood, followed, and improved all the time by everyone
      (continuous improvement)

Pattern 1 can look like pattern 3 from the outside: if there’s no effective
management in place, management can’t even cause much harm.

As long as everything goes well, pattern 2 can be mistaken for pattern 3. When
things get in trouble, the differences become obvious.

In practice, patterns 4 and 5 hardly exist.

# What Is Needed To Change Patterns?

The prevailing pattern is best detected by the way people think and communicate:

- Problems are handled by individuals in reactive ways (pattern 1).
- Tools and concepts are used, but in the wrong way (pattern 2; e.g. reasoning
  remains verbal, despite of using “statistics”).
- Linear reasoning (“A caused B”) and unjustifiable certainty in what is known
  prevailing (pattern 2).
- Problems are handled less emotionally; emergencies are handled better; people
  act more proactively (pattern 3).
- Measurement is used, but is sometimes meaningless (process not stable enough
  to gain useful data; pattern 3).
- Processes and measurements are stable; single managers can’t force
  organizations into big mistagkes (pattern 4).
- Scientific reasoning; more problems prevented than handled (pattern 5).

In order to improve the quality of the organization, the quality of thinking
needs to be improved first.

Every pattern has its models (implicit or explicit) that guide the
organizations’s thinking.

Sometimes there is not enough incentive to change patterns, so it makes sense to
remain with the old, sufficient one. However, this is only a concious decision
if the information about incentives and about other patterns is known.

A pattern change might cause more (temporary) costs in some department
(development) in order to save costs in another department (service). Such
change is only possible if the organization supports this change on a higher
level.

The higher the demands posed by customers and the problems itself, the higher a
pattern is needed. There is also a tradeoff: lower demands by the customer
combined with higher demands of the problem itself could be satisfied with the
same pattern.

An organization can remain in a pattern for a long time if:

- customers are not demanding
- problems aren’t getting more demanding
- there’s no competition

Under those circumstances, an organization can even stagnate.

Resistance to change often stems from certain _thinking patterns_:

- circular argument
    - don’t try because you might fail
    - we don’t know if you’d fail, because you don’t try
- classic software cycle
    - we do the best possible job; if others do their job better, their problem
      must be easier
    - consultants have bad development habits and therefore must be isolated
      from internal developers; so we don’t know how they work
    - our rockstar developer is never at fault; if something fails, someone else
      is to blame; so the rockstar’s weaknesses are never found
    - our rockstar developer knows most about software; if alternatives are to
      be investigated, ask our rockstar; so we’ll never use something the
      rockstar doesn’t understand

Those _closed circuits_ can be opened by asking if your rate of success is high
enough. Over time, evidence to the contrary might accumulate. Unfortunately,
patterns 0, 1, and 2, which need change the most, often don’t keep records of
their failures and their cost.

Cultural patterns can be broken by starting the information flowing:

- technical reviews offer insight into the products
- send people to seminars to discover what other people do
- ask upper management:
    - how do you spot failures/poor quality?
    - apply this definition to individual cases

Patterns 0, 1, and 2 are based on a lack of trust:

- pattern 0: we only trust ourselves
- pattern 1: we don’t trust managers
- pattern 2: we don’t trust programmers

Higher patterns are not “more mature”, but “more open”:

- pattern 0: as open as the individual
- pattern 1: open to information exchange between developer and user
- pattern 2: open to information exchange between developer, user, and manager
- pattern 3: open in all directions to information about the product
- pattern 4: open in all directions about the process
- pattern 5: open in all directions about the culture

Creating trustworty sub-systems reduces the amount of communication needed
(“checking up”) and is needed to open up. Trust reduces the need for data;
increasing data flows might indicate trouble. If in trouble, there’s no time to
learn better ways how to develop software (vicious cycle). Past successes create
inertia; a past strenght become a weakness:

- lots of code: a lot of value, a lot to maintain
- past practices: were successful, no need to improve them is seen
- people’s attitudes: worked then, why change?

Any culture must accomplish these tasks:

1. present: keep performing today; don’t slip backwards
2. past: maintain the foundation from yesterday; don’t forget what you know
3. future: build the next pattern to guide the change process

To move to a higher pattern, the following things have to be learned:

- 0 to 1: humility (exposure to what others are doing)
- 1 to 2: ability (technical training and experience)
- 2 to 3: stability (quality software management)
- 3 to 4: agility (tools and techniques)
- 4 to 5: adaptability (human development)

Lockons are strong forces that prevent change (e.g. driving on one particular
side of the road in England vs. Germany).

# Control Patterns for Management

Organizations can remain at pattern 1 or 2, because their problems do not
require them to be elsewhere. However, higher demands require different
patterns, otherwise they experience the grief cycle:

1. denial: control the pain by controlling the information (don’t notice)
2. blaming others
3. tradeoffs
4. realizing that changes are needed

Quality standards and productivity are moving targets: demands are getting
higher all the time. When shooting at moving targets, instruction only helps if
it is general enough (shooting at moving targets in general, not at a specific
moving target in particular).

Moving targets are most likely being hit by firing many bullets (aggregation).
In a big market, many solution bullets are fired at single problems. Within an
organization, developing multiple programs to tackle very critical problems can
be useful, for the redundancy provides means for comparison.

Pattern 2 organizations often unknowingly perform serial aggregation. A failed
project is tried again later.

Pattern 1 organization often produce a lot of redundant tooling, because
programmers are not aware of colleagues facing the same issues.

In pattern 2, measures to “improve efficiency” often work against aggregation by
the means of centralization.

Evaluating different alternatives before bying a software is also aggregation.
This process is often seen in pattern 3.

Aggregating is like shooting with a shotgun; feedback control is like shooting
with a rifle. Cybernetics is the “science of aiming“. In pattern 1, a cybernetic
model starts with the idea of a system to be controlled:

- inputs
    - resources
    - requirements
    - randomness
- outputs
    - software
    - other outputs
        - competence
        - tools developed
        - stronger/weaker teams
        - etc.

The system’s behaviour is governed by the formula:

&gt; Behaviour depends on both state and input.

Thus, the control also depends on what’s going on internally (state).

The model of pattern 1 organizations says:

a. tell us what you want (don’t change your mind)
b. give us some resources (whenever we ask)
c. don’t bother us (eliminate randomness)

For pattern 0, there’s no point a.

In pattern 2, aggregation is done by adding more resources to the system, which
is tightly controlled. The internal state of the development systemd won’t be
affected by the controller’s efforts, only the inputs:

- make them smarter by training, tooling, hiring
- motivate them with cash, more interesting assignments

In pattern 3, the controller can measure performance. Inputs and state must be
connected for feedback control by comparing the desired state to the actual
state.

Pattern 2 erroneously equate “controller” with “manager“. The first law of bad
management:

&gt; When something isn’t working, do more of it.

Managers _are_ controllers, but so is everybody involved in the project.

In pattern 3, management is mostly feedback control:

- plan what should happen
- observe what is happening
- compare planned with observed
- take actions to bring the observed closer to the planned

When pattern 2 organizations try to move to pattern 3, they start to make
observations, but don’t know which ones are useful (false focus on quantity; no
means of measuring quality).

Measuring data (e.g. by doing reviews) doesn’t help unless the controller
propertly acts upon the findings.

Without information, nothing can be controlled for very long. A process must be
stable and yield visual evidence of progress, which is rarely the case in
pattern 2.

Quality software development not only needs “computer science” or “cybernetics“,
byt also an _engineering discipline_:

&gt; the application of scientific principles to practical ends as the design,
&gt; construction, and operation of efficient and economical structures, equipment,
&gt; and systems.

- An organization has the pattern in which it turns out products _consistently_.
- Emphasize what the organization is doing well to inforce that instead of
  confronting the organization with its “sins” all the time.
- Control means, that some error level can be held in check, not that there are
  no errors.
- Pattern 3 looks like pattern 2 from the outside if there are no errors to be
  handled.
- Action is the result of comparing the observed with the planned.
- Aggregation sometimes yields better returns than feedback control for certain
  situations (e.g. handling of emergency cases).
- Pattern 4 uses feedback control to not only to improve the product, but also
  the process.
- Pattern 5 applies feedback control to the entire organization.

# Making Explicit Management Models

A controller must not only have accurate and timely observations, but also
understand those observations (“system models“). One must know: 1) what is
important to observe, and 2) what is the right response to an observation.

I pattern 1 and 2, those system models are implicit, e.g. “more pressure =
faster work” or “bugs occur at random“, and therefore hard to discuss, test and
improve; the organization is stuck in its current pattern, and therefore hard to
discuss, test and improve; the organization is stuck in its current pattern.

A lack of calendar time is not necessarily the cause for a project to fail, but
the reason why other failures are being detected. Fred Brooks rephrased:

&gt; Lack of calendar time has forced more failing software projects to face the
&gt; _reality of their failure_ than all other reasons combined.

Or:

&gt; Lack of calendar time has forced more failing software projects to face the
&gt; _incorrectness of their models_ than all other reasons combined.

Brooks’ failure dynamics (and faulty system model):

- poor estimation techniques (depends on a model, such as “all will go well“)
- confuse effort with progress (effort and progress often correlated, but not
  always; no correlation in other cases, e.g. lines of code and progress)
- managers lack effectiveness to be “courteously stubborn” (lack of a model to
  be stubborn about)
- poor monitoring of schedule progress (drawing _some_ models from other
  engineering disciplines could help)
- adding manpower to late projects (again: false correlation)

&gt; More software projects have gone awry for lack of quality, which is part of
&gt; many destructive dynamics, than for all other causes combined.

&gt; More software projects have gone awry from management taking action based on
&gt; incorrect systemd models than for all other causes combined.

The problem is not only one particular dynamic, but misunderstanding the model
behind the dynamic.

Software managers often choose a linear model when non-linear forces are at
work. They try to use patterns that worked for small systems also for big
systems, which have different dynamics (_scaling fallacy_). This is common for
pattern 2 managers and usually leads them into software crises.

Two programmers performing one unit of work won’t simply produce two units when
working together, for their interaction produces non-linearities:

    1 + 1 = 2 + stimulation gain - interference loss

Adding people to a late project increses the total work to be done:

- old workers need to train new workers
- more coordination is required
- people in the project might work _against_ one another

Scaling Fallacy: “Large systems are like small systems, just bigger.” (Wrong!)

Written and spoken language is linear, therefore we often fall for linear
models. Two-dimensional _diagrams of effects_ are a better fit for non-linear
interdependencies:

- nodes: measurable quantities (“cloud” symbols for conceptual/actual
  measurements)
- arrow from node A to node B: quantity A has an effect on quantity B
    - dot on the arrow: A moves in one direction, B moves in _another_ direction
    - no dot: A and B move in the _same_ direction
- start with quantities of interest (outputs), e.g.:
    - number of bugs
    - time spent to deal with bugs
    - number of bugs caused by fixing other bugs
- work back to possible causes using brainstorming
    - too little developer training
    - too little testing done
    - too much pressure to finish new feature fast
- connect causes and effects with arrows (with/without dots to indicate reverse
  effects)
- chart secondary effects
    - more bugs -&gt; more time spent fixing bugs -&gt; less time testing new features -&gt; more bugs
    - more bugs -&gt; less time for training -&gt; worse quality in new code -&gt; more bugs
- reading the diagram from cause to effect:
    - even number of dots on the path: inverse effects that cancel out one another
    - odd number: effects reinforce one another
- connecting a quantity to an arrow indicates a multiplicative effect

The purpose of these models is not rigorous numerical analysis, but stimulating
thinking. The important part is not the resulting diagram, but the process of
_diagramming_.

As long as the numbers are small, linear effects can be assumed, because small
deltas also produce small effects. As soon as non-linear effects are
measured—and an exponential relationship is detected—the system might already be
in deep trouble.

# Feedback Effects

Some actions can’t be reversed, not even with help from higher management.

The Humpty Dumpty Syndrome:

1. manager becomes aware of a big risk
2. manager talks about the risk with higher management
3. higher management sees the risk as unlikely, doesn’t react immediately, but
   promises lots of resources in case of emergency
4. manager convinces himself that everything is fine
5. the problem gets worse in a non-linear way; management throws resources at
   the problem, the problem gets worse anyway, and the manager is used as a
   scapegoat

The manager is not _courteously stubborn_ but skilled at not facing reality (Brooks).

Previous actions _can’t always_ be revoked. Two Fallacies:

1. Reversible Fallacy: “What is done can always be undone.”
    - firing half the staff
    - hire them back the next day
2. Causation Fallacy: “Every effect has a cause—and we can tell which is which.”
    - causality is not a one-way street
    - feedback cycles reinforce themselves

Feedback cycles are self-reinforcing, making it hard to distinguish cause from
effect:

1. more bugs -&gt; more fixes -&gt; even more bugs
2. too little time to test -&gt; more bugs -&gt; more bugfixing -&gt; even less time to test
3. low quality -&gt; more frustration -&gt; less motivation -&gt; even worse quality

In a feedback cycle, cause and effect can look the same.

Systems with positive feedback loops either explode or collapse, depending on
the naming of the variable (measuring “quality” vs. “defects“; quality
collapses, defects explode).

Explosion and collapse change a system until the current model of the system no
longer applies. (Too many bugs: system is abandoned or bugs aren’t tracked any
longer.)

Managers are often too optimistic that everything goes well or things will get
better by themselves. Pattern 2 managers don’t know how to reason or communicate
about problems, slip into Humpty Dumpty Syndrome, and delay action even further.
Becoming aware that the problem got too big, they apply corrections that are too
big, starting an evern worse feedback cycle. (Adding more people to the project;
forcing them to work overtime; cutting technical reviews to save time, etc.)

Non-linearities are introduced in at least three different ways:

1. feeding back changes that _contribute to the workload_
    - more testing
    - more bugfixing
2. feeding back changes that _diminish the workforce_
    - more meetings
    - more training of workforce added
3. waiting too long, so that only big changes can have an effect, which creates
   other non-linear effects
    - switching technology
    - re-organizing the team
    - firing/hiring people

A system with positive feedback loop can only be stabilized by introducing an
adverse negative feedback loop. Management action that doesn’t introduce this
correcting loop is cosmetic and only will delay the disaster, e.g. overtime work
for testing and fixing bugs. (Overtime work will in fact create more positive
feedback loops contributing to the problem.) Proper solutions must introduce a
negative feedback loop, such as properly conducting technical reviews, which
will both diminish the number of bugs and the time spent fixing bugs.

A controller can only be effective when he’s connected by _two_ feedback loops
to the system, and if at least one of those loops is negative.

- reacting to schedule slippage by reducing requirements
- reacting to more bugs by conducting technical reviews
- reacting to illness by reducing overtime
- reacting to poor customer acceptance by design training

In pattern 3, the controller is connected to the system by two feedback loops:

1. through resources
2. through requirements

Action is only effective when taken early. To act small, one needs to sharpen
his the powers of observation.

A negative feedback loop is _not_ desirable, because stability is _not always_
desirable (e.g. when changing the pattern of an organization). Here, positive
feedback loops have to be established.

# Steering Software

In pattern 2, a plan is the sum of its steps:

&gt; It’s possible to make a project plan and follow it exactly.

In pattern 3, a plan gives orientation on where a projects stands:

&gt; Plans are rough guides. We need steering to stay on course.

In order to steer a project, not only meaningful measurements based on accurate
effect models are needed, but also models on how interventions will affect the
system under control. (If plans always could be followed strictly, interventions
would be futile.)

Unlike pattern 1, pattern 2 works with plans, often wrapped into methologies
(i.e. the Waterfall Model), which describe an ideal series of steps.

In classic Waterfall, there’s no going back:

1. Requirements
2. Analysis
3. Design
4. Coding
5. Testing
6. Operations

Modified Waterfall models have the notion of returning to an earlier stage
(rather GOTOs than proper iterations):

- requirements and analysis re-considered after design
- design and coding re-considered after testing

This unplanned re-considerations make estimation harder, which can cause
schedule overruns.

Sequential methods are like turn-by-turn directions that don’t consider
real-time circumstances (traffic jams, road works) or mistakes (taking the wrong
turn, taking it too late). Sequential methods are based on an _ideal set of
instructions_. It is also assumed, that mistakes can be corrected without
intervention.

Small mistakes can be corrected by individuals. However, bigger projects hold
more potential for bigger mistakes that cannot be corrected without
interventions.

When sticking to a sequential plans, small detours are never tried; the
territory around the chosen route is never explored. However, there might be
better ways.

Organizations tend to rely on past experience of successful projects. As new
tools and methods are introduced, the relevance of past experiences and the
intuitive understanding of feedback effects can be undermined.

A more iterative Waterfall only has some specific places for feedback at the end
of a phase. However, waiting for such a phase to end (“design“, “coding“) bears
risks:

- Not collecting feedback during a phase makes it impossible to detect  problems
  early on.
- Re-iterating a phase after a problem has been detected causes major
  disturbances in the whole process, say, doing a re-design after coding has
  been “finished“. Such “big” steps back are rather avoided than undertaken; the
  organization might go on with a flawed intermediate product to the next phase.

This methodology can be summarized as “doing it completely right or doing it
over“. There’s no notion of small deviations and according corrections.

Instead of one global process (Waterfall), a project can be split up in smaller
task cells that provide their own feedback loop (e.g. a User Story).

&gt; Good intervention models will help us to understand what we can’t control, but
&gt; a faulty model may lead us to overlook a number of effective interventions.

In Waterfall, it is assumed that every phase brings the project closer to the
desired state. However, in any phase there might happen things that bring the
project in a worse state:

- errors are introduced in the design and code
- project members get falsely confident
- project members burn out, quit
- conflicts between project members emerge; the no longer want to work together

Such model don’t account for “soft” factors, such as employees getting
frustrated or sick.

Errors introduced in the coding phase could originate in any of the (earlier)
phases (wrong requirements, erroneous analysis, bad design, bad coding).
However, they don’t manifest themselves before testing. Likewise, frustration
and tension might rise during the whole project, but the conflicts might only
erupt towards the (planned) end of the project.

Methodologies are often only product focused and don’t account for other outputs
(“soft” factors).

Pattern 2 managers often would rather hide in their offices and work with plans
rather than with human beings. The role of human action in project management is
often denied. However, human decision points are the places where a crisis can
be prevented.

&gt; More software projects have gone awry _because their managers didn’t know how
&gt; to respond to lack of calender time_ than for all other causes combined.

Loops that concern management contain decisions by people:

&gt; Whenever there’s a human decision point in the system, it’s not the event that
&gt; determines the next event, but someone’s _reaction_ to that event.

There are two types of laws concerning software engineering management:

1. “natural” laws (accept them)
2. “human decision laws” (learn to control them)

People often mistake human decision laws for natural laws:

- “We were late with coding, _therefore_ we had less time to test the
  application.”
    - _therefore_ suggests that a natural (or logic) law was followed
    - doing less testing was actually a human decision
- “We could not finish a proper design, but we _had to_ start coding, _because_
  time for design was up.”
    - _because_ suggests that one should start implementing an unfinished design
      when the clock tells you so
    - continuing with an unfinished design was a human decision

Pay attention to words that suggest a logical succession of events, such as “had
to“, “because“, “therefore“, and ask for the _reasoning behind_ it. Record
instances of human reasoning disguised as laws of nature.

# Failing to Steer

In pattern 2, managers only plan what should happen. In pattern 3, they should
also: observe, compare the observed to the planned, and take actions to move the
observed closer to the planned.

There are three dynamics standing in their way:

1. Victim Mentality
    - When bad things happen, managers fail to see their points of action.
      However, what counts are not only those events, but also (and foremost)
      the reactions to those events. “Victim language” can be reframed as
      “Controller language“.
    - Brooks’ Law (adding people to a late projects makes it later) can be
      mitigated: As long as the added staff doesn’t interfere with the existing
      staff’s process, they still can add value (checking and improving
      documentation, reviewing code, creating test cases, do chores for others
      in the project group).
2. Suppressing “Negative Talk”
    - If employees are punished for negative reports, they’re incentivized to
      produce rosy looking fake reports:
        - quick and dirty fixes to problems so that they don’t have to be
          reported
        - classify problems with lowered severity
        - grouping multiple problems together so that it looks like there are
          fewer problems
        - blaming users and the environment
        - interpret problems in a beneficial way
    - Inaccurate reports lead to improper actions: like navigating with the
      wrong man and the wrong coordinates.
    - Employees should be rewarded for handing in accurate reports instead.
3. Using Wrong Intervention Models
    - By acting on the wrong intervention model, problems are made worse.
    - Example: A software project conducted using an iterative approach is
      behind schedule. He decides to sacrifice quality to make up for the time
      lost. As more defects occur, the manager reacts by putting “recovery
      functions” into the product in order to make up for the defects. The
      software project falls even more behind schedule, and quality detoriates
      even further, which requires more “recovery functions“. The customer, who
      has to wait longer, wants to be rewarded for his patience—by asking for
      more features. This creates a vicious cycle.
    - The control points are working backwards: every intervention makes the
      problems worse. In this case, the interventions just need to be reversed.
      In the above example, a trust period is introduced, so that the developers
      can catch up and improve the product’s quality.
    - Two parties blaming each other creates a mutually destructive feedback
      loop. This can be solved with a better understanding of the system—unless
      the two parties have gotton too far from each other and prefer revenge
      over reconciliation.
    - Tools do not determine how they are being used. They can be applied for
      the worse or for the better.

# Why It’s Always Hard to Steer

When the dynamics of a  process is regulated by human decisions, _intervention
dynamics_ is at play. When human decisions have no power to alterate the dynamic
of a process, _natural dynamic_ is at play. A stone of a certain weight can be
lifted by a human for some time (intervention dynamics), but gravity constrains
that process in the long run (natural dynamic). Likewise, a manager can increase
the output of a team by letting its members work overtime (intervention
dynamics), but the returns will diminish after some time (natural dynamics).
Natural dynamics limit the effect interventions can have.

The _Square Law of Computation_ limits what a mind (i.e. a brain or a group of
brains) can accomplish. In order to keep a system under control, a model to
compute effects of interventions is required. A model can be expressed as a set
of equations with roughly one equation per relevant measurement in the system.

&gt; Unless some simplification can be made, the amount of computation to solve a
&gt; set of equations increases at least as fast as the square of the number of
&gt; equations. (Square Law of Computation)

A software project can be modeled as a game, in which a control strategy is
applied to go from an initial bad state (present position) to a final good state
(winning position). In a deterministic game, the player is in the position of
the perfect controller. There is no randomness, and the model covers all
possible ways a game can unfold (moves and countermoves). Tic Tac Toe and Chess
are both games that—theoretically—allow for perfect control. However, there are
orders of magnitude more ways a game of Chess can unfold than a game of Tic Tac
Toe. Therefore, there’s no perfect model (yet) to play chess. Instead, _general
principles_ (heuristics) are applied to reduce the set of _possible moves_ to a
much smaller set of _most promising moves_, which makes the game much more
manageable (e.g. don’t sacrifice your queen to capture a pawn) with limited
computing capacity. Such general principles are useful _most of the time_ but
might prevent the best play  _some of the time_.

Software engineering is way more complex and noisy (more randomness) than Chess.
Consider machine instructions as moves in the game, and the number of
instructions needed for a significant program (thousands, millions?) compared to
a game of Chess, which usually ends before 100 moves are played. However,
software engineering efforts are directed at reducing such non-linearities to
keep the Square Law of Computation under control, which is also accomplished by
applying general principles, such as:

- Don’t add people late in a project to catch up. (Brooks’ Law)
- Use the smallest possible team of the best possible people.
- Don’t shoot the messenger.
- Break your code up into modules.

These general principles are the simplifications (“Unless some simplifications
can be made“) of the Square Law of Computation. Collections of such general
principles make up the cultural pattern of a software organization. Such guiding
principles are needed because of the _Size/Complexity dynamic_:

&gt; Human brain capacity is more or less fixed, but software complexity grows at
&gt; least as fast as the square of the size of the program.

When a software project succeeds, ambitions rise, and the problems being
attempted to solve grow bigger. Bigger (and harder) problems drive an
organization to new, yet untried patterns. (If there’s no ambition for solving
harder problems or achieving better quality, an organization can stay with its
current pattern.)

It’s hardly possible to alter the capacity of our brains (hiring smarter people
only works to some extent), but it’s possible to alter how much of our capacity
is used, and for what purpose. Software engineering attempts to simplify the
solutions to larger problems—raising the success rate in response to higher
ambition.

A software project an be seen as a game against nature. The interventions of a
controller (moves) can be measured as effects (countermoves by nature). When the
controller sees that nature moves the project on the losing path, an
intervention move is required to bring it back to the winning path.

The _Fault Location_ dynamic states that under a constant error rate (say, one
bug introduced per developer and sprint), a larger system contains more faults
(Size/Complexity dynamic). The effect required to locate errors therefore grows
non-linearly. The Fault Location dynamic is a natural dynamic, not the result of
developers performing worse. Pattern 2 managers miss this point and apply
the wrong interventions; trying to work against nature. Both Size/Complexity and
Fault Location dynamic can be tackled using modularization.

The _Human Interaction_ dynamic states that the number of interactions between
people grows non-linearly to the amount of people being added to a project.

# What it Takes to Be Helpful

The computational power required to control a project grows non-linearly with
the problem size. Due to this non-linearity, being twice as smart does _not_
allow you to solve problems twice as big:

&gt; Ambitious requirements can easily outstrip even the brightest developer’s
&gt; mental capacity.

Given a program that is capable of playing the perfect game of chess: If the
board’s size is increased from 8x8 (64) to 10x10 (100) fields, the number of
possible moves explodes, even though the board size only grows by roughly 50%.

Every pattern has its size/effort curve, showing how well it will do given
problems of different sizes. This curve grows non-linearly: Increasing
requirements by 10% causes an additional effort of way more than 10%. The bigger
the problem, the bigger the _growth rate_ of the effort.

Software projects are highly variable. Due to this noise, a linear curve often
fits almost as good as an exponential one in a size/effort graph. Plotting data
on a logarithmic scale might help better understanding the data, but also trick
the eye into seeing linear patterns (Log-Log-Law):

&gt; Any set of data points forms a straight line if plotted on log-log paper.

Managers influence projects by choosing methods, tools, and people, which make
up an organization’s cultural pattern. Two different methods (or tools, or
teams) A and B will have different curves on a size/effort graph: Method A is
cheaper for small projects (say, due to less overhead), but reaches a limit of
feasible projects quite quickly. Method B, on the other hand, s more expensive
for small projects, but allows to deal with bigger projects than method A.

Therefore, organizations adopt _two_ (or more) methods instead of just one
“standard method” for software development, which are then picked from depending
on a project’s estimated size. This requires managers to take an important
decision at the beginning of every project: which method to choose? In a blaming
environment, taking such decisions can get a manager into trouble. Therefore,
they’d often prefer to have a “standard model” being imposed on them.

Managers choose their methods for other reasons than just effort. Risk is also a
very important factor. Depending on the problem size, different methods have
different chances of success. The bigger the project, the higher the risk, the
lower the chance of success.

Using a method for the first time is riskier and more effortful than doing so
for the umpteenth time. Managers are therefore reluctant to try out new methods,
patterns, languages, etc. The risk on the decision maker can be reduced by:

1. moving the decision to a higher level of management, so that the risk is
   spread wider (and higher)
2. reducing the size of the first project, which is intended to be a pilot
   project for learning (as opposed to getting attention)
3. reducing the criticality of the first project

Harm is rarely done out of malice, but of good intentions combined with wrong
assumptions about the underlying problem (wrong model). It is often being tried
to control systems that lie beyond one’s capacity:

- What one “knows” are often only simplifications (heuristics) carried over from
  simpler situations.
- One fails to see the true dynamics in an environment with lots of randomness.
  Once those dynamics are put under control one by one, the system gets stable
  enough for a transition from pattern 2 to pattern 3.
- Behaviours that might produce good results in the short run (e.g. deploying
  hacks to fix bugs, skip on testing to save time) cause trouble in the long
  run. One fails to see this connection, gets addicted to such a bad behaviour,
  and stays with the faulty intervention model—a disease of limited intelligence.

Some interventions cause more harm than good, so one might conclude that doing
harm is actually intended—which is very rarely the case! To analyze such
situations without getting paranoid, the _Helpful Model_ can be applied:

&gt; No matter how it looks, everyone is _trying_ to be helpful.

This model takes away the blame and lets us look at the true dynamics, i.e.
beyond anybody’s intention.

Mental models can’t be eliminated, but only replaced by better ones (_Principle
of Addition_):

&gt; The best way to reduce ineffective behaviour is by adding a more effective
&gt; behaviour.

Organizations get addicted to certain practices, which relieve their pain in the
short run, but are harmful in the long run. The more a practice is applied, the
worse one feels, and the more one seeks the relief of the addictive behaviour.
Such harmful, addictictive practices can be countered by adding long range
measurements with according rewards and punishments.

Better than curing addictions is preventing them in the first place:

&gt; The way people behave is not based on reality, but on their _models_ of
&gt; reality.

Implanting more effective models is the most helpful intervention.

# Responses to Customer Demands

An organization expresses that it is in crisis by the attitudes towards its
customers. If it gets too entangled with its internal problems, it forgets its
reason for existence. Complaints about too many customers and their
“unreasonable” demands are uttered. The more customers an organization has, the
more requirements it needs to fulfill, the bigger the system becomes—and, due to
the Size/Complexity dynamic—the more complex. The chances of conflicting
requirements also grows, causing additional trouble to resolve—either by
implementation or explanation.

The more customer a software organization has, the higher the demands on it
grow. More features are requested, more errors reported, and more configurations
must be supported—and all this within shorter time spans. An organization under
such pressure must either omove to a new cultural pattern in order to cope with
those higher demands—or it reduces its number of (satisfied) customer. (Fewer
customers tend to expect having all their expectations met, whereas thousands of
customers have lower expectations, considering their lower relative importance
to the organization.)

Managers want to keep the systems under their control in a healthy state. They
want to keep both the amount and the extent of interference from the outside
low, because too much interference can be disruptive to a system working well.
Customers give inputs to a system (requirements, error reports, money), and in
turn expect outputs from that system (software satisfying their requirements,
error reports being handled in time, correctly, and professionally). Some of
those inputs cause random disturbances. Like the managers on the inside, the
customer acts as an external controller to the system. Sometimes, the internal
and external controller are in conflict, causing conflicts within the system.

Not every user is a paying customer, but every user has expectations on how a
software is supposed to behave. Most users also notice errors that need to be
dealt with. All users cause load on a system. Every user has requirements, but
only paying customers buy their right to define the quality by requirements.

The marketing department is a surrogate for customers that acts on behalf of
them within the organization. They represent the customers and users against the
developers—sometimes well, sometimes badly. Marketing is supposed to act as a
filter between customers and developers. It decreases disturbances from
customers, but is itself closer to the developers, and therefore more prone to
cause disturbances itself by bypassing all the defenses. The side effects of an
interfering marketing department can be worse than the benefits gained from
their filtering of customer inputs towards the developers.

Developers can act as customer surrogates themselves and influence software
directly—for the better or worse. In a variable culture (pattern 1), developers
act without explicit or approved customer requirements. Pattern 2 managers want
to eliminate such bypassing of processes. In an unstable organization,
programmers frequently make changes without unauthorized, some of them remaining
unnoticed internally (but not by the customers). The more programmers an
organization has, the more potentially dangerous customer surrogates it has to
deal with.

Testers are supposed to faithfully replicate customer use. However, they make
frequent implicit decisions about what they think the customer _really_ wants or
needs. Since they are usually closer to the developers than to the customers,
there are more prone to the influence of the former than the latter.

As the number of customers increases, the amount of customer interactions
increases, too, usually non-linearly. The cost of an interruption is not only
the duration of the interruption itself (say, a five minute customer call), but
also an additional reimmersion time needed to get back to the original task (say,
fifteen minutes). The more people that are affected by a single
interruption—say, a phone call pulling a meeting attendee away, causing the
whole meeting to be interrupted—the longer the reimmersion time becomes:
Gathering a dispersed crowd costs time—and requires interrupting them at the
task they picked up in the meantime.

The number and size of meetings grows as the customer base grows. After a
certain amount of customers is reached, it is no longer possible to satisfy
their needs completely all the time. The number of possible configurations
(combinations of different platforms, versions used, integrations with other
software, customized software parts) starts to grow exponentially over time.
More configurations remain untested, causing more error reports. Those cause
more patches being deployed, which become harder to test on different
configurations. A software organization either changes its culture—or lowers the
level of support provided for its customers.

When software is releases, it is passed from one group (developers) to another
(customers). It is started being used productively. The release initiates a
couple of important dynamics:

- Multiple versions of a software now exist, which all need to be maintained.
- More errors are found and reported at a faster rate by actual users.
- Reported errors need to be fixed more quickly: Development is not driven by an
  external clock.
- Deploying faulty patches becomes much more expensive—and causes trouble for
  more customers. Also, the same errors are reported by multiple parties, which
  multiplies the amount of interactions for a single error.
- If individual patches are deployed for single customers, the system becomes
  harder to understand and maintain, especially if the patch was is poorely
  documented.
- All those problems need to be dealt with quickly—disrupting the development
  process of the entire organization, which leads to poorer quality and more
  stress in the long run. A vicious cycle unleashes.

Issuing fewer releases with higher quality can break those negative,
self-reinforcing dynamics. Many mature organizations release their software
only twice a year. Organizations also start to prioritize their customers.
“Nice” customers that pay more are better served than “nasty” ones that pay
less.
</content>
    </entry>
    <entry>
        <title>Vom präzisen Sprachgebrauch</title>
        <link href="https://paedubucher.ch/articles/2021-05-09-vom-praezisen-sprachgebrauch.html"/>
        <author>
            <name>Patrick Bucher</name>
        </author>
        <id>https://paedubucher.ch/articles/2021-05-09-vom-praezisen-sprachgebrauch.html</id>
        <updated>2021-05-09T12:30:00Z</updated>
        <content type="text/markdown; charset=UTF-8">
&gt; Besides a mathematical inclination, an exceptionally good mastery of one's
&gt; native tongue is the most vital asset of a competent programmer.

— _Edsger W. Dijkstra_, [EWD 498](https://www.cs.utexas.edu/users/EWD/transcriptions/EWD04xx/EWD498.html)

Sprache ist für mich sehr wichtig, und zwar nicht _obwohl_, sondern _weil_ ich
Informatiker bin. Obwohl ich ohne grössere Probleme in den mathematischen
Fächern durch das Studium gekommen bin, sind meine Fähigkeiten in diesem Fach
nicht besonders stark ausgeprägt. (Meine Mathematiklehrer vom Gynmanisum können
dies sicherlich bestätigen.)

Fremdsprachen lerne ich zwar auch nicht besonders schnell, aber nach längerer
Beschäftigung mit einer Sprache kann ich in dieser doch recht präzise und
flüssig kommunizieren. Noch wichtiger finde ich, dass man sich der Unterschiede
zwischen verschiedenen Sprachen bewusst wird. Denn wortweise Übersetzungen sind
nicht nur stylistisch schlecht, sondern oft schlichtweg falsch. So lässt sich
dieser Satz aus dem Deutschen:

&gt; Ich glaube, ich spinne.

nicht eins zu eins ins Englische übersetzen:

&gt; I believe, I spider.

Dies gilt auch für Programmiersprachen; auch diese sollten _idiomatisch_
gebraucht werden: Was in der Programmiersprache C korrekt und idiomatisch sein
mag:

    for (int i = 0; i &lt; n; i++) {
        sum += values[i];
    }

Sollte beispielsweise in Clojure ganz anders aussehen:

    (reduce + values)

Von jemandem, der bei Sätzen wie dem folgenden nicht leicht mit den Augen rollen
muss:

&gt; Es macht Sinn hier ausserhalb der Box zu denken und eine Extrameile zu gehen,
&gt; damit wir das Produkt rechtzeitig ausrollen können.

Erwarte ich auch keine stilistischen Meisterleistungen im Programmcode, gerade
wenn mehrere Programmiersprachen in einem Projekt zum Einsatz kommen.
(Mangelndes Verständnis für Sprachidiome dürfte wohl auch der Grund sein, dass
JavaScript nach vielen Jahren doch noch ein `class`-Schlüsselwort erhalten hat.)

Doch braucht man gar nicht die stilistische Ebene und die Übersetzungen zwischen
verschiedenen Idiomen heranzuziehen um sprachliche Ungereimtheiten zu finden.
Oft hapert es schon beim Gebrauch einzelner Wörter:

&gt; Das läuft auf einem _physikalischen_ Server, nicht auf einem virtuellen.

Physik ist die Lehre der Körper, und «physikalisch» bezeichnet etwas _die Lehre
der Körper betreffendes_. Das Gegenteil eines virtuellen Servers ist ein
_physischer_ Server: ein Server, der _als Körper_ vorhanden ist. Zu einem
«physikalischen» Server könnte man sich vielleicht einen chemischen oder
biologischen Server als Gegensatz vorstellen. (Ein _physikalischer_ Server wäre
einer, auf dem physikalische Berechnungen durchgeführt werden.) Hier dürfte es
sich nicht einmal um ein Übersetzungsproblem aus dem Englischen («physical»)
handeln, ist doch der _physician_ ein Arzt (der sich mit menschlichen Körpern
beschäftigt) und der _physicist_ ein Physiker (der sich mit der Lehre von
Körpern befasst) ‒ Achtung: Verwechslungsgefahr!

In englischsprachigen Vorträgen wird oft ‒ zu oft! ‒ das Verb «to consume»
verwendet:

&gt; We provide services and libraries that you can _consume_.

Das Wort «konsumieren» (denn dieses Wort lässt sich auf Englisch und Deutsch
gleich verwenden) bedeutet, dass etwas _verbraucht_ wird. In der Informatik ist
dies etwa im Zusammenhang mit Messaging (korrekterweise) zu lesen:

&gt; The producer sends out messages, which are _consumed_ by multiple workers.

Die Nachrichten _müssen_ konsumiert werden, ansonsten würden sie mehrfach
verarbeitet. Einen Dienst (engl. service) oder eine Programmbibliothek (engl.
library) kann man jedoch beliebig oft verwenden, ohne dass diese dabei
verbraucht werden würden. (Bei einem Bezahlservice _verbraucht_ man vielleicht
Tokens oder Credits wenn man den Service _verwendet_.)

Natürlich kann man Ausdrücke wie «physikalischer Server» oder «to consume a
library» pragmatisch deuten und dabei das Gemeinte korrekt vom Gesagten
unterscheiden. Diese Toleranz führt jedoch dazu, dass die Unterschiede
zwischen verschiedenen Begriffen («physisch» und «physikalisch»; «konsumieren»
und «verwenden») verloren gehen und man dadurch weniger klar kommunizieren kann:

&gt; — The message was _consumed_ by all workers.&lt;br&gt;
&gt; — So we have a race condition in the messaging component?&lt;br&gt;
&gt; — No. The workers just use `get` instead of `pop` on the message stack. Works as intended…&lt;br&gt;
&gt; — But then those messages are not _consumed_, but only _used_.&lt;br&gt;
&gt; — So what's the difference?

Oder:

&gt; — Wir brauchen zusätzliches IT-Budget um neue physikalische Server zu beschaffen.&lt;br&gt;
&gt; — Aber wir haben doch gar kein Physik-Departement an unserer Universität!?&lt;br&gt;
&gt; — Was tut denn das zur Sache? Wir brauchen die Server um Word-Dateien zu archivieren.&lt;br&gt;
&gt; — Und was hat denn das mit Physik zu tun?&lt;br&gt;
&gt; — Gar nichts, aber ich will nicht, dass die Dokumente in die Cloud gelangen.&lt;br&gt;
&gt; — Ach, sie meinen einen _physischen_ Server?&lt;br&gt;
&gt; — Natürlich, davon rede ich schon die ganze Zeit!

Diese Dialoge sind natürlich weit hergeholt und dürften ‒ hoffentlich ‒ so nie
zu vernehmen sein. Schliesslich schreibe ich diese Sprachglosse nur, um mich zu
unterhalten. (Man beachte: sprachliches Feingefühl kann eine zusätzliche Quelle
von Ärgernis _und_ Heiterkeit im Alltag sein.)
</content>
    </entry>
    <entry>
        <title>Tests Are Not Specifications</title>
        <link href="https://paedubucher.ch/articles/2021-05-02-tests-are-not-specifications.html"/>
        <author>
            <name>Patrick Bucher</name>
        </author>
        <id>https://paedubucher.ch/articles/2021-05-02-tests-are-not-specifications.html</id>
        <updated>2021-05-02T10:00:00Z</updated>
        <content type="text/markdown; charset=UTF-8">
In _The Clean Coder_, Chapter 7 (_Acceptance Tests_), Robert C. Martin writes
(p. 109):

&gt; But the real reason these tests aren't redundant is that their primary
&gt; function _is not testing_. The fact that they are tests is incidental. […]
&gt; The fact that they automatically verify the design, structure, and behaviour
&gt; that they specify is wildly useful, **but the specification is their true
&gt; purpose**. [Bold emphasis mine.]

OK, boomer, here's your spec:

    assert f(0) == 1
    assert f(-1) == -2
    assert f(+1) == 4
    assert f(+4) == 13
    assert f(-9) == -26

Now implement `f(x)`. The requirements are clear, aren't they?

You're still here? Where's the implementation!? I told you everything you need.
You got test cases! They _are_ the specification; you just told me this, and of
course, _Uncle Bob is always right_, they told me in school.

Still not done? OK, you're fired. Here's the Python implementation. Easy, isn't
it?

    def f(x):
        return (x + 2) * 3 - 5

Those young developers are no good, after all. Let's hire one of those old
programmers, please bring in that old, more-arrogant-than-Uncle-Bob, grumpy
Dijkstra guy ([EWD
981](https://www.cs.utexas.edu/users/EWD/transcriptions/EWD09xx/EWD981.html)):

&gt; At this stage I can give you some behavioural advice. Contrary to what
&gt; misguided—and misguiding—educationists may have told you, don't waste your
&gt; time looking at specific examples. Trying to come to grips with a large set by
&gt; looking at a few—hopefully representative—elements of that set is one of the
&gt; most ineffective ways of spending one's time. Go immediately for the general
&gt; case, or, in other words, **attack the set not by looking at specific elements
&gt; but by analysing the definition of the set.** [Bold emphasis mine.]

So you're not interested in some randomly picked elements of both the problem
and solution set, but in those set's definition? Is that all you need? Well,
here's that definition:

    y = f(x) = (x + 2) * 3 - 5

But I can also provide you with a set of test cases, or _specs_, as they are
called by some developers. Oh, you don't need them? But what is QA supposed to
do all day long then? You mean we can fire them? Wow, that sounds great. You're
hired, Mr. Dijkstra!

Did I hear this right, Mr. Dijkstra, _tests are not specifications_? Now you're
talking…
</content>
    </entry>
    <entry>
        <title>Gendersprache, schlechte Sprache</title>
        <link href="https://paedubucher.ch/articles/2020-09-27-gendersprache-schlechte-sprache.html"/>
        <author>
            <name>Patrick Bucher</name>
        </author>
        <id>https://paedubucher.ch/articles/2020-09-27-gendersprache-schlechte-sprache.html</id>
        <updated>2020-09-27T09:00:00Z</updated>
        <content type="text/markdown; charset=UTF-8">
Neulich bin ich auf einer Webseite auf ein Angebot _«für Benutzer*innen»_
gestossen. Dabei handelt es sich nicht etwa um ein
[Glob-Pattern](https://en.wikipedia.org/wiki/Glob_(programming)), worauf [der
Asterisk](https://www.duden.de/rechtschreibung/Asterisk) schliessen lassen
könnte, sondern wohl um ein
[Gendersternchen](https://de.wikipedia.org/wiki/Gendersternchen). (In der
Linguistik wird dieses Zeichen zur Kenntlichmachung von ungrammatischen 
Konstrukten verwendet. Die Ironie dieses Zufalls dürfte im weiteren Verlauf des
Artikels deutlich werden.)

Was soll nun dieses Gendersternchen bewirken? Es kürzt das umständlichere «für
Benutzer und Benutzerinnen» ab. Oder war es «für Benutzerinnen und Benutzer»?
Wie auch immer die Reihenfolge gewählt wird: der Gebrauch dieses Konstrukts
geht auf ein Missverständnis zurück, nämlich dass mit «Benutzer» immer eine
männliche Person gemeint ist. Schliesslich hat das Wort ja den bestimmten
Artikel «der». Um zu zeigen, _dass_ es sich hier um ein Missverständnis handelt,
betrachten wir folgenden Satz:

&gt; Der Benutzer ist jemand, der eine Anwendung benutzt.

Nun formulieren wir den Satz um, ohne dessen Bedeutung zu ändern:

&gt; Der Benutzer ist _eine Person, die_ eine Anwendung benutzt.

_Der_ Benutzer kann also _die_ Person sein. Folgt man der Argumentation hinter
dem Gendersternchen, nämlich dass «der Benutzer» sich auf ein männliches
Individuum bezieht, müsste sich «die Person» auf eine Frau beziehen. Wie kann
denn nun ein Subjekt gleichermassen männlich und weiblich sein?

Die Antwort ist einfach: die deutsche Sprache unterscheidet zwischen einem
_grammatischen_ Geschlecht (_Genus_) und einem _natürlichen_ oder _biologischen_
Geschlecht (_Sexus_). So kann «der Benutzer» durch «der Mensch», durch «das
Individuum» und durch «die Person» ersetzt werden. Natürlich lässt sich «der
Benutzer» auch durch Namen ersetzen.  Angenommen, Maria und Martin sind
Benutzer, wären die folgenden Sätze grammatikalisch korrekt:

&gt; Maria ist eine Person, _die_ eine Anwendung benutzt.

&gt; Martin ist eine Person, _die_ eine Anwendung benutzt.

&gt; Maria ist ein Mensch, _der_ eine Anwendung benutzt.

&gt; Martin ist ein Mensch, _der_ eine Anwendung benutzt.

&gt; Maria ist ein Individuum, _das_ eine Anwendung benutzt.

&gt; Martin ist ein Individuum, _das_ eine Anwendung benutzt.

Somit kann «der Benutzer» sowohl männliche als auch weibliche Personen
bezeichnen. Dies gilt jedoch nicht für «die Benutzerin», die sowohl
_grammatisch_ weiblich ist, als auch eine Person _natürlichen weiblichen
Geschlechts_ bezeichnet.

Ob jedoch mit «der Benutzer» nur das grammatische oder auch das natürliche
männliche Geschlecht gemeint ist, verrät uns nur der _Kontext_. Wenn sich nun
das eingangs erwähnte Angebot, das sich an _«Benutzer*innen»_ richtet, sich
bloss noch an _«Benutzer»_ richten würde, wäre eine Interpretation nötig. Ist
hier das grammatische oder das natürliche männliche Geschlecht gemeint?

Das Substantiv «Benutzer» ist vom Verb «benutzen» abgeleitet. Es bezeichnet eine
Person, die etwas benutzt. Gleichermassen ist ein «Programmierer» eine Person,
die programmiert, und ein Helfer eine Person, die hilft. Ein Wissenschaftler ist
eine Person, die Wissenschaft betreibt, und ein Fussballer spielt ganz einfach
Fussball. Das natürliche oder biologische Geschlecht der Person (der _Sexus_),
welche diese Handlung verrichtet, ist bei dieser Wortbildung weder relevant,
noch analytisch daraus erfahrbar.

Im Englischen funktioniert die Substantivbildung aus Verben durch Hinzufügen der
Wortendung «er» ebenfalls: _A player is a person that plays a game._ Der
bestimmte Artikel «the» impliziert jedoch keinen Genus, wodurch auch niemand auf
die Idee kommen würde, daraus auf den Sexus zu schliessen.

Ich denke, dass die meisten Leute bei einem Online-Angebot «für Benutzer» den
Kontext dahingehend interpretieren können, dass sich die männliche Form hier nur
auf das grammatische und nicht auch auf das natürliche Geschlecht bezieht.
Warum sollte denn eine Online-Plattform ca. die Hälfte der potentiellen _User_
‒ ein Ausweichen auf Englisch ist hier herrlich bequem, und dürfte für den
inflationären Gebrauch von Anglizismen mitschuldig sein ‒ von seinem Angebot
ausschliessen?  (Vielleicht weil es ein Webseitenbetreib*er*, und keine
Webseitenbetreiber*in* ist?  Doch ich drifte in Sarkasmus ab…) Wer es für
nötig empfindet, sein Zielpublikum mit «Benutzer\*innen» anzusprechen, spricht
diesem die entsprechende Interpretationsfähigkeit implizit ab. (Ein ähnliches
Phänomen ist bei der sogenannten _leichten Sprache_ zu beobachten, bei der man
sich als Leser so vorkommt, als ob man von oben herab wie ein Kind angesprochen
werde.) Somit empfinde ich den Gebrauch gendergerechter Sprache als
bevormundend, ja herablassend.

Um zu der Fehlinterpretation zu gelangen, dass mit dem Wort «Benutzer» Frauen
ausgeschlossen sind, muss man schon eine ganz dicke Genderbrille aufgesetzt
haben, welche die kontextsensitive Interpretation von Sprache erschwert. Wer
hinter jedem «der» eine patriarchalische Unterdrückungsstruktur wittert, dürfte
ohnehin Schwierigkeiten haben, Alltagsformulierungen pragmatisch aufzufassen.
Natürliche Sprache ist ambivalent und muss zwecks Verständnis wohlwollend
interpretiert werden. Wer käme denn auf die Idee, einem Sprecher beim Gebrauch
der Wörter «Sonnenuntergang» und «Sonnenaufgang» die Anhängerschaft an das
ptolemäische Weltbild zu unterstellen? (Und wieder drifte ich in Sarkasmus ab…
Ob es wohl am Thema liegt?)

Wer mit der Genderbrille durch die Welt gehen will, der darf das von mir aus
tun. Ich möchte das nicht. So ist meine Konsequenz des vorher geschilderten
Zusammenhanges, dass ich keine gendergerechte Sprache verwende. Wer
Formulierungen wie «Benutzer*innen» verwenden möchte, darf das gerne tun. Ich
halte es bloss für umständlich und bevormundend. (Dass ich die als Informatiker
beim Asterisk zunächst an ein Glob-Pattern denke, ist eine Berufskrankheit. Ich
kann den Kontext jedoch dahingehend interpretieren, dass hier eben _kein_
Glob-Pattern gemeint ist. Auch kann ich bei der Bezeichnung «der Benutzer»
kontextsensitiv interpretieren, dass hier mit dem bestimmten Artikel «der» nur
das grammatische und nicht das natürliche Geschlecht gemeint ist.)

Sprache ist die Interpretation von Zeichen auf verschiedenen Ebenen. Durch das
Explizitmachen von natürlichen Geschlechtern durch Gendersternchen gewinnen wir
höchstens, dass eine verschwindend geringe Menge von Missverständnissen
ausgeräumt wird; Missverständnisse, die durch eine pragmatische Lesart gar
nicht auftreten würden. (Oder kann mir jemand von einer Informatikerin
berichten, die auf einem Computer über Administratorenrechte verfügt, sich aber
unter der Bezeichung «Administrator des Computers» nicht angesprochen fühlt?)
Die Notwendigkeit von Interpretation könnte in diesem Sinne noch weiter
reduziert werden, indem man etwa auf Metaphern oder sprachlich ambivalente
Figuren aller Art verzichten würde. Dies mag für bestimmte Textarten sinnvoll,
ja notwendig sein: etwa bei Gesetzestexten, Verträgen oder technischen
Dokumentationen. Im Alltagsgebrauch oder für den vorliegenden polemischen Text
führte eine solche Einschränkung jedoch nur zu einer Verarmung der Sprache.

Gendersternchen («Benutzer*innen») und Binnen-I («BenutzerInnen») funktionieren
zudem nicht zufriedenstellend mit allen Fällen. Die Formulierung «der
Anmeldezeitpunkt des Benutzers» (Genitiv) müsste gendergerecht _und_
syntaktisch/grammatikalisch korrekt etwa als «der Anmeldezeitpunkt der
Benutzerin bzw. der Anmeldezeitpunkt des Benutzers» formuliert werden.
Varianten mit Gendersternchen oder Binnen-I funktionieren nicht («der
Anmeldezeitpunkt der/des Benutzer\*in») oder verkommen zu einer syntaktischen
Monstrosität («der Anmeldezeitpunkt der/des Benutzer(in/s)»). In der Praxis
werden solche Formulierungsprobleme mit dem Verzicht auf den Genitiv umgangen.
Die Sprache verliert weiter an Ausdrucksstärke und Variantenreichtum.

Ein weiteres Problem an «gegenderter» Sprache ist, dass Formulierungen wie
«Helferinnen und Helfer» explizit ein natürliches Geschlecht suggerieren, wo
dieses nicht relevant ist. Da «Helferin» immer biologisch weiblich ist, gewöhnt
sich der Leser durch diese Nebeneinanderstellung zum «Helfer» zusehends an die
Interpretation, das bei letzterem das männliche biologische Geschlecht gemeint
sein muss. Da man weibliche oder sachliche Wörter wie «Person» oder «Individuum»
nicht durch Hinzufügen oder Weglassen einer Silbe «gendern» kann, wird das
grammatische Maskulinum zusehends sexualisiert, was hingegen bei Femininum und
Neutrum nicht geschieht.

Apropos Anglizismen: Wer denkt, wie ich es weiter oben angesprochen habe, man
könne dem Gendersternchen entgehen, indem man das deutsche Wort «Benutzer» durch
das englische Wort «user» ersetzt, für den habe ich schlechte Nachrichten! Immer
häufiger sehe ich die Bezeichnung _«User*innen»_, naturgemäss nur auf
deutschsprachigen Webseiten. Gibt es im Englischen keine verräterischen
bestimmten Artikel, verliere ich diese nützliche Ambivalenz beim Übersetzen ins
Deutsche. (Daher kommt es, dass englische Lehnwörter wie «Blog» von
verschiedenen Leuten mit unterschiedlichen bestimmten Artikeln bezeichnet
werden: «das Blog» und «der Blog» sind beide anzutreffen.) So mache ich mir das
Schreiben einfacher, indem ich sowohl auf Gendersprache als auch auf das
Umschwenken von geschlechtsneutralen Anglizismen verzichte. (Um Anglizismen
komme ich als Informatiker ansonsten kaum herum.)

Germanisten und Linguisten mögen meine obige Argumentation gerne zerpflücken.
Ich bin gerne dazu bereit, etwas neues zu lernen. Wie viel Ambivalenz und
Interpretationsspielraum ‒ sei es durch das Weglassen von Gendersternchen oder
durch das Beifügen von sprachlichen Figuren ‒ ich meinen Lesern zumute, ist
jedoch meine persönliche Entscheidung. So bin ich in dieser Beziehung
_konservativ_, zumal ich hinter dem sogenannten _progressiven_ Anpassungsdruck,
der auf unsere Sprache ausgeübt wird, mehr Tendenzen zu einem Neusprech als eine
sprachliche Bereicherung erkenne. Sprachliche Figuren _oder_ Gendersternchen:
beides habe ich nicht im Angebot.
</content>
    </entry>
    <entry>
        <title>Arch Linux Setup with Disk Encryption</title>
        <link href="https://paedubucher.ch/articles/2020-09-26-arch-linux-setup-with-disk-encryption.html"/>
        <author>
            <name>Patrick Bucher</name>
        </author>
        <id>https://paedubucher.ch/articles/2020-09-26-arch-linux-setup-with-disk-encryption.html</id>
        <updated>2020-09-26T13:00:00Z</updated>
        <content type="text/markdown; charset=UTF-8">
I've been using Arch Linux since 2016. I got to understand the system better
since then, and installing Arch nowadays is rather a strain on my fingers than
on my brain. I automated my personal setup procedure to some extent with a
couple of shell scripts, which I'm adjusting as time goes and hardware changes.
I collect those scripts and setup instructions in a [GitHub
repo](https://github.com/patrickbucher/docs/tree/master/arch-setup).

Even though I've often taken my laptop with me, especially for
university, I never made the effort to encrypt my disk or a single partition
thereon. Sensitive data, such as passwords, are stored encrypted using GPG. And
my GPG private key is protected with a strong passphrase; so strong, that I
mistype it once in a while.

Having stored SSH keys on my laptop, however, is a risk in terms of data
protection. I do not protect most of my SSH keys with passwords. (_Not_ having
to use passwords is one of the conveniences SSH gives you, after all.) But some
of those SSH keys are gateways to quite some important data, and therefore I
need to re-consider my security practices.

Protecting SSH keys with a passphrase is very inconvenient, especially if you do
a lot of `git push` operations via SSH. It's possible to use a password
protected SSH key for `git` only, but having to deal with multiple SSH keys is
inconvenient, too. (My rule is to use one SSH key per client machine.) So
encrypting the hard disk or at least some partitions of it might be the better
solution, because it only requires me to enter an additional password once
during startup, which is once or twice a day.

# dm-crypt and LUKS

The Arch wiki provides an article on [how to encrypt an entire
system](https://wiki.archlinux.org/index.php/Dm-crypt/Encrypting_an_entire_system).
Such a setup is based on LUKS (Linux Unified Key Setup), a disk encryption
specification, and dm-crypt, the Linux kernel's device mapper for encrypted
devices.

The wiki mentions different
[scenarios](https://wiki.archlinux.org/index.php/Dm-crypt/Encrypting_an_entire_system#Overview)
for disk encryption. The first option, [LUKS on a
partition](https://wiki.archlinux.org/index.php/Dm-crypt/Encrypting_an_entire_system#LUKS_on_a_partition),
is very simple, but not ideal for me, because I'm not just using one giant root
partition, but separate partitions for `/var`, `/tmp` and `/home`. With this
setup, a process filling up my `/var/log` directory won't prevent me from
writing to `/home` or the root partition, which might be necessary to deal with
the issue.

One could argue that it's sufficient to just encrypt the `/home` partition,
which contains the most sensitive data, after all. However, it's possible that
some program logs sensitive data to `/var/log` or stores it under `/opt`, so
that an encrypted `/home` partition won't prevent sensitive data from being
leaked. An unencrypted root partition also makes it possible to tamper
programs, so that data is leaked later.

It's also possible to encrypt the partitions mentioned seperately, which would
require me to enter my password upon booting for every single partition. This is
highly impractical, and therefore I won't do it that way.

The second option, [LVM on
LUKS](https://wiki.archlinux.org/index.php/Dm-crypt/Encrypting_an_entire_system#LVM_on_LUKS),
is a better fit for my purpose. Here, LUKS provides one single block device, on
top of which the encrypted partitions are created. The other, arguably more
sophisticated scenarios, are mostly useful when dealing with multiple disks, or
if an encrypted boot partition is required. So let's stick to scenario two: LVM
on LUKS.

# Partitioning

My Lenovo Thinkpad X1 has a SSD with roughly 475 GB and 16 GB memory. This is
important when considering partition sizes. I usually create my partitions as
follows:

1. A 256 MB `/boot` partition for the bootloader (FAT 32).
2. A 16 GB (the size of my memory) swap partition.
3. A 128 GB `/` (root) partition (ext4).
4. A 64 GB `/var` partition (ext4).
5. A 8 GB `/tmp` partition (ext4).
6. And a `/home` partition with the remainder of the space, i.e. roughly 260 GB
   in my case (ext4).

The choice of partition sizes is subject to debate. Some prefer to have a swap
partition with twice the size of the physical memory. Some prefer to make the
root, `/var/`, and `/tmp` partitions smaller or bigger. But the above
partitioning scheme was never the cause of any trouble for me, so far.

With those decisions taken, let's proceed to the setup.

# Setup Procedure

So let's go through the setup step by step.

## Prepare the Installation Medium

First, I download the latest Arch amd64 image via BitTorrent [Magnet
link](magnet:?xt=urn:btih:db56a13a6555179990837759ca27274d0be49aca&amp;dn=archlinux-2020.09.01-x86_64.iso)
from the [download](https://www.archlinux.org/download/) page and verify its
checksum against the one on the website:

    $ sha1sum archlinux-2020.09.01-x86_64.iso
    95ebacd83098b190e8f30cc28d8c57af0d0088a0

Then I copy the image on a USB dongle with a capacity of at least 700 MB:

    $ sudo dd if=archlinux-2020.09.01-x86_64.iso of=/dev/sda bs=4M
    $ sync

Then I unplug my USB dongle and plug it into the Thinkpad X1 I'd like to setup,
and boot from it (UEFI boot). Once the installation environment is loaded, let's
continue with the setup.

My Thinkpad X1 has a US keyboard, which I [prefer to
use](https://paedubucher.ch/articles/2020-09-16-compose-key-on-x.html) nowadays.
However, it's possible to change the keyboard layout as follows (for Swiss
German), if wanted:

    # loadkeys de_CH-latin1

Next, I establish a WiFi connection with `frzbxpdb5` being my network SSID, 
`wlan0` being my WiFi device, and `[topsecret]` being my password:

    # iwctl --passphrase '[topsecret]' station wlan0 connect frzbxpdb5

Let's continue with the partitioning of the disk.

## Partitioning

In order to partition the hard disk, one first needs to know the device name,
which can be found using `lsblk`:

    # lsblk

This lists `sda`, the USB dongle, and `nvme0n1`, the 475 GB solid state drive
aforementioned.

### Filling the Disk with Junk Data

As a first step, the disk shall be overwritten with random data.  Even though I
haven't used this laptop productively yet, and so there's no sensitive data
stored on it, it is still a good practice to fill it up once with random data.
This makes it harder for an attacker to distinguish between encrypted data and
random junk.

    # shred --random-source=/dev/urandom --iterations=1 /dev/nvme0n1

In my case, using only one single iteration is sufficient. If there was actual
data stored on the disk, multiple iterations should be considered.

### Creating a Boot Partition

Next, I create a new GPT partition scheme on that disk:

    # parted -s /dev/nvme0n1 mklabel gpt

I'm not going to encrypt my boot partition, so I create it as I do for a regular
setup without encryption (as a FAT 32 boot partition, that is):

    # parted -s /dev/nvme0n1 mkpart boot fat32 1MiB 257MiB
    # parted -s /dev/nvme0n1 set 1 esp on
    # mkfs.fat -F 32 /dev/nvme0n1p1

I leave a gap of 1 MB at the beginning, so no matter what block size my SSD
uses, the boot partition will always be properly aligned. The `esp` flag
identifies the partition as a UEFI system partition.

### Creating a Partition for Encryption

Now comes the crucial part. All the encrypted data is put into a single big
partition (`nvmen1p2`), taking up all of the remaining disk space, which is then
partitioned using a volume manager:

    # parted -s /dev/nvme0n1 mkpart cryptlvm 257MiB '100%'

The encryption is set up on this partition:

    # cryptsetup luksFormat /dev/nvme0n1p2

Enter &quot;YES&quot; if asked for confirmation, and pick a strong passphrase to be
entered twice. (At this point, remember exactly which keyboard layout you're
on!).

To further work with the encrypted partition, let's open it, which requires to
enter the password chosen before:

    # cryptsestup open /dev/nvme0n1p2 cryptlvm

Now a physical volume for the volume mapping needs to be created:

    # pvcreate /dev/mapper/cryptlvm

The partitions are going to be managed in a volume group, which I simply call
`volgrp` for the sake of brevity:

    # vgcreate volgrp /dev/mapper/cryptlvm

Now everything is set up to create the remaining, i.e. the encrypted partitions.

### Creating the Remaining Partitions

Those are created using `lvcreate` by setting a size (`-L`/`-l` parameter) and a
name (`-n` parameter):

    # lvcreate -L 16G volgrp -n swap
    # lvcreate -L 128G volgrp -n root
    # lvcreate -L 64G volgrp -n var
    # lvcreate -L 8G volgrp -n tmp
    # lvcreate -l '100%FREE' volgrp -n home

**Update**: As the user _fra-san_ [pointed
out](https://unix.stackexchange.com/questions/611421/arch-linux-setup-with-encryption-lvm-on-luks)
in a comment on StackExchange, leaving some space open rather than using `-l
'100%FREE'` for the `/home` partition is useful when partition sizes should be
increased later.  Shrinking partitions requires to unmount them, whereas growing
them can happen with the affected partition still being mounted. (Check out
`lvresize` for details.)

The partitions are going to be formatted using the `swap` and `ext4` format,
respectively:

    # mkswap /dev/volgrp/swap
    # mkfs.ext4 -F /dev/volgrp/root
    # mkfs.ext4 -F /dev/volgrp/var
    # mkfs.ext4 -F /dev/volgrp/tmp
    # mkfs.ext4 -F /dev/volgrp/home

For the actual setup, those partitions (and the `/boot` partition created
before) need to be mounted to `/mnt`:

    # mount /dev/volgrp/root /mnt

    # mkdir /mnt/boot
    # mount /dev/nvme0n1p1 /mnt/boot

    # swapon /dev/volgrp/swap

    # mkdir /mnt/var
    # mount /dev/volgrp/var /mnt/var

    # mkdir /mnt/tmp
    # mount /dev/volgrp/tmp /mnt/tmp

    # mkdir /mnt/home
    # mount /dev/volgrp/home /mnt/home

Now the partitions are ready for a regular bootstrap installation. (Setting up
the boot loader will require some more specific instructions to disk encryption
later on.)

## Bootstrap Installation

Now let's install the base system, together with the `lvm2` package:

    # pacstrap /mnt base linux linux-firmware lvm2

In order to get the mounting done automatically upon restart, let's save it in
the `fstab` file:

    # genfstab -U /mnt &gt;&gt; /mnt/etc/fstab

When this is done, let's switch into the installed environment:

    # arch-chroot /mnt

A password needs to be set for the `root` user:

    # passwd

In order to have a WiFi connection after rebooting, let's install a couple of
networking packages (some of those specific to my Intel hardware):

    # pacman -S iw wpa_supplicant dialog intel-ucode netctl dhcpcd

I set the time zone to Zurich (Europe), update and save the system time:

    # ln -sf /usr/share/zoneinfo/Europe/Zurich /etc/localtime
    # timedatectl set-ntp true
    # hwclock --systohc

For language and locale, I simply use `en_US.UTF-8` and `en_US-UTF-8 UTF-8`,
respectively:

    # echo 'en_US.UTF-8 UTF-8' &gt;&gt; /etc/locale.gen
    # locale-gen
    # echo 'LANG=en_US.UTF-8' &gt; /etc/locale.conf

Due to the lack of imagination, I call my laptop simply _carbon_:

    # echo carbon &gt; /etc/hostname

This is a very basic setup. Now let's make sure it can be booted by installing
the boot loader:

## Configuring the Boot Loader

I've always been using the systemd boot loader on Arch Linux, which is quite
simple to configure. First, the computer needs to get a unique id, then the boot
loader can be installed into the `/boot` partition:

    # systemd-machine-id-setup
    # bootctl --path=/boot install

Now comes the tricky part: The UUID of the boot partition needs to be figured
out. `blkid` lists various partitions, but which one to choose? It's the
LUKS partition containing the encrypted volume: `/dev/nvme0n1p2`. Its UUID can
be extracted as follows, and shall be saved into a variable for later use:

    # uuid=$(blkid | grep 'crypto_LUKS' | egrep -o ' UUID=&quot;[^&quot;]+&quot;')
    # uuid=$(echo $uuid | awk -F '=' '{ print $2 }' | tr -d '&quot;')

The first line lists the partitions (`blkid`), extracts the line with the
encrypted partition (`grep`), and further extracts the part of the line defining
the UUID (`egrep`). Don't forget the space in front of `UUID`, otherwise the
`PARTUUID` is extracted, too. In the second line, the definition
(`UUID=&quot;abc...&quot;`) is split at the equal sign, of which the second part
(`&quot;abc...&quot;`) is taken using `awk`. Then the surrounding double quotes are
removed with `tr`. The variable `$uuid` now contains the UUID of the encrypted
partition.

**Update**: As the user _fra-san_ [pointed
out](https://unix.stackexchange.com/a/611507/223188), there's an easier way to
extract the UUID:

    # uuid=$(blkid --match-tag UUID -o value /dev/nvme0n1p2)

Having this information, the entry for the boot loader can be created as
follows:

    # cat &lt;&lt;EOF &gt;/boot/loader/entries/arch.conf
    title   Arch Linux
    linux   /vmlinuz-linux
    initrd  /initramfs-linux.img
    options cryptdevice=UUID=${uuid}:cryptlvm root=/dev/volgrp/root
    EOF

Now a simple bootloader configuration needs to be created:

    # cat &lt;&lt;EOF &gt;/boot/loader/loader.conf
    default arch
    timeout 0
    editor  0
    EOF

## Creating the Ramdisk Environment

As mentioned earlier, there are some further adjustments for disk encryptions to
be made. Let's open `/etc/mkinitcpio.conf` and go to the `HOOKS` definition:

    HOOKS=(base udev autodetect modconf block filesystems keyboard fsck)

Which needs to be extended with `encrypt` and `lvm2`:

    HOOKS=(base udev autodetect modconf block filesystems keyboard fsck encrypt lvm2)

Then the ramdisk environment can be created:

    # mkinitcpio -P

Which generated the `initramfs-linux.img` file referred to from the boot loader
entry created before, as well as a fallback `initramfs-linux-fallback.img`.

## Finishing

Now that everything is set up, leave the `chroot` environment, unmount the new
system's partitions, and shut down the computer:

    # exit
    # umount -R /mnt
    # shutdown -h now

After removing the USB dongle, start the system again.

Early in the boot process, you'll be asked to enter the passphrase for
`/dev/nvme0n1p2`. After doing so, the system will boot. If not, you did
something wrong in the process, because it's working just fine on my Thinkpad X1
Carbon.

I also had issues getting everything right in the first place, but fortunately
got help on the [Linux &amp; Unix
StackExchange](https://unix.stackexchange.com/q/611421/223188). Thanks to the
user _frostschutz_ for pointing out the core issue. The user _Cbhihe_ pointed
out that my partition sizes weren't sustainable, so that I adjusted it
accordingly for this article.

# Conclusion

We set up Arch Linux with encrypted partitions using the variant LUKS on LVM.
The system is split up into multiple partitions, which are all encrypted using
the same password over one common encrypted volume.

The `/boot` partition remains unencrypted, which would allow an attacker to
tamper with my system boot, and, potentially, with my entire setup. So if my
laptop gets lost, there is no 100% guarantee that my system hasn't been
manipulated. However, having all the other partitions encrypted makes
manipulation or data leaks extremely unlikely.

When my laptop gets stolen or lost for an extended period of time, it is still a
good idea to revoke all the SSH and GPG keys. But thanks to encryption, that
procedure won't be an act of emergency, but one that could be carefully planned
for and executed at a convenient time.

The system set up so far is really just a base installation. In order to work on
this computer, further tasks have to be performed, as roughly described in my
[Arch
Setup Notes](https://github.com/patrickbucher/docs/blob/master/arch-setup/arch-setup.md)
on GitHub. Maybe I'll write a follow-up article in the future to cover those
steps more in detail.
</content>
    </entry>
    <entry>
        <title>Basic Printing on OpenBSD</title>
        <link href="https://paedubucher.ch/articles/2020-09-20-basic-printing-on-openbsd.html"/>
        <author>
            <name>Patrick Bucher</name>
        </author>
        <id>https://paedubucher.ch/articles/2020-09-20-basic-printing-on-openbsd.html</id>
        <updated>2020-09-20T18:00:00Z</updated>
        <content type="text/markdown; charset=UTF-8">
I have a roughly ten year old Brother HL-5370DW printer on the shelf next to me.
This printer is mostly used by my wife to print sewing patterns. When I was
studying computer science, I sometimes printed documents I've written for
proofreading. I often was able to find typos that I didn't see on the screen
even after proofreading the document two or three times. However, I didn't
bother to print out my bachelor thesis. Printing 120 pages just for proofreading
just seemed a waste to me. I did my proofreading on the screen extra carefully,
and nobody complained about typos. (Which doesn't mean that there were none.)

Having finished my studies, I hardly ever print out documents. However, I still
prefer to read long texts on paper rather than on the screen. Therefore I often
buy technical books as paperbacks or hardcovers rather than ebooks. And if I buy
an ebook with demanding content, I print out those sections for offline reading.

Having switched to OpenBSD for my private computing shifted my reading habits
more towards manpages. When I need to figure out how something works on
OpenBSD, `apropos(1)` beats Google as a starting point in many cases. Some
manpages are really long, for example `ksh(1)`. I have a book on the Korn Shell
in my basement, which covers `ksh93`.  However, there are some differences
between `ksh93` and OpenBSD's `pdksh`. So reading the manpage not only gives me
more accurate information, but also _less_ to read.

So why not printing out the manpage `ksh(1)`? I can do so even nicely formatted
using PostScript:

    $ man -T ps -O paper=a4 ksh &gt;ksh.1.ps

Now `ksh.1.ps` can be read with `zathura(1)`, given that the package
`zathura-ps` is installed:

    # pkg_add zathura zathura-ps
    $ zathura ksh.1.ps

But why using PostScript and not PDF like anybody else for the last twenty five
years? Because PostScript is the least common denominator and, thus, supported
out of the box by OpenBSD. (For fancier printing options, check out `cups`, but
I'd like to keep it minimalistic for the moment.)

# Printer Setup

I figured out how to configure my printer by reading the section _The lpd
Printing Daemon_ in the 16th chapter of [Absolute OpenBSD (2nd
Edition)](https://nostarch.com/obenbsd2e) (p. 306-307) by [Michael W
Lucas](https://mwl.io/). This is how I applied the configuration to my local
setup.

First, I created the file `/etc/printcap` with the following content:

    lp|brother:\
        :sh=:\
        :rm=192.168.178.52:\
        :sd=/var/spool/output/brother:\
        :lf=/var/log/lpd-errs:\
        :rp=brother

There must be a newline at the end of the file. The line breaks are escaped
using backslashes, except for the last line. The options are defined as follows:

- The first line defines two names for my printer: `lp`, which should always be
  there, and `brother`, which is my arbitrary name for the printer.
- The second line (`sh`) defines that no _burst page_ (summarizing the last
  print job on a special page) should be printed.
- The third line (`rm`) refers to the printer on the network. My FritzBox always
  gives the same IP to my printer. It's also possible to use the printer's
  hostname.
- The fourth line (`sd`) defines the spooler directory for this printer. Print
  jobs are written into that directory.
- The fifth line (`lf`) defines a log file for error messages, which you hopefully
  never need to check.
- The sixth line (`rp`) defines the remote printer name.

Next, the spooler directory needs to be created. It must be owned by the user
`root` and the group `daemon`. Regular users need write access to this directory
in order to print documents:

    # mkdir /var/spool/output/brother
    # chown -R root:daemon /var/spool/output/brother
    # chmod 770 /var/spool/output/brother

Now the printer daemon `lpd` needs to be activated. To do so on system startup,
add the following line to `/etc/rc.conf/local`:

    lpd_flags=&quot;&quot;

Then start the service:

    # /etc/rc.d/lpd restart

**Update (2020-09-21)**: As one reader on
[Hacker News](https://news.ycombinator.com/item?id=24535357#24538879) pointed
out, the last two steps can be performed using `rcctl(8)`:

    # rcctl enable lpd
    # rcctl restart lpd

The manpage says that `rcctl(8)` was introduced in OpenBSD 5.7 back in 2015.
_Absolute OpenBSD (2nd Edition)_ is from 2013 and, thus, older than that. (At
the time of this writing, I'm using Version 6.7.)

Another reader pointed out that setting the access rights to `777` is a bad
practice. That's true, and I actually got the reasoning behind this wrong: I
thought any user must be able to write to the spooler, because any user is
supposed to print. However, it's `lpd` that is writing to the spooler, which of
course runs under the `daemon` group. Therefore, the access rights for
`/var/spool/output/brother` should be set to `770`, not to `777` (as corrected
above).

# Printing Documents

Now the printer is ready to accept jobs. In order to print the PostScript file
generated before, just run `lpr` on the file:

    $ lpr ksh.1.ps

It's also possible to send the PostScript output directly to the printer (this
is Unix, after all), if no preview is needed:

    $ man -T ps -O paper=a4 ksh | lpr

Printing plain text files behaved strange on my setup, but could to using the
`pr` formatter with `lpr` as follows:

    $ lpr -p plain.txt

Instead, I also convert plain text files to PostScript, which looks quite nice
on paper. I use `enscript(1)` for this task:

    # pkg_add enscript
    $ enscript plain.txt -o plain.ps
    $ lpr plain.ps

PDFs can also be converted to PostScript using `pdf2ps(1)`, which comes with
GhostScript, i.e. the `ghostscript` package:

    $ pdf2ps document.pdf document.ps

Unfortunately, this doesn't work with all PDFs. But for the time being, I have
enough manpages to read. Printing PostScript works extremely fast, by the way.
When I press return at the end of a `lpr` command, I can see the status LED on
my printer start blinking almost immediately.
</content>
    </entry>
    <entry>
        <title>Compose Key on X</title>
        <link href="https://paedubucher.ch/articles/2020-09-16-compose-key-on-x.html"/>
        <author>
            <name>Patrick Bucher</name>
        </author>
        <id>https://paedubucher.ch/articles/2020-09-16-compose-key-on-x.html</id>
        <updated>2020-09-16T21:00:00Z</updated>
        <content type="text/markdown; charset=UTF-8">
I've been using the Swiss keyboard layout for most of my life. Things changed
when I first had to work on a Mac Book. If the Swiss keyboard layout is not
great for programming on a usual keyboard, because it requires combinations with
Alt-Gr to type in braces and brackets, I consider the Mac version of it outright
horrible, because braces and brackets are located on the digits row. Those
symbols are not even painted onto their respective keys, which makes the
transition for non-Mac users even harder.

# Entering the US Keyboard Layout

So instead of learning an additional inefficient way of typing in special
characters, I decided to adopt to the US keyboard layout. This layout gives you
most characters required for programming with a single key or a combination of
Shift and another key.

One issue of the US layout is that it doesn't provide characters such as _ö_ or
_é_, which I still need to type in for emails, for documentation, and for this
article, of course. Mac OS provides a composing mechanism that lets you type a
double quote followed by a letter like _o_, and then combines those characters
to the German umlaut _ö_.  If you just want to type a double quotation mark,
you need to type Space right after it, so that the quotation mark is not
attempted to be combined with the next character entered. This is annoying, of
course, when programming. So I never really got warm with the Mac keyboard
layout and input method. An external keyboard with US layout didn't help much
in that respect, but at least allowed me to type in those cumbersome
combinations faster and more precisely.

# Switching Keyboard Layouts

Another solution to the goal conflict of typing in German umlauts and special
programming characters as fast as possible is to switch the keyboard layout as
needed. I'm very familier with this approach, because I use the Russian keyboard
layout once in a while. I modified my [dwm](http://dwm.suckless.org/)
configuration so that pressing Super+Tab changes my keyboard layout using the
following script (`switchkb`):

    #!/bin/sh

    layout=&quot;$(setxkbmap -query | grep layout | egrep -o [a-z]{2}$)&quot;
    if [ &quot;$layout&quot; == &quot;ch&quot; ]; then
        setxkbmap ru
    elif [ &quot;$layout&quot; = &quot;ru&quot; ]; then
        setxkbmap us 
    elif [ &quot;$layout&quot; = &quot;us&quot; ]; then
        setxkbmap ch
    fi

This lets me cycle through the keyboard layouts I need very quickly. I also
display the currently active keyboard layout in my status bar using
[slstatus](http://tools.suckless.org/slstatus/), which I described in a
[previous
article](https://paedubucher.ch/articles/2020-09-05-openbsd-on-the-desktop-part-i.html).
Even though this is a feasible solution, I still have to switch mentally between
the Swiss German and the US keyboard layout. Typing in a double quote in a German
email requires me to press another key than typing in that very same character
when programming. This additional mental burden is wearing me down on a usual
working day, which consists of roughly 70% programming and 30% communicating.
The communication part might actually be bigger, but some of the communication
also takes part in English, so that I'd be better off using the US rather than
the Swiss German keyboard layout.

Typing cyrillic letters still requires the Russian keyboard layout, so I won't
get rid of my `switchkb` script and Super+Tab. However, since typing in
punctuation marks using the Russian keyboard layout is almost the same as
typing in those characters on the US keyboard layout, I rather ditch my native
Swiss German layout and embrace the imperialistic option.

# Composing Characters

While being able to write source code faster and not having to distinguish
between English, German, and Russian when typing in punctuation marks sounds
great in terms of _efficiency_, not typing in the typographically correct
representations of German umlauts (_Ae_, _Oe_, _Ue_ instead of _Ä_, _Ö_, _Ü_)
or misspelling French words with accent marks (_depecherent_ instead of
_dépêchèrent_) is bad in terms of communicating _effectively_. So there must be
at least a way to type in those characters somewhat efficiently.

One option would be to type in those characters as hexadecimal Unicode code
points.  I've already learned a few thereof by heart, such as U+2012, U+2013,
and U+2014 for dashes of different lenghts, or simple to remember ones like
U+00ab and U+00bb for guillemets (_«»_). In `vim`, those sequences can be
entered by presing Ctrl+V U followed by the part after U+. In GTK applications,
Ctrl+U enters a mode to enter those codes to be finished with a Space (or maybe
some other character not representing a hexadecimal digit).

However, remembering a lot of Unicode code points is not a very intuitive way
to type and probably leads to a lot of lookups on
[FileFormat.Info](https://www.fileformat.info), which is at least a more
sophisticated way than just googling those character code points, which usually
ends up on FileFormat.Info anyway, or, worse, on a big Wikipedia page
discussing the history and cultural significance of the `LATIN SMALL LETTER C
WITH CEDILLA` (_ç_) before giving me it's code point U+00e7.

## Entering the Compose Key

A better option is certainly the _Compose Key_. (Check out this [Wikipedia
Article](https://en.wikipedia.org/wiki/Compose_key) for a discussion of its
history and cultural significance). Even though not even the nerdiest of nerd
keyboards come with a physical compose key nowadays, the X Window System still
supports the underlying mechanism. And since X is running on both my Arch Linux
and OpenBSD computers, using its compose key mechanism helps me both at work
and for my private computing, the latter consisting mostly of writing articles
about entering special characters on special computing setups and the like.

A good reference for this mechanism is the manual page `Compose(5)` or
`XCompose(5)` on both Arch Linux and OpenBSD. (I guess it also applies to
FreeBSD, but I didn't check yet). However, there's some additional information
required to do a basic setup, so here's a quick guide.

First, a compose key needs to be picked. There are various options, which can be
looked up as follows:

    # OpenBSD
    $ grep 'compose:' /usr/X11R6/share/X11/xkb/rules/base.lst

    # Arch Linux
    $ grep 'compose:' /usr/share/X11/xkb/rules/base.lst

Which shows a list of keys that could be used for the compose key:

      compose:ralt         Right Alt
      compose:lwin         Left Win
      compose:lwin-altgr   3rd level of Left Win
      compose:rwin         Right Win
      compose:rwin-altgr   3rd level of Right Win
      compose:menu         Menu
      compose:menu-altgr   3rd level of Menu
      compose:lctrl        Left Ctrl
      compose:lctrl-altgr  3rd level of Left Ctrl
      compose:rctrl        Right Ctrl
      compose:rctrl-altgr  3rd level of Right Ctrl
      compose:caps         Caps Lock
      compose:caps-altgr   3rd level of Caps Lock
      compose:102          &amp;lt;Less/Greater&amp;gt;
      compose:102-altgr    3rd level of &amp;lt;Less/Greater&amp;gt;
      compose:paus         Pause
      compose:prsc         PrtSc
      compose:sclk         Scroll Lock

Caps Lock is a good choice, but would make it harder to type anonymous hate mail
in all-caps to people not agreeing on my operating system preferences.
Therefore I pick the Menu key, which I didn't even use once in my dark Windows
days. The compose key can be defined in `~/.xinitrc`, ideally together with the
choice of the aforementioned imperialistic keyboard layout:

    setxkbmap us
    setxkbmap -option compose:menu

On my Thinkpad (running Arch Linux), I prefer the PrtSc key, which is placed
between AltGr and Ctrl on the right hand side of the Space bar. I don't often
take screenshots, since the content of my screen is mostly text, which is
better captured using primary selection. (Did I already mention that I use
Arch, btw.?)

    setxkbmap us
    setxkbmap -option compose:prtsc

After restarting X, the compose key is ready for action. Now I can type `Menu &quot;
O` to enter _Ö_ on my PC (OpenBSD), or `PrtSc ' e` to enter _é_ on my Laptop
(Arch Linux). Not all together at the same time, but as a sequence, which is
much more convenient.  Pre-defined sequences can be looked up in the compose
files under `/usr/X11/share/X11/locale/[locale]/Compose` on OpenBSD, or
`/us/share/X11/locale/[locale]/Compose` on Arch Linux, respectively. As locale,
I simply use `en_US.UTF-8` (imperialism, remember?), which gives me a wide
range of sequences (hand-picked examples):

    &lt;Multi_key&gt; &lt;C&gt; &lt;o&gt; 			: &quot;©&quot;   copyright # COPYRIGHT SIGN
    &lt;Multi_key&gt; &lt;R&gt; &lt;o&gt; 			: &quot;®&quot;   registered # REGISTERED SIGN
    &lt;Multi_key&gt; &lt;plus&gt; &lt;minus&gt;     	: &quot;±&quot;   plusminus # PLUS-MINUS SIGN
    &lt;Multi_key&gt; &lt;s&gt; &lt;s&gt;            	: &quot;ß&quot;   ssharp # LATIN SMALL LETTER SHARP S
    &lt;Multi_key&gt; &lt;E&gt; &lt;equal&gt;        	: &quot;€&quot;   EuroSign # EURO SIGN
    &lt;Multi_key&gt; &lt;u&gt; &lt;slash&gt; 		: &quot;µ&quot;   mu # MICRO SIGN
    &lt;Multi_key&gt; &lt;quotedbl&gt; &lt;A&gt;     	: &quot;Ä&quot;   Adiaeresis # LATIN CAPITAL LETTER A WITH DIAERESIS
    &lt;Multi_key&gt; &lt;acute&gt; &lt;E&gt;        	: &quot;É&quot;   Eacute # LATIN CAPITAL LETTER E WITH ACUTE
    &lt;Multi_key&gt; &lt;asciitilde&gt; &lt;N&gt;   	: &quot;Ñ&quot;   Ntilde # LATIN CAPITAL LETTER N WITH TILDE
    &lt;Multi_key&gt; &lt;comma&gt; &lt;c&gt;        	: &quot;ç&quot;   ccedilla # LATIN SMALL LETTER C WITH CEDILLA
    &lt;Multi_key&gt; &lt;v&gt; &lt;z&gt; 			: &quot;ž&quot;   U017E # LATIN SMALL LETTER Z WITH CARON

Those sequences are very intuitive to type, so looking them up will hardly be
needed. This configuration also allows me to type in scientific
transliterations of Russian words, such as _Č_ in _Čechov_ (`Menu &lt; C`) instead
of the German _Tschechow_ or the English _Chekov_. Same with _š_ in _Puškin_
and _Ž_ in _Dr. Živago_.

If those sequences do not suffice for one's particular needs, more sequences can
be defined in the file `~/.XCompose`. When doing so, it is important to also
include the original definitions mentioned above as follows, so that it's not
needed to re-define them all:

    include &quot;%S/en_US.UTF-8/Compose&quot;

The `%S` resolves to the path `/usr/X11R6/share/X11/locale` on OpenBSD and
`/usr/share/X11/locale` on Arch Linux. (`%L` would resolve to the current
locale and, thus, would be shorter, but I prefer the more explicit way using
`%S`.) Additional rules can be defined as follows:

    &lt;Multi_key&gt; &lt;colon&gt; &lt;minus&gt; &lt;parenright&gt; : &quot;☺&quot; U263A # WHITE SMILING FACE

This allows to type the smiley using `Menu : - )` on my OpenBSD machine, and
`PrtSc : - )` on my Arch Linux laptop. The left hand side of the colon defines
the key sequence. The right hand side defines a key symbol (`keysym` in X
lingo), a string, or both, followed by an optional comment. It is also possible
to enter whole strings in that manner:

    &lt;Multi_key&gt; &lt;m&gt; &lt;a&gt; &lt;c&gt; : &quot;fanboy&quot;

Whose effect I leave for you to figure out.

# Update (2020-09-17)

A former [fellow student](https://github.com/chefe) just made me aware of the
keyboard variant `altgr-intl` for the US layout. It can be set as follows:

    $ setxkbmap us -variant altgr-intl

This layout variant offers some useful shortcuts, all to be entered in
combination with AltGr:

- AltGr + q: ä
- AltGr + y: ü
- AltGr + p: ö
- AltGr + e: é
- AltGr + r: ë
- AltGr + s: ß

Unfortunately, an e with grave accent is not easily available, but the
`altgr-intl` variant is still a good alternative for German.
</content>
    </entry>
    <entry>
        <title>OpenBSD on the Desktop (Part II)</title>
        <link href="https://paedubucher.ch/articles/2020-09-12-openbsd-on-the-desktop-part-ii.html"/>
        <author>
            <name>Patrick Bucher</name>
        </author>
        <id>https://paedubucher.ch/articles/2020-09-12-openbsd-on-the-desktop-part-ii.html</id>
        <updated>2020-09-12T15:00:00Z</updated>
        <content type="text/markdown; charset=UTF-8">
A week ago, I've installed [OpenBSD on my
Thinkpad](https://paedubucher.ch/articles/2020-09-05-openbsd-on-the-desktop-part-i.html).
I've been using it now and then, and already have changed a couple of things in
respect to the original setup described in the article. I also installed OpenBSD
on the Dell Optiplex on which I [previously installed
FreeBSD](https:///paedubucher.ch/articles/2020-08-11-freebsd-on-the-desktop.html)
a month before. This means that I'm no longer using FreeBSD on the desktop, at
least not for the moment. However, FreeBSD is running on a disk station I built
earlier this summer. Maybe I'll describe that particular setup (using ZFS) in a
later article.

Except for that storage server, I'd like to use OpenBSD for most of my private
computing. In this article, I describe some GUI tweaks and additional setup
tasks I perfmormed in order to feel more at home on my OpenBSD machines. Some of
the tasks performed are _not_ specific to OpenBSD, but could also be applied to
a Linux setup.

# doas

`sudo` originally came from the OpenBSD community. It is almost as widely used
in the Unix world as SSH, which is the most prominent OpenBSD project.  However,
`sudo` became bigger and harder to configure. Therefore, Ted Unangst came up
with a simpler alternative called `doas`, which stands for _Dedicated OpenBSD
Application Subexecutor_. `doas` is less powerful than `sudo`, but much smaller,
easier to configure, and, thus, more secure. The full rationale can be read in
[Ted Unangst's Blog](https://flak.tedunangst.com/post/doas).

A basic `doas` setup requires to login as root for one last time. The
configuration shall be kept extremely simple. I'd like to permit all users from
the `wheel` group (which is just me on my computers) to use `doas` without
entering the password every time but only once when executing a command that
requires `root` permissions. This is only a single line in `/etc/doas.conf`:

    permit persist :wheel    

Let's check this setup by logging in as a user of the wheel group and trying to
update the packages:

    $ doas pkg_add -u

This works, so bye bye `root` account.

# Fonts for dwm, dmenu, and st

By default, `dwm`, `dmenu`, and `st` use a monospace font of size 10, or
pixelsize 12, respectively, which is hard to read on a screen with a high
resolution. On Linux, I use the the TrueType font DejaVu Sans Mono. For OpenBSD,
I'd rather use something more minimalistic: the [Terminus bitmap
font](http://terminus-font.sourceforge.net/).

As `pkg_info -Q terminus` shows, this font comes in different versions. I prefer
the version with the centered tilde, which I install:

    $ doas pkg_add terminus-font-4.47p0-centered_tilde

Let's reconfigure `st` first, for testing changes doesn't require a restart of
the window manager. I stored my suckless sources in `~/suckless`, so the
font for `st` can be configured in `~/suckless/config.h`. I replace the existing
font configuration

    static char *font = &quot;Liberation Mono:pixelsize=12:antialias=true:autohint=true&quot;;

with

    static char *font = &quot;Terminus:pixelsize=24&quot;;

The options `antialias` and `autohinting` are not needed for a bitmap font, so I
left them away. 24 pixels is rather big, but my screen is big enough to show two
text editors with more than 80 characters per line next to each other, so let's
keep it this way. I rebuild and reinstall `st`, then switch to `dwm`:

    $ doas make install
    $ cd ../dwm

The font configuration in the `config.h` file looks a bit different here:

    static const char *fonts =      { &quot;monospace:size=10&quot; };
    static const char dmenufont =   &quot;monospace:size=10&quot;;

Let's just use the same font as for `st` here:

    static const char *fonts =      { &quot;Terminus:pixelsize=24&quot; };
    static const char dmenufont =   &quot;Terminus:pixelsize=24&quot;;

Note that I'm using `pixelsize` instead of `size` here. (24pt would be much
bigger than 24px.) Then I rebuild and reinstall `dwm`.

    # make install

This configuration appllies also to `dmenu` and `slstatus`, so we're done with
the fonts.

# X Background

By default, the desktop background is a pattern of black and grey dots, which is
a strain to the eye. Even though I rarely look at an empty desktop for long, I'd
rather change this to a solid color. This can be done by adding a command to
`~/.xinitrc`:

    xsetroot -solid black

Right before `dwm` is executed.

# USB Flash Drive

Even though SSH is almost ubiquitous nowadays, a USB flash drive is still useful
when it comes to exchanging data between computers, especially if Windows is
involved, or if the network does not allow SSH.

Block storage devices are accessible through the device nodes `/dev/sd*`,
whereas `*` stands for the number of the disk. The disks can be listed as
follows:

    $ sysctl hw.disknames
    hw.disknames=sd0:ef0268c97ae7a246

Only `sd0` is active, even though I already plugged in my USB dongle. However,
the system already figured out that there is a second disk:

    $ sysctl hw.diskcount
    hw.diskcount=2

The next free disk would have the name `sd1`. The device nodes can be created by
running the `MAKEDV` script in `/dev`:

    $ cd /dev
    $ doas sh MAKEDEV sd1

Let's initialize a new MBR partition schema on `sd1`:

    $ doas fdisk -iy sd1

The new disk layout can be checked using `disklabel`:

    $ doas disklabel sd1
    # /dev/rsd1c
    ...

The first line of the output tells us that there's a partition under
`/dev/rsd1c`. (The `r` refers to «raw», as opposed to «block».) The partition
can be formatted using `newfs` by referring to that partition name:

    $ doas newfs sd1c

This creates a default FFS (Fast File System) partition, which is useful to
exchange data between BSD operating systems. The formatted partition is then
ready to be mounted:

    $ doas mount /dev/sd1c /mnt

## Other Partition Types

Other partition types are available under other utilities.

### FAT32

The following command creates a FAT32 partition:

    $ doas newfs_msdos -F 32 sd1c

The `-F 32` parameter specifies FAT32 (as opposed to FAT16 or FAT8). To mount
the partition, use the according `mount` command:

    $ doas mount_msdos /dev/sd1c /mnt

### EXT2

In order to create an `ext2fs` file system, the partition type needs to be
specified accordingly. First, you might consider a GPT partition schema instead
of MBR (additional `-g` parameter):

    $ doas fdisk -igy sd1

Then use `disklabel` interactively to define a new partition:

    $ doas disklabel -E sd1

First, delete all the partitions with `z`. Then, create a new partition with
`a`, and make sure to specify the type as `ext2fs` instead of the default
`4.2BSD`. Notice that the new partition has a different letter (say, `a`), so
you need to use `sd1a` instead of `sd1c` for the next steps. Write the changes
by typing `w`, then exit with `q`. Now you can format and mount the partition:

    $ doas newfs_ext2fs sd1a
    $ doas mount_ext2fs /dev/sd1a /mnt

# SSH Key (GitHub)

In order to access my GitHub repositories, I first create a new SSH key:

    $ ssh-keygen -t rsa -b 4096

Since I manage my passwords with `pass` (of which more later), I don't know most
of them by heart. So I can't just login to GitHub and add my public key.
Therefore, I copy my public key to my laptop, on which I'm already logged in to
GitHub.

This can be either done using `scp`, for which `sshd` has to be running on my
laptop (which currently has the IP `192.168.178.53`):

    $ scp ~/.ssh/id_rsa.pub 192.168.178.53:/home/patrick

Or using the USB flash drive formatted with `ext2` from before:

    $ doas newfs_ext2fs -I sd1a
    $ doas mount_ext2fs /dev/sd1a /mnt
    $ doas cp ~/.ssh/id_rsa.pub /mnt/

Then `id_rsa.pub` can be copied into the according [GitHub Settings
Page](https://github.com/settings/ssh/new), after which cloning GitHub
repositories should work on the OpenBSD machine:

    $ git clone git@github.com:patrickbucher/conf

# GPG Key

My passwords are encrypted using GPG. To encrypt them, I need to copy my private
key from my other machine. First, I list my private keys:

    $ gpg --list-keys --keyid-format SHORT
    pub   rsa2048/73CE6620 2016-11-11 [SC]
          22F91EE20D641CBCF5B8678E82B7FE3A73CE6620
    uid         [ultimate] Patrick Bucher &lt;patrick.bucher@mailbox.org&gt;
    sub   rsa2048/AF6246E3 2016-11-11 [E]

Then I export both public and private key to an according file using the armored
key format:

    $ gpg --export --armor 73CE6620 &gt; public.key
    $ gpg --export-secret-key --armor 73CE6620 &gt; private.key

The two key files can be copied via SSH or the USB flash disk again, which I
won't show here.

Back on my OpenBSD machine, I need to install GnuPG first, because OpenBSD only
has `signify` installed by default:

    $ doas pkg_add gnupg

I pick the 2.2 version. Now I can import my keys:

    $ gpg2 --import private.key
    $ gpg2 --import public.key

The key is not trusted so far, so I need to change that:

    $ gpg2 --edit-key 73CE6620
    &gt; trust
    &gt; 5
    &gt; y
    &gt; quit

5 stands for ultimate trust, which seems appropriate.

# Password Manager

I use `pass` as a password manager, which can be installed as the
`password_store` package in OpenBSD:

    $ doas pkg_add password-store

Now that I have both my GPG private key and a working SSH key for GitHub, I can
clone my passwords stored on a private GitHub repository:

    $ git clone git@github.com:patrickbucher/pass .password-store

Now I can copy my GitHub password to the clipboard as follows:

    $ pass -c github

# Aliases

I use a lot of aliases, such as `gcl` as a shortcut for `git clone`, and `gad`
for `git add`, etc. Since OpenBSD uses a Public Domain Korn Shell by default,
the `.bashrc` configuration from my Linux machines won't work here, unless I
switch to `bash`, which is not exactly the point of using OpenBSD.

I define my aliases in `~/.kshrc` (excerpt):

    alias gcl='git clone'
    alias gad='git add'

In order to load those settings, an according `ENV` parameter needs to be
defined in `~/.profile` (see `man 1 ksh` for details):

    export ENV=$HOME/.kshrc

After the next login, `~/.profile` is reloaded, and the aliases are ready to be
used.

# Conclusion

Not only is my enhanced setup now ready to do some serious work, but I also
increased my understanding of some OpenBSD subjects. There are still things to
be improved and to be understood, but my setup is now good enough so that I no
longer need a Linux machine running next to it. I'm looking forward to use and
learn about OpenBSD in the time to come. I'll write additional articles on the
subject as soon as I have enough subject material ready.
</content>
    </entry>
    <entry>
        <title>OpenBSD on the Desktop (Part I)</title>
        <link href="https://paedubucher.ch/articles/2020-09-05-openbsd-on-the-desktop-part-i.html"/>
        <author>
            <name>Patrick Bucher</name>
        </author>
        <id>https://paedubucher.ch/articles/2020-09-05-openbsd-on-the-desktop-part-i.html</id>
        <updated>2020-09-05T20:00:00Z</updated>
        <content type="text/markdown; charset=UTF-8">
Let's install OpenBSD on a Lenovo Thinkpad X270. I used this computer for my
computer science studies. It has both Arch Linux and Windows 10 installed as
dual boot. Now that I'm no longer required to run Windows, I can ditch the dual
boot and install an operating system of my choice.

# Preparation

First, I grab my work Thinkpad running Arch Linux and some USB dongle big enough
for the [amd64 miniroot
image](https://cdn.openbsd.org/pub/OpenBSD/6.7/amd64/miniroot67.fs) (roughly
five megabytes, that is). This small image does not include the file sets, which
will be downloaded during installation instead. I also download the [SHA256
checksums](https://mirror.ungleich.ch/pub/OpenBSD/6.7/amd64/SHA256) from the
Swiss mirror, and verify the downloaded image, before I copy it on my dongle:

    $ sha256sum -c --ignore-missing SHA256 
    miniroot67.fs: OK
    $ sudo dd if=miniroot67.fs of=/dev/sda bs=1M

# Installation

The Thinkpad X270 is connected to my network through Ethernet. The WiFi firmware
usually needs to be installed separately, so only Ethernet will work out of the
box. The BIOS has UEFI activated. OpenBSD and UEFI has issues on older hardware
(at least on a 2014 Dell laptop I have), but let's try it on this laptop,
anyway.

I plug in the dongle prepared before, and start the computer. I interrupt
the regular boot with Enter and pick an alternative boot method by pressing F12.
Now I pick my USB dongle. After roughly a minute, the installer has been
started. Now I follow these steps:

- I choose the option `I` to install OpenBSD.
- For the keyboard layout, I pick `sg`, for Swiss German.
- As a hostname, I simply pick `x270`, because it's a Thinkpad X270, and I'm not
  very creative when it comes to naming things.
- From the available network options (`iwm0`: WiFi, `em0`: Ethernet, and
  `vlan0`: Virtual LAN), I pick `em0`.
- I try to get an IPv4 address over DHCP, which seems to work very quickly.
- Next, I type in my very secret root password twice.
- I do _not_ start `sshd` by default, because I don't need to connect to this
  machine through SSH. It's supposed to be a workstation, not a server.
- The X Window System should not be started by `xnodm(1)`, so I leave it to
  `no`.
- Neither do I want to change the default to `com0`.
- I set up my user `patrick` with my proper name `Patrick Bucher`, and a decent
  password.
- The time zone has been detected properly as `Europe/Zurich`, which I just
  leave the way it is.
- The installer detected two disks: `sd0` and `sd1`. Since `sd0` is the detected
  SSD in my laptop, the UEFI issue from my Dell laptop doesn't exist on this
  computer. I pick `sd0` for the root disk, since `sd1` is my USB dongle.
- I choose to use the whole disk with a GPT partitioning schema, because it's
  2020.
- An auto-allocated layout for `sd0` is presented. It looks decent to me, so I
  just go with that auto layout.
- I don't want to initialize another disk, so I just press Enter (`done`).
- Since the miniroot image does not come with the file sets, I pick `http` as
  the location for the sets.
- I don't use a proxy, and use the mirror `mirrog.ungleich.ch` and the server
  directory `pub/OpenBSD/6.7/amd64` as proposed.
- Next, I unselect the game sets by entering `-game*`. (I heard that they're not
  much fun to play.) I leave all the other sets activated, including the `x`
  sets, which will be required for the GUI later on.
- After those sets are installed, I press Enter (`done`). Now the installer
  performs various tasks, after which I choose to `halt` the computer. This
  gives me time to remove the USB dongle.

# First Boot

I now restart my laptop, and OpenBSD boots. This takes more time than booting
Arch Linux, which uses `systemd`, whereas OpenBSD uses `rc`, which performs the
startup tasks sequentially.

There's a message showing up that various firmware (`intel-firmware`,
`iwm-firmware`, `inteldrm-firmware`, `uvideo-firmware`, and `vmm-firmware`) has
been installed automatically. Very nice, indeed.

## WiFi Connection

Now that the `iwm-firmware` has been installed, I can connect right away to my
WiFi network `frzbxpdb5`. I create a file called `/etc/hostname.iwm0`, wich
`hostname` being a literal string, and `iwm0` being the WiFi network card. The
connection to my WiFi network consists of a single line:

    dhcp nwid frzbxpdb5 wpakey [my-wpakey]

Whereas `frzbxpdb5` is my WiFi network's ESSID, and `[my-wpakey]` needs to be
replaced by the actual WPA key.

Then the networking can be restarted for that device:

    # sh /etc/netstart iwm0

This script is kind enough to set the file permissions of `/etc/hostname.iwm0`
to `640`, and then connects to my WiFi network.

I unplug the Ethernet cable and `ping openbsd.org`, which works fine, even after
a restart.

# Installing the GUI

My GUI on Unix-like systems is based on the Dynamic Window Manager (`dwm`) and a
couple of other tools, such as `dmenu`, `st`, `slstatus`, `slock`, all created and
maintained by the [Suckless](http://suckless.org/) community.

This software doesn't come with configuration facilities, but needs to be
configured in the respective C header file `config.h`, and then re-compiled.
Even though OpenBSD offers `dwm` as a package, customizing and configuring that
window manager requires to build it from source.

## Building `dwm` and Friends

First, I need to install `git` to fetch the source code:

    # pkg_add git

Then I fetch the source code for `dwm`, `dmenu`, `st`, and `slstatus` from [Suckless](http://suckless.org/):

    $ git clone https://git.suckless.org/dwm
    $ git clone https://git.suckless.org/dmenu
    $ git clone https://git.suckless.org/st
    $ git clone https://git.suckless.org/slstatus

### Building `dwm`

Next, I try to build `dwm`:

    $ cd dwm
    $ make

This fails with an error message (`'ft2build.h' file not found`), which reminds
me of building `dwm` on FreeBSD roughly a month before. Since I can finde the
header file at another location:

    # find / -type f -name ft2build.h
    /usr/X11R6/include/freetype2/ft2build.h

I simply can modify the `config.mk` accordingly by changing

    FREETYPEINC = /usr/include/freetype2

to

    FREETYPEINC = $(X11INC}/freetype2

Actually, I only need to comment the above line, and uncomment the line below

    # OpenBSD (uncomment)

The Suckless folks obviously are friendly towards OpenBSD, which is also
noticable in other places (more evidence to be shown further below).

The next compilation attempt succeeds:

    $ make

So let's install `dwm`, too:

    # make install

By default, and as to be seen in `config.h`, the keyboard combination
`[Alt]+[Shift]+[Enter]` (deeply engraved into the muscle memories of many `dwm`
users) starts the `st` terminal. This will be built in a while. However, I
prefer to use the _Super_ or _Windows_ key instead of `Alt`, since the former
is of no use in OpenBSD, and the latter still comes in handy when working with
the emacs readline mode. Therefore, I change the `MODKEY` from

    #define MODKEY Mod1Mask

to

    #define MODKEY Mod4Mask

Then I rebuild and reinstall `dwm`:

    # make install

### Building `st`

Let's switch over to the `st` source directory and just try to compile it:

    $ cd ../st
    $ make

Here, we get a warning that the function `pledge` (an OpenBSD mitigation, which
is built into the `master` branch, but surrounded by an `ifdef` preprocessor
statement, so that it will only be compiled for OpenBSD) is imported implicitly.
Let's just ignore this warning for now.

What's worse, the compilation fails with the error message:

    ld: error: unable to find library -lrt

Here, the FAQ comes in handy, stating that

    If you want to compile st for OpenBSD you have to remove -lrt from
    config.mk, ...

Having done so in `config.mk`, `st` compiles without any further issues, and,
thus, can be rebuilt and installed:

    # make install

### Building `dmenu`

Even OpenBSD users with Suckless tools have to open another GUI application than
a terminal emulator once in a while. For this purpose, Suckless offers `dmenu`.
Let's switch over to it and compile it:

    $ cd ../dmenu
    $ make

Again, we have the issue with `ft2build.h`, which can be resolved as above with
`dwm`: by using the proper path for `FREETYPEINC` in `config.mk`. Afterwards,
the build succeeds, and `dmenu` can be installed:

    # make install

### Building `slstatus`

`dwm` has a status bar on the top right, which can be used to show various
information. I used to write some shell commands in `.xinitrc` to compose such a
status line, and then set it by `xset -b` once every five seconds or so. This
approach generates a multitude of processes every couple of seconds.

`slstatus` is a C programm that is capable of showing various kinds of more or
less useful information. Let's switch over to `slstatus` and see, what is
available in `config.def.h`:

    $ cd ../slstatus
    $ less config.def.h

The comments section lists different functions (`battery_perc` for the battery
percentage, `datetime` for date and time information, `temp` for thermal
information, etc.). I usually display the CPU load, the battery percentage, the
memory usage, the current keyboard layout, and the current date and time.

Before configuring those, let's try to compile `slstatus`:

    $ make

This worked fine, so let's configure the information to be displayed in
`config.h`:

    static const struct arg args[] = {
        /* function    format    argument */
        { datetime,    &quot;%s&quot;,     &quot;%F %T&quot; },
    };

This renders the current date as follows:

    $ date +&quot;%F %T&quot;
    2020-09-05 19:26:38

I also like to have the weekday included, but not the seconds, so I define a
different argument string:

    $ date +&quot;%a %Y-%m-%d %H:%M&quot;
    Sat 2020-09-05 19:27

That's better, so let's use it in `config.h` (surrounded with some spaces in the
format string):

    static const struct arg args[] = {
        /* function    format    argument */
        { datetime,    &quot; %s &quot;,   &quot;%a %Y-%m-%d %H:%M&quot; },
    };

The other settings I like to have do not require any arguments, at least not on
OpenBSD, so I only need to define a decent format string (with `|` as a
seperator) for those:

    static const struct arg args[] = {
        /* function    format           argument */
        { cpu_perc,     &quot; cpu: %s%% |&quot;, NULL },
        { battery_perc, &quot; bat: %s%% |&quot;, NULL },
        { ram_used,     &quot; mem: %s |&quot;,   NULL },
        { keymap,       &quot; %s |&quot;         NULL },
        { datetime,     &quot; %s &quot;,         &quot;%a %Y-%m-%d %H:%M&quot; },
    };

This actually compiles, so let's install it:

    # make install

## Configuring X Startup

Now that all software is compiled and installed, let's run X. To do so, a file
`.xinitrc` in the user's directory is required (`/home/patrick/.xinitrc`):

    setxkbmap ch
    slstatus &amp;
    exec dwm

This sets the keyboard map for X to Swiss German, starts `slstatus` in the
background, and then executes `dwm`.

X can now be started by typing `startx`. This is a bit cumbersome to type every
time, so let's define a symbolic link to it:

    # ln -s &quot;$(which startx)&quot; /usr/bin/x

Now let's start X:

    $ x

If everything was configured properly, `dwm` shows up, and the status line says
that the whole system only uses roughly 60 megabytes of RAM. That's slim. The
keyboard combinations to open `st` and `dmenu` work, too.

# Conclusion

Installing a basic GUI with Suckless software was a rather smooth experience on
OpenBSD. (For FreeBSD, I deliberately have chosen a rather fine-grained approach
to installing X packages, which caused some additional work.) However, various
settings require additional tweaking. I also didn't use audio yet, which require
the volume buttons to be configured accordingly in `dwm`.

I'll also need to setup `sudo` or `doas`. As a regular Linux user, I'm used to
`sudo`, of course, but the simplicity of `doas` is a good argument to try it as
an alternative.

But those are things I'd like to cover in an upcoming article.
</content>
    </entry>
    <entry>
        <title>FreeBSD on the Desktop</title>
        <link href="https://paedubucher.ch/articles/2020-08-11-freebsd-on-the-desktop.html"/>
        <author>
            <name>Patrick Bucher</name>
        </author>
        <id>https://paedubucher.ch/articles/2020-08-11-freebsd-on-the-desktop.html</id>
        <updated>2020-08-11T22:00:00Z</updated>
        <content type="text/markdown; charset=UTF-8">
I'm a happy user of [Arch Linux](https://www.archlinux.org/) both on my private
computers and on my work laptop. I even managed to get through four years of
university with my setup, and only had to bring a Windows machine on some rare
occasions, even though some professors are openly hostile towards a Linux setup.
(It doesn't run Microsoft Project and the real Excel, after all…)

Recently, I got interested in the BSDs, especially in
[OpenBSD](https://www.openbsd.org/) and in [FreeBSD](https://www.freebsd.org/).
Even though OpenBSD with its minimalistic appeal is better suited to my taste,
I'm currently looking at FreeBSD for a couple of reasons. First, I have to
maintain a storage server (a FreeBSD box using ZFS) at work. Second, I've also
built up a storage server at home. FreeBSD gives me the things I need mostly out
of the box: ZFS with redundancy on really cheap hardware. And third, I just like
to learn new things.

# Why FreeBSD?

However, when it comes to learning new things in my spare time, I'd rather spend
my time on something that will be useful in the long run. I use the [Lindy
Effect](https://en.wikipedia.org/wiki/Lindy_effect) as a guide: Technologies
like Kubernetes, the latest JavaScript framework, or Web Assembly have only been
around for a couple of years, and it's possible that those will undergo major
changes or vanish alltogether as fast as they came. Older technologies _that are
still around nowadays_, on the other hand, can be expected to be around for many
more years. Examples are the C programming language, various Unix shells, and ‒
FreeBSD. (I also make exceptions to this rule now and then. For example, I
learned Go and Rust in the summers of 2018 and 2019, respectively. Go, which had
its 1.0 release earlier than Rust, proofed to be the more stable choice.)

FreeBSD is now more than 25 years old. Its roots, however, go back to AT&amp;T's
original Unix from the 1970s. (All the code has been replaced or rewritten
since.) Being old is not enough, of course; a technology is only worthwhile
learning if it is still alive. Even though FreeBSD is rarely on the front page
of [Hacker News](https://news.ycombinator.com/), it is still widely used.
[Netflix](https://papers.freebsd.org/2019/fosdem/looney-netflix_and_freebsd/)
streams videos through FreeBSD systems, and the operating system of the
PlayStation 4, [Orbis
OS](https://en.wikipedia.org/wiki/PlayStation_4_system_software), is based on
FreeBSD.

FreeBSD is not only likely to stay around for a long time, it probably also
won't undergo fundmental changes very soon or very often. One example is the
startup system of FreeBSD, which is still based on `init` and `rc` scripts.
Ubuntu, on the other side, switched from SysVinit to Upstart in 2006, and again
from Upstart to systemd in 2015. That is one init system to learn for a lifetime
(FreeBSD) vs. three in less than a decade (Ubuntu).

Documentation is another advantage of FreeBSD. Having a system that rarely
introduces breaking changes makes it easier and more worthwile to provide good
documentation. The FreeBSD team not only provides good [manual
pages](https://www.freebsd.org/cgi/man.cgi), but also a well curated
[FAQs](https://www.freebsd.org/doc/en_US.ISO8859-1/books/faq/), a
[Wiki](https://wiki.freebsd.org/), and the very useful
[Handbook](https://www.freebsd.org/doc/en_US.ISO8859-1/books/handbook/).
FreeBSD material has such a long shelf life so that it is even worthwhile to
print books about that operating system. I'm reading [Absolute
FreeBSD](https://nostarch.com/absfreebsd3) now (from front to back, that is).
Michael Warren Lucas, the author of this book, has written [even more
books](https://www.tiltedwindmillpress.com/product-category/tech/) on FreeBSD
(and OpenBSD), which are not only useful and of high-quality, but also
well-written and fun to read.

As I started to work with FreeBSD, I suddenly realized what the term
«distribution» is probably supposed to mean: Not just a bunch of software
cobbled together with more or less frequent upgrades, but an entire operating
system that not only provides working software, but also the means to build that
very software on a standard installation. (Such a system is technically
described as _self hosting_.) A kernel can be compiled and installed with a
single command. Thanks to the ports tree, the packages can be compiled easily
and in a consistent way. I haven't tried _all_ Linux distributions, of course,
but quality standards that high are certainly not the rule in the GNU/Linux
world. (Debian and Arch, the Linux distributions I use the most and know the
best, are still absolutely great operating systems.)

# FreeBSD on the Desktop?

It's safe to say that FreeBSD is a good choice for servers in a Unix
environment. But does it also work well on a desktop computer? And is it even an
option for the kind of desktop I like to run: not GNOE, KDE, or Xfce, but a
minimalistic setup based on [dwm](http://dwm.suckless.org/), which probably
isn't used by many. Since dwm can only be configured by modifying the `config.h`
file, I won't be able to use the version from the ports tree. I use my desktop
computer mainly for work (programming, reading, writing, researching information
on the internet) and some entertainment mostly provided through the web browser
(reading, videos).

I generally use mid-range commodity hardware with on-board graphics, so hardware
compatibility should not be an issue. My desktop computer is a small-form Dell
OptiPlex 7040 from 2016 with an Intel Core i5 CPU. I replaced the original 128
TB SSD with a 500 GB model last year, and upgraded the original 8 GB of memory
with a ridiculous amount of 24 GB. (Some RAM bars just happened to lay around
here…) The computer has a WiFi card, of course, but since my router is just next
to my desk, I rather use a stable ethernet connection.

I already tried out OpenBSD once on that computer, and didn't have any issues
getting it to run. So FreeBSD having access to the same code base under a
compatible license is likely to suppport this hardware as well. Let's just try
it out!

# Preparations

FreeBSD supports multiple versions at any given point in time. At the time of
this writing, 12.1 and 11.4 are the current versions intended for production.
Let's pick the most recent version 12.1. The [download
page](https://download.freebsd.org/ftp/releases/amd64/amd64/ISO-IMAGES/12.1/)
for the `amd64` architecture offers various options. The compressed
`mini-memstick` archive weighs the least and provides everything that is needed
for an installation on a computer with internet connection. I download it to my
laptop running Arch Linux, and then verify the checksum:

    $ wget https://download.freebsd.org/ftp/releases/amd64/amd64\
    /ISO-IMAGES/12.1/FreeBSD-12.1-RELEASE-amd64-mini-memstick.img.xz
    $ wget https://download.freebsd.org/ftp/releases/amd64/amd64/\
    ISO-IMAGES/12.1/CHECKSUM.SHA512-FreeBSD-12.1-RELEASE-amd64
    $ sha512sum -c CHECKSUM.SHA512-FreeBSD-12.1-RELEASE-amd64 --ignore-missing
    FreeBSD-12.1-RELEASE-amd64-mini-memstick.img.xz: OK

The archive (389 MBs) needs to be unpacked and is then copied to a USB dongle
(`/dev/sda`):

    $ unxz FreeBSD-12.1-RELEASE-amd64-mini-memstick.img.xz
    # dd if=FreeBSD-12.1-RELEASE-amd64-mini-memstick.img of=/dev/sda bs=1M
    $ sync

# Initial Setup

The BIOS is setup to use UEFI rather than legacy boot. I plug in the USB dongle
and start the FreeBSD installer. These are the settings I use during setup:

- Keymap: Swiss-German (`ch.kdb`)
- Hostname: `optiplex` (I'm not very imaginative when it comes to naming
  things.)
- Components: just `lib32`, `ports`, and `src`
- Network: interface `em0` with DHCP and IPv4
- Mirror: Main Site
- Partitioning: Auto (UFS, I'm going to learn about ZFS later) on the entire
  disk `ada0` using GPT and the following partitions (device, space, type,
  label, mount point):
    - `ada0p1`: 200 MB `efi` boot (no mount point)
    - `ada0p2`: 24 GB `freebsd-swap` swap0 (no mount point, size of physical memory)
    - `ada0p3`: 16 GB `freebsd-ufs` root `/`
    - `ada0p4`: 4 GB `freebsd-ufs` temp `/tmp`
    - `ada0p5`: 4 GB `freebsd-ufs` var `/var`
    - `ada0p6`: 32 GB `freebsd-ufs` usr `/usr`
    - `ada0p7`: 386 GB `freebsd-ufs` home `/home` (remainder of the space)
- Root password: I won't tell you, but a strong one!
- CMOS clock: UTC
- Time Zone: Europe/Switzerland (`CEST`)
- Services: `sshd`, `moused`, `ntpd`, `powerd`, and `dumpdev`
- Security Hardening Options: everything
- User: `patrick` with additional group `wheel` (to become root), the `tcsh`
  shell, a strong password, and, otherwise, suggested settings

It can be argued if the chosen partition sizes are reasonable. However, it is
always a good idea to use separate `/tmp` and `/var` partitions to make sure
that no process can fill up the entire disk. (Using a separate `/usr` partition
is an issue on Linux nowadays, since the widely used init system systemd
requires access to `/usr`. On FreeBSD, it is still possible to do so without any
issues.)

Make sure to use `efi` as the type for the boot partition, not `freebsd-boot` as
suggested in _Absolute FreeBSD_ (3rd Edition on page 36).

# First Boot, First Issues

After the installation, I choose to shutdown the system. I unplug my USB dongle
as soon as the screen turns black.

Before the first boot, I have to change my boot options in the BIOS so that the
computer boots from the SSD on which FreeBSD was just installed.

The system boots and even shows my mouse on the terminal! The network is up and
running. However, there is a message warning me that the leapsecond file is
expired. The
[solution](https://forums.FreeBSD.org/threads/leapseconds-file-expired.56645/post-322290) suggested in the FreeBSD Forum

    # service ntpd onefetch

fails with a certificate verification error. A [bug
report](https://bugs.freebsd.org/bugzilla/show_bug.cgi?id=230017) suggest to
install the package `ca_root_nss`:

    # pkg install ca_root_nss

Which not only installs the package management software, but also solves the
issue above: The warning concerning the leapsecond file doesn't appear after the
next boot. Now that the basic system is up and running, let's tackle the GUI!

# Installing the GUI

Since _Absolute FreeBSD_ doesn't cover graphical user interfaces, I have to
resort to the handbook. In [Chapter
5.3](https://www.freebsd.org/doc/handbook/x-install.html) it says that the
easiest way to setup the X Window System is to install the `xorg` package. Since
I prefer a minimalistic setup, I opt for the `x11/xorg-minimal` package instead:

    # pkg install x11/xorg-minimal

This package depends on Python 3.7, Perl 5, and Wayland, among others, and
weighs roughly 1 GB, which is not exactly minimalistic in my opinion. On the
other hand, it is notable that the base setup works without Perl or Python.
(Which _is_ minimalistic.)

## Compiling `dwm`

Since I like to keep my `dwm` version up to date, I fetch the sources using
`git`, which first needs to be installed:

    # pkg install git
    # git clone https://git.suckless.org/dwm

A first naive compilation attempt fails:

    # cd dwm
    # make
    drw.c:5:10: fatal error: 'X11/Xlib.h' file not found
    #include &lt;X11/Xlib.h&gt;

The `config.mk` file expects the header files to be located under
`/usr/X11R6/include`. However, FreeBSD has those files stored under a different
location:

    # find / -type f -name Xlib.h
    /usr/local/include/X11/Xlib.h

So in `config.mk`, the lines

    X11INC = /usr/X11R6/include
    X11LIB = /usr/X11R6/lib

need to be replaced with

    X11INC = /usr/local/include
    X11LIB = /usr/local/lib

The next compilation fails with another error (different error message, yay!):

    # make
    drw.c:6:10: fatal error: 'X11/Xft/Xft.h' file not found

That's the price I have to pay for minimalism, I guess. Executing `pkg search
Xft` reveals the package `libXft`, which I install:

    # pkg install libXft

This shows to be a good idea, because now I'm getting a different error message:

    # make
    Xft.h:39:10: fatal error: 'ft2build.h' file not found

It turns out that the file is on the system, but cannot be found:

    # find / -type f -name ft2build.h
    /usr/local/include/freetype2/ft2build.h

Again, the `local` path segment is missing in `config.mk`:

    FREETYPEINC = /usr/include/freetype2

Which is changed as follows:

    FREETYPEINC = /usr/local/include/freetype2

Retry, fresh error again:

    # make
    dwm.c:40:10: fatal error: 'X11/extensions/Xinerama.h' file not found

The `config.mk` contains the following section:

    # Xinerama, comment if you don't want it
    XINERAMALIBS  = -lXinerama
    XINERAMAFLAGS = -DXINERAMA

So what is _Xinerama_, after all? According to
[Wikipedia](https://en.wikipedia.org/wiki/Xinerama):

&gt; Xinerama is an extension to the X Window System that enables X
&gt; applications and window managers to use two or more physical displays as
&gt; one large virtual display. 

Since I have only one screen, I can do without Xinerama, so I comment out those
lines:

    # Xinerama, comment if you don't want it
    # XINERAMALIBS  = -lXinerama
    # XINERAMAFLAGS = -DXINERAMA

Now `dwm` compiles, and I can install it:

    # make dwm
    # make install

## Starting Xorg with `dwm`

I switch to my personal account and create the file `/home/patrick/.xinitrc`
with the following content:

    exec dwm

Now I run `startx`, which unfortunately fails:

    Fatal server error:
    (EE) no screens found(EE)

The error log `/var/log/Xorg.0.log` does not offer any additional information
that seems helpful to me. It turns out that `/etc/X11` is empty. [Section
5.4](https://www.freebsd.org/doc/handbook/x-config.html) of the handbook is
about Xorg configuration. I create a minimalistic configuration for my graphics
card (onboard Intel GPU) in `/etc/X11/xorg.conf`:

    Section &quot;Device&quot;
        Identifier &quot;Card0&quot;
        Driver     &quot;intel&quot;
    EndSection

I also need to install the display driver with the matching kernel module,
because my choice of `xorg-minimal` from before.

    # pkg install xf86-video-intel drm-kmod

(Note that «drm» doesn't stand for «digital rights management» in this context,
but for «direct rendering modules».) The kernel module can be activated on
startup by adding it to the `rc.conf` as follows:

    # echo 'lkd_list=&quot;/boot/modules/i915kms.ko&quot;' &gt;&gt; /etc/rc.conf

After a restart, the console is shown in a much higher resolution. However,
`startx` now complains about a missing font. Let's install the `xorg-fonts` meta
package, which should provide a monospace font needed for `dwm`:

    # pkg install xorg-fonts

Now, finally, `dwm` works! Since `startx` is long to type, I define the alias
`x` for it in `~/.cshrc`:

    alias x startx

And start `dwm`:

    $ x

## Configure `dwm`

By default, `dwm` uses the Alt key as the modifier key (`MODKEY`). I prefer to
use the «Windows» or «Super» Key, for it has no other purpose on my system.
(`Alt` is useful for some emacs-style readline commands.) To do this, the
`MODKEY` variable has to be changed in `config.h` as follows:

    #define MODKEY Mod4Mask

The default rules make Firefox appear on the last tag, and Gimp to be used with
floating layout, which makes no sense with more recent versions of Gimp. Let's
just undefine those rules:

    static const Rule rules[] = {
        {NULL, NULL, NULL, 0, 0, -1},
    };

I also like my windows to be split evenly:

    static const float mact = 0.50;

As a terminal, let's use `qterminal` instead of `st`, for the latter does not
support scrollback buffers:

    static const char *termcmd[] = {&quot;qterminal&quot;, NULL};

`qterminal` and `dmenu` need to be installed:

    # pkg install qterminal dmenu

## Status Line

`dwm` can display status information using the `xsetroot` command. The text to
be displayed is computed in a background task that can be defined in `.xinitrc`.
On laptops, I usually print the battery status. On desktops, the current date
and time suffices. Here's the `.xinitrc` that displays this information
(surrounded by spaces) in five second intervals:

    while true
    do
        xsetroot -name &quot; $(date +'%Y-%m-%d %H:%M') &quot;
    done &amp;
    setxkbmap ch
    exec dwm

The keymap is also set to the `ch` (i.e. Swiss German) variant just before
executing `dwm`. The `xsetroot` and `setxkbmap` utilities need to be installed
for this:

    # pkg install xsetroot setxkbmap

## Volume Control

In order to test audio, let's download the Free Software Song:

    $ curl https://www.gnu.org/music/free-software-song.ogg &gt; fss.ogg

I prefer `mplayer`, which needs to be installed:

    # pkg install mplayer

Make sure to include `/usr/local/bin` in your `$PATH` variable in order to run
`mplayer` without further path specification (`.cshrc`):

    export PATH=&quot;$PATH:/usr/local/bin&quot;

Playing the song as follows works if I plug in a headphone into one of the front
audio sockets:

    $ mplayer fss.ogg

The devices are listed in `/dev/sndstat` and switched by setting the respective
device number:

    # sysctl hw.snd.dfault_unit=1

The default volume is set to 85, which is quite loud for Richard Stallman's
singing voice. The volume can be changed relatively or absolutely using the
`mixer` command:

    $ mixer vol -10
    Setting the mixer from 85:85 to 75:75
    $ mixer vol 50
    Setting the mixer from 75:75 to 50:50

I don't always want to type that command, but rather use the volume keys on my
keyboard. So let's add a couple of commands to the `dwm` config (`config.h`,
just before the `keys[]` section):

    static const char *upvol[] = {&quot;mixer&quot;, &quot;vol&quot;, &quot;+5&quot;});
    static const char *downvol[] = {&quot;mixer&quot;, &quot;vol&quot;, &quot;-5&quot;});

For the key mapping, I first need to figure out the key codes for my volume
keys, which can be done using `xev`:

    # pkg install xev
    $ xev &gt; xev.out

Just press the volume up and volume up button in that order. Then close the
`xev` window and inspect `xev.out`.

**Unfortunately, the volume keys do not trigger an event.** There must be
something wrong with the keyboard configuration. So let's use Page Up and Page
Down to increase and decrese the volume (`keys[]` array in `config.h`):

    static Key keys[] = {
        // lines omitted
        { MODKEY, XK_Page_Up,   spawn, {.v = upvol}   },
        { MODKEY, XK_Page_Down, spawn, {.v = downvol} },
    };

Then simply re-compile, re-install, and re-start `dwm`:

    # make install
    $ x

Now Richard Stallman can be made to sing louder or quieter by pressing
Super+PgUp and Super+PgDown, respectively, _which is goood, hackers, which is,
goo-oo-ood!_

# Conclusion

Setting up the FreeBSD base system was rather easy. I made the mistake of using
`freebsd-boot` and not `efi` as the partition type for the boot partition, which
seems to be a mistake in the otherwise amazing book _Absolute FreeBSD_.

Installing the `x11/xorg-minimal` package instead of the full `xorg` package
caused some additional trouble, but helped me to better understand which
components are actually required to compile and run `dwm`. Instead of just
installing Xinerama, as I always did on Linux, the extra pain of libraries not
found made me investigate if I actually need that component. It turned out, I
don't.

I also needed to install the graphics driver and according kernel module
manually. Doing so, I realized that FreeBSD offers a nice graphical console,
which is a good fit for a `tmux` environment I use once in a while to work
absolutely focused.

Having audio running (almost) out of the box was a positive surprise. The
`mixer` interface is very simplistic. Switching audio devices, however, requires
an option to be changed using `sysctl`. This calls for some additional `dwm`
shortcuts!

My keyboard (a Cherry board with MX Brown switches) doesn't work properly out of
the box. I read about `uhidd`, which could be used to fix my issue with the
volume keys. But for the moment, I have a working setup.

I'll come back to the open issues in a later article. But first, I'd like to
work with my new FreeBSD desktop as much as possible to gain more experience.
</content>
    </entry>
    <entry>
        <title>«Four in a Row» in Haskell (Part II)</title>
        <link href="https://paedubucher.ch/articles/2020-08-05-four-in-a-row-in-haskell-part-ii.html"/>
        <author>
            <name>Patrick Bucher</name>
        </author>
        <id>https://paedubucher.ch/articles/2020-08-05-four-in-a-row-in-haskell-part-ii.html</id>
        <updated>2020-08-05T23:00:00Z</updated>
        <content type="text/markdown; charset=UTF-8">
In my [last
article](https://paedubucher.ch/articles/2020-08-03-four-in-a-row-in-haskell-part-i.html),
I outlined the purpose of a _stock program_: a non-trivial coding exercise to
be done in every new programming language somebody is learning. I also stated
that «Four in a Row» is becoming my personal stock program, and that I'd like to
implement it in Haskell.

The main challenge in Haskell is the functional programming paradigm.
Immutability is the main difference between an implementation of «Four in a
Row» in a functional programming language compared to rather structured
programming languages such as C or Python. The object-oriented aspect of an
implementation in Python makes hardly any difference, for OOP equally allows for
mutable an immutable programming beneath the surface. (In introductory courses
on OOP, hidden mutability is rather praised as a virtue than frowned upon; the
disadvantages of mutability are only taught in advanced courses by showing the
advantages of constructs like immutable classes. Learn and unlearn, but I'm
digressing…)

A later re-implementation of my stock program in Python might profit from the
experiences made in Haskell. Structured programming also allows for
immutability, and list comprehensions allow for compact code to produce new
state based on older state, without modifying existing state. (This
re-implementation could be subject matter for a fourth article, but let's not
get ahead of ourselves.)

In this article, I'm going to show how the board logic for the game «Four in a
Row» can be implemented in Haskell.

# Let There Be Code

As analyzed in my previous article, the board logic consists of five building blocks:

1. Create an empty grid with given dimensions.
2. Validate if a move (i.e. the choice of a column) is allowed for a given board.
3. Set a player's stone in the right place on the grid based on the choice of a column.
4. Detect if a player has won the game by checking if four of the player's
   stones lay in a horizontal, vertical, or diagonal line.
5. Format the grid as a string in order to display it on the command line.

The last building block, formatting, won't be covered in this article. I first
have to learn more about strings, formatting, and IO in Haskell, but I don't
like to wait to cover the other parts, which I'm already capable of
implementing with my current knowledge.

## Type Glossary

Before implementing the actual logic, let's define a couple of type aliases:

    type Grid = [Row]
    type Row = [Int]
    type Col = [Int]
    type Stone = Int

A grid (type `Grid`) is a list of rows. A row (type `Row`) itself is a list of
integers. As discussed in my previous article, 0 is going to be used for empty
fields. The fields occupied by player one and two shall be represented by the
numbers 1 and 2, respectively.

Just like a row, a column (type `Col`) is a list of integers. It is an
alternative way to express the relationships between individual fields. The
`Grid`, however, uses the `Row` type as its building blocks.

A `Stone` is an integer, too. It represents a player's number for fields
occupied by his or her stones.

Those types won't add powerful abstractions to the program, but make the
signature of certain functions a bit clearer. (It's also possible to limit the
scope of the types declared to certain values, but let's focus on the program
logic instead.)

## Creating a Grid

The function `new_grid` accepts two integer parameters (number of rows and
columns), and produces a grid of those dimensions:

    new_grid :: Int -&gt; Int -&gt; Grid
    new_grid r c = [new_row c | _ &lt;- [1..r]]

A list comprehension is used to build up the grid as a list of `r` rows. The
row itself is created by a function `new_row`:

    new_row :: Int -&gt; Row
    new_row c = [0 | _ &lt;- [1..c]]

Again, a list comprehension is used to build a single row consisting of `c`
elements: one per column.

The `new_grid` function can be used as follows (`&gt;` indicates the REPL, the
output has been wrapped for better readability):

    &gt; new_grid 6 7
    [[0,0,0,0,0,0,0],
     [0,0,0,0,0,0,0],
     [0,0,0,0,0,0,0],
     [0,0,0,0,0,0,0],
     [0,0,0,0,0,0,0],
     [0,0,0,0,0,0,0]]

## Validating a Move

A move solely consists of a column index. Let's assume a 6x7 grid (6 rows, 7
columns) if nothing else is stated. A valid move must be in the range of 0
(leftmost column) to 6 (inclusive, rightmost column).

For a move to be valid, the column must have an empty field, i.e. it must
contain the value 0. Since the columns are filled up from the bottom, a column
is not full if its the top-most field is equal to 0. So this validation seems
trivial.

However, in order to deal with _columns_ rather than _rows_ (remember, the grid
is defined in terms of rows, not the other way around), we first need a way to
gather the fields of a column. The function `get_column` expects a grid and a
column index and returns the fields belonging to that particular column:

    get_column :: Grid -&gt; Int -&gt; Col
    get_column g c = [row !! c | row &lt;- g]

A list comprehension is used to select the element at index `c` in every grid
row using the index operator (`!!`).

The function `is_valid_move` simply extracts the column chosen by the player
and checks its topmost field to be empty (equals 0, that is):

    is_valid_move :: Grid -&gt; Int -&gt; Bool
    is_valid_move g c = (get_column g c) !! 0 == 0

(Notice that no boundary checks are implemented throughout the program, unless
absolutely necessary for getting the logic right.)

This function can be used as follows:

    &gt; g = new_grid 6 7
    &gt; is_valid_move g 0
    True

## Setting a Stone

The first two building blocks were easy to write without modifying state.
Performing a move on the grid by setting a stone into a certain column,
however, is a step that requires a modification of some sort. The solution is
to not mutate the given grid, but to produce a new grid based on the given grid
by accounting for a player's move.

The function `apply_move` expects a grid, a column (chosen by the player and
validated using `is_valid_move`), and the player's number (to set the right
value in the new grid):

    apply_move :: Grid -&gt; Int -&gt; Int -&gt; Grid

Because only a column is given, the row coordinate has to be figured out. Since
stones played are falling down the grid in the physical version of the game,
the bottom-most free field of a column has to be found:

    bottom_most :: Grid -&gt; Int -&gt; Int -&gt; Int
    bottom_most g v c = length (takeWhile (\x -&gt; x == v) col) - 1
                        where col = get_column g c

The lowest free position is found by extracting a subsequent list of a given
value `v`, which can be handed in as an argument. (The value 0 has to be used
for this particular use case by the caller.) The built-in function `takeWhile`
is used to extract a list based on a lambda expression: Elements are taken from
the column as long as the lambda expression holds true. The bottom-most position
of a column with the given value `v` is simply the length of the extracted sub
list minus one (indexes are zero-based). Again, the `get_column` function is
used to get access to the fields of a particular column. 

Now `apply_move` can be implemented as follows:

    apply_move g c p = replace_value g r c p
                       where r = bottom_most g 0 c

Another function is needed: `replace_value`, which creates a new grid based on
the existing grid `g`, by setting the player's stone value `p` to the coordinate
`(r,c)`. (The row coordinate is figured out using `bottom_most`, as shown
above.)

The function `replace_move` is implemented as follows:

    replace_value :: Grid -&gt; Int -&gt; Int -&gt; Stone -&gt; Grid
    replace_value g r c p = take r g ++ [new_row] ++ drop (r + 1) g
                            where new_row = replace_row_value (g !! r) c p

Given the row index `r`, the first `r` rows are taken. (This excludes the row
to be transformed, because the index is zero-based.) The row at index `r` is
computed as `new_row` in a further step. The remaining rows are extracted from
the existing grid by dropping the first `r + 1` rows from it. Those three
components are concatenated to a new grid using the `++` operator.

The `new_row` looks like the old row at index `r`, expect that a single value
at index `c` (the column) has to be replaced with the player's value `v`. The
function `replace_row_value` performs this transformation:

    replace_row_value :: Row -&gt; Int -&gt; Stone -&gt; Row
    replace_row_value r c p = take c r ++ [p] ++ drop (c+1) r

The same logic using `take` and `drop` can be implemented for the column's
fields like for the grid's rows before. The empty field at column index `c` can
simply be replaced by a list solely consisting of the player's stone value `v`.
List concatenation is used again to produce the tranformed column.

A move can be applied as follows:

    &gt; g = new_grid 6 7
    &gt; g1 = apply_move g 3 1
    &gt; g1
    [[0,0,0,0,0,0,0],
     [0,0,0,0,0,0,0],
     [0,0,0,0,0,0,0],
     [0,0,0,0,0,0,0],
     [0,0,0,0,0,0,0],
     [0,0,0,1,0,0,0]]

    &gt; g2 = apply_move g1 4 2
    &gt; g2
    [[0,0,0,0,0,0,0],
     [0,0,0,0,0,0,0],
     [0,0,0,0,0,0,0],
     [0,0,0,0,0,0,0],
     [0,0,0,0,0,0,0],
     [0,0,0,1,2,0,0]]

`apply_move` could also be invoking `is_valid_move` for validation. But this
task should be left for the client to be implemented later on.

## Detecting a Win

Figuring out whether or not a player's most recent move leads to a win is the
hardest part of this program, no matter what implementation language is used.
(However, I didn't try Prolog _yet_ for this.) Let's analyze the problem.

First, what do we know? The player with a number (1 or 2) just picked a column
(between 0 and 5 in our 6x7 grid). A stone was set in the bottom-most empty
field of that column. The actual row where the stone landed in is unknown.
However, this information can be found out: it is the top-most row of the chosen
column holding the player's stone value. All the fields above must be empty.

Second, what do we need to find out? Starting from the coordinates (given
column, row figured out as described above), there are three possibilities to
build a row of four values: horizontal, vertical, and diagonal lines. A
horizontal line is a row, and a vertical row is a column. Diagonal lines can
occur in two directions: ascending or descending. So we actually need to account
for four kinds of rows, which need to be extracted from the row/column
coordinates.

Third, once the horizontal, the vertical, and the two diagonal lines going
through the player's stone most recently set are established, a simple check can
be done: Does the line, which can be represented as a list, contain a list of
four of the player's stones? If that's the case, the player just won the game.

Let's implement that algorithm in a top-down manner!

The function `is_win` expects a grid, a column, and a player's stone value, and
returns a boolean value indicating if the player just won the game:

    is_win :: Grid -&gt; Int -&gt; Stone -&gt; Bool
    is_win g c p = horizontal_win g row p ||
                   vertical_win g c p ||
                   diagonal_win g row c p
                   where row = top_most g p c

Three predicate functions `horizontal_win`, `vertical_win`, and `diagonal_win`
handle the three different shapes of winning rows. To check for a vertical win,
the row is irrelevant. For the other wins, the row where the player's stone just
landed in is figured out using the `top_most` function:

    top_most :: Grid -&gt; Stone -&gt; Int -&gt; Int
    top_most g v c = length (takeWhile (\x -&gt; x /= v) col)
                     where col = get_column g c

This function expects a grid, a player's stone value, a column, and returns the
top-most row containing the player's stone. Going through the column from top to
bottom, values are read into a list as long as they are not equal to the
player's stone value. The length of that list is the row coordinate of the
player's top-most stone in that column. Again, the column is extracted using the
`get_column` function explained further above.

### Vertical and Horizontal Win

A vertical and horizontal winning row can be detected in the same manner. The
only difference is that the former works on columns, and the latter on rows:

    horizontal_win :: Grid -&gt; Int -&gt; Stone -&gt; Bool
    horizontal_win g r p = contained fiar (g !! r)
                           where fiar = [p | _ &lt;- [1..4]]

    vertical_win :: Grid -&gt; Int -&gt; Stone -&gt; Bool
    vertical_win g c p = contained fiar (get_column g c)
                         where fiar = [p | _ &lt;- [1..4]]

In both cases, a grid, an index (row or column, respectively), and a player's
stone value is expected. The boolean return value indicates whether or not the
row or column contains a sub-list consisting of four of the player's stone
values: `fiar`, which is built using a list comprehension.

For the horizontal win, the row can be directly accessed from the grid using the
row index (`g !! r`). For the vertical win, the `get_column` function is used
once again.

The function `contained` is the tricky part. This function checks whether or not
a smaller list (first argument) is part of a larger list (second argument). A
possible implementation looks as follows:

    contained :: Eq a =&gt; [a] -&gt; [a] -&gt; Bool
    contained [] []                     = True
    contained [] ys                     = True
    contained xs []                     = False
    contained (x:xs) (y:ys) | x == y    = and [x == y | (x,y) &lt;- zip xs ys]
                                          &amp;&amp; length xs &lt;= length ys
                                          || contained (x:xs) ys
                            | otherwise = contained (x:xs) ys

The lists processed can be of any type that supports the comparison operator
(`Eq a`). A boolean value is returned indicating whether or not the first list
is contained in the second list. The function is implemented using pattern
matching, which covers the following cases:

1. An empty list is contained in another empty list (first base case).
2. An empty list is contained in any non-empty list (second base case).
3. A non-empty list is not contained in an empty list (negative base case).
4. A non-emtpy list is _possibly_ contained in another non-empty list (complex
   case).

The «possibly» in the fourth case can be resolved as follows: If the first
elements of the two lists do match, the remainders of the two lists need to be
checked for a match. A list comprehension zipping those tails together and
comparing the corresponding elements creates a list of booleans indicating
matches. If all those booleans are `True`, the first list must be contained in
the second list, _if the second list is at least as long as the first list_.
(Notice that the `zip` function only picks values until the shorter of the two
zipped lists is exhausted. The length check ensures that the comparison of the
lists does not end prematurely.)

The `otherwise` case is processed when the two list's heads do not match. In
this case, the `contained` function is invoked again with the full first list
and the second's list tail: It shall be checked whether or not the first list is
contained in the second's list tail.

### Diagonal Win

Detecting a diagonal win works in the same manner as detecting a horizontal or
vertical win. However, there are two subtle details that make the implementation
more complicated:

First, there are _two_ kinds of diagonal lines: ascending and descending. This
can be handled by implementing two different functions.

Second, extracting a diagonal line as a list from the two-dimensional grid is
much more complicated than extracting a horizontal line (row) or a vertical line
(column).

Let's start with the `diagonal_win` function, which accounts for both winning
rows in ascending or descending order:

    diagonal_win :: Grid -&gt; Int -&gt; Int -&gt; Stone -&gt; Bool
    diagonal_win g r c p = contained fiar (diag_asc g r c) ||
                           contained fiar (diag_desc g r c)
                           where fiar = [p | _ &lt;- [1..4]]

The function expects a grid, both row and column indication, and the player's
stone value. As always, a boolean value indicating a win is returned. A win is
detected, if the list of four player's stone values is contained in either the
ascending or the descending diagonal line.

Those lines are extracted from the grid using the `diag_asc` and `diag_desc`
functions, respectively. The two functions look quite similar, but have subtle
differences in the way they process the grid:

- An _ascending_ row starts at the bottom of the grid, i.e. with the highest row
  index. It starts at the left, i.e. with the lowest column index.
- A _descending_ row starts at the top of the grid, i.e. with the lowest row
  index. It also starts at the left, and, thus, with the lowest column index.

The function `diag_asc` expects a grid and both row and column indices. It
returns the ascending diagonal row containing that coordinate:

    diag_asc :: Grid -&gt; Int -&gt; Int -&gt; [Int]
    diag_asc g r c = [g !! i !! j | (i,j) &lt;- zip rows cols]
                     where
                       nrows   = length g
                       ncols   = length (g !! 0)
                       offset  = max (min (nrows - r - 1) (ncols - c - 1)) 0
                       max_row = r + offset
                       min_col = c - offset
                       rows    = reverse [0..max_row]
                       cols    = [min_col..ncols-1]

The function is implemented using a list comprehension. The variable `i` is the
row index, `j` the column index. Those indices are obtained by zipping a list of
row indices (`rows`) with a list of column indices (`cols`). The starting and
end point of those lists are the tricky part.

Consider this grid, in which `-` stands for an empty field, and the upper-case
`F` for the field played most recently (with the `r` and `c` arguments as
indices). All the fields indicated with a lower-case `f` are to be extracted for
the ascending diagonal holding the upper-case `F`:

        !
    0 1 2 3 4 5 6
    - - - - - - f 0
    - - - - - f - 1
    - - - - f - - 2
    - - - f - - - 3
    - - F - - - - 4 !
    - f - - - - - 5

The row and column indices of `F` are given as 4 and 2. The starting point at
the bottom-left can be figured out by shifting the coordinates by an _offset_.
This offset is the smaller value of the following two differences:

- `rows - r - 1`: the number of rows minus the row index (minus one to account
  for the zero-based row index)
- `cols - c - 1`: the number of columns minus the column index (minus one;
  zero-based index again)

The offset is set to 0, if either difference becomes negative (boarder
clipping). The offset is calculated as follows:

    offset = max (min (nrows - r - 1) (ncols - c - 1)) 0
    offset = max (min (6 - 4 - 1) (7 - 2 - 1)) 0
    offset = max (min 1 4) 0
    offset = max 1 0
    offset = 1

And the starting points `max_row`/`min_col` (bottom left) are calculated based
on the given indices of `F` as follows:

    max_row = r + offset
    max_row = 4 + 1
    max_row = 5

    min_col = c - offset
    min_col = 2 - 1
    min_col = 1

The diagonal line can be drawn up to the row index 0 and the column index 6.
Here, it is possible to always use the maximum value, because the `zip` function
will stop picking values once the shorter list is exhausted.

The number of rows and columns can simply be figured out using the `length`
function applied on the grid as a whole and on a single row thereof:

    nrows = length g
    ncols = length (g !! 0)

Notice that in order to create a list containing the _falling_ values from
`max_row` down to 0 (`rows`), a rising list from 0 to `max_row` has to be
created and reversed:

    &gt; reverse [0..max_row]
    [0,1,2,3,4,5]

The other way around, an empty list would be created:

    &gt; [max_row..0]
    []

The somewhat easier to understand function `diag_desc` is simply pasted here
without any further comments.  Figuring out how it works is left to the reader.
The extensive comments above on `diag_asc` certainly help for this purpose:

    diag_desc :: Grid -&gt; Int -&gt; Int -&gt; [Int]
    diag_desc g r c = [g !! i !! j | (i,j) &lt;- zip rows cols]
                      where
                        offset  = min r c
                        min_row = r - offset
                        min_col = c - offset
                        nrows   = length g
                        ncols   = length (g !! 0)
                        rows    = [min_row..nrows-1]
                        cols    = [min_col..ncols-1]

# Conclusion

The complete board logic required to implement a basic «Four in a Row» game has
been implemented in Haskell. The whole code described, plus some additional
attempts to format the grid as a string, can be found on
[GitHub](https://github.com/patrickbucher/programming-in-haskell/blob/master/four-in-a-row/Board.hs).

The linked code also defines a module `Board` which exports the public interface
of the board consisting of the four building blocks discussed in this article
and its predecessor. The file
[BoardTest.hs](https://github.com/patrickbucher/programming-in-haskell/blob/master/four-in-a-row/BoardTest.hs)
defines a couple of unit tests written in HUnit for basic verification of the
logic.

The actual board logic requires a little less than 100 SLOC. Comparable
implementations I've written in Python and C only take up slightly more lines. I
could have made some functions _shorter_, but probably not _clearer_ with my
limited knowledge of Haskell.

The `contained` function, for example, looks a bit bulky, but actually contains
very little logic. It is possible that the negative base case could be
eradicated, because a length check is already performed in the complex case.
However, I rather have a clear statement of the base cases than saving an easy
to understand line of code.

I might revisit this code and improve it as my knowledge of Haskell improves.
But the next step in my journey is to implement an interactive game based on
this board, which will be the subject of an article to be published in weeks or
maybe months.
</content>
    </entry>
    <entry>
        <title>«Four in a Row» in Haskell (Part I)</title>
        <link href="https://paedubucher.ch/articles/2020-08-03-four-in-a-row-in-haskell-part-i.html"/>
        <author>
            <name>Patrick Bucher</name>
        </author>
        <id>https://paedubucher.ch/articles/2020-08-03-four-in-a-row-in-haskell-part-i.html</id>
        <updated>2020-08-03T12:00:00Z</updated>
        <content type="text/markdown; charset=UTF-8">
In a [recent interview](https://youtu.be/O9upVbGSBFo?t=3741), Brian W. Kernighan
said that he always re-implements the same program when he's learning a new
programming language. In his case, it's a programm to process a text file
containing a variable number of lines. In this task, his programming language
AWK (Kernighan is the «K» in «AWK») shines, for it was designed for that kind of
a task.

Such a _stock program_ allows to evaluate a programming language from a certain
perspective. Different programs offer different perspectives. I personally
didn't have such a stock program yet, but there is at least one program I have
already implemented in multiple programming languages: the board game _Four in a
Row_.

# Four in a Row: My Stock Program

This game is played by two players, usually on a 7x6 grid (seven columns, six
rows). The grid is setup to be perpendicular to the table, so that the stones
fall to the lowest free field of the chosen column. The players take turns
setting their stones (red for one player, yellow for the other one). The player
that first can set four of stones into a horizontal, vertical, or diagonal row
wins the game.

I first implemented this game as a program towards the end of my first year as
an apprentice. The task was an optional assignment in an introductory
programming class. C was used as the implementation language. A more recent
re-implementation of that program is available on
[GitHub](https://github.com/patrickbucher/prog/blob/master/vier_gewinnt/vier_gewinnt.c).
The hardest part was to get the winning detection right, especially for the
diagonal rows. Since the grid was implemented as a two-dimensional array,
diagonals clipping the edge would erroneously also be detected as a winning row.
Some additional checks for index boundaries fixed the issue.

16 years later, my apprenticeship already was far in the past. I was studying
computer science in my eight and last term. For a _Game Design_ class, I had to
write a case study on improving an existing game. I picked _Four in a Row_ and
extended it with a couple of new game mechanics. The case study, written in
German, and the source code, can be found on
[GitHub](https://github.com/patrickbucher/v13r93w1nn7), too. This time, I used
Python as the implementation language. The [NumPy](https://numpy.org/) library
made this task very comfortable, and I was able to implement the board logic
with rather few lines of Python code. The unit tests, implemented using
[PyTest](https://docs.pytest.org/en/stable/), took up far more lines than the
actual code.

Both versions were implemented for the command line. However, the latter
version was implemented in a way that would also support graphical frontends.

## Building Blocks

Having implemented the same program with much more programming experience and
using a different programming language, the resulting code looked quite
different. However, I was able to detect some common patterns.

On a very high level, there are two parts for such a program: First, the _board
logic_ that deals with the grid, its manipulations and validations (Is a row not
full yet?  What is the bottom-most empty row in a given column? Are four stones
of the same color in a row?). Second, the _game logic_, which consists of a big
loop that lets the players take turns setting their stones, prints the grid, and
ends the game upon a win or draw.

The board logic can be taken further apart into the following components:

1. **Creating an Empty Grid**: At the beginning of a game, an empty grid with
   given dimensions has to be created. (The physical game is played on a 7x6
   grid, but a computer game can offer additional flexibility with the number of
   rows and columns given as arguments.)
2. **Validating a Move**: As soon as all fields of a column are filled, the
   column must no longer be chosen by players. A function is needed that checks
   which columns still have at least one empty field.
3. **Setting a Stone**: If a stone is to be set into a non-full column, the
   bottom-most empty row of that column has to be figured out. Then, the field
   is modified by setting the player's stone into that position.
4. **Detecting a Win**: After every move, it has to be checked whether or not
   the grid contains four stones of the same color laying in the same
   horizontal, vertical, or diagional row, without any gaps in between. If the
   detection gets to know which player did the last move, and into what
   coordinates that stone was put, the algorithm has to do less work, as opposed
   to an approach where the whole grid is evaluated for both players. (For the
   case-study, I had to use the latter approach, for one of the additional game
   mechanics allowed to flip the grid, which required a full evaluation of the
   whole grid afterwards.)
5. **Formatting the Grid**: This part could also be implemented in the game
   logic.  However, offering the capability to print the current grid from the
   board component (be it a module or a class) in a nicely formatted way is a
   good design decision in terms of cohesion. This function can be made very
   flexible by accepting formatting parameters, such as the characters to be
   used to display fields that are empty, or contain a stone of either player.

A function to format the current grid makes an important separation between the
inner state of the grid and its textual representation on the command line. It
is a good idea to represent the state of the fields as _numbers_ internally,
but to use _characters_ in order to display them nicely on the command line.
Internally, `0` can used for empty fields. For fields holding a stone of player
one or two, the values `1` and `2`, respectively, can be used. The empty field
can be displayed using a whitespace character, an underscore, or a dash. The
stones of the players can be easily distinguished when using `x` and `o` for
their output.

# Towards Haskell

The programming language _Haskell_, which has been mentioned in this article's
title, but not in the text ever since, shall be used to create an additional
implementation of the _Four in a Row_ game. But why Haskell?

First, I'm currently learning Haskell. It turns out that writing useful programs
in Haskell is not that easy, because advanced concepts like Monads have to be
understood in order to perform input/output operations. I'm working through the
rather dense book [Programming in Haskell (Second
Edition)](https://www.cs.nott.ac.uk/~pszgmh/pih.html) (by Graham Hutton) at the
moment, and I've almost finished the first part. The knowledge acquired from
those first nine chapters allows me to implement the board logic. The
interactive part then has to wait until I (nearly) finished the book.

Second, I'm interested in functional programming. I consider Haskell as a
stepping stone into that programming paradigm. I have some minor experience in
Prolog, and I'd like to learn Erlang later on. Knowledge about functional
programming also helps when programming in Python and JavaScript, which also
support features like lambda expressions, higher-order functions, and, in case
of Python, list comprehensions.

Implementing _Four in a Row_ in Haskell gives me a couple of challenges.
Unlike an implementation in C or Python, the grid must not be modified during
gameplay. A new grid, representing the fresh state, has to be build up based on
the previous state and the player's action, instead. I also need to figure out
how to detect a winning row in a declarative way, i.e. without loops and
counter variables. The input/output of the actual game logic will probably be
the biggest challenge later on. The game logic, implemented as a loop in both C
and Python, needs to be implemented using a different mechanism.

My plan is to implement the board logic, consisting of the five components
stated above, in the next couple of days in Haskell. I'll write an article
describing my approach and containing the code for the board logic as soon as I
have a decent solution for the problems stated. The game logic has to wait for a
couple of weeks of even months, depending on my progress with _Programming in
Haskell_.

Stay tuned, and feel free to put (maybe needed?) pressure on me, when those
articles do not appear any time soon…
</content>
    </entry>
    <entry>
        <title>Virtual Machines with libvirt and Networking</title>
        <link href="https://paedubucher.ch/articles/2020-08-01-virtual-machines-with-libvirt-and-networking.html"/>
        <author>
            <name>Patrick Bucher</name>
        </author>
        <id>https://paedubucher.ch/articles/2020-08-01-virtual-machines-with-libvirt-and-networking.html</id>
        <updated>2020-08-01T22:30:00Z</updated>
        <content type="text/markdown; charset=UTF-8">
I'd like to dig deeper into system administration tasks. At work, I have to
manage a fleet of Linux servers with Puppet. And in my spare time, I'd like to
manage the servers I run with Ansible or Puppet in the future.

Virtual Machines are easily obtained nowadays. Cloud providers such as Digital
Ocean or Exoscale offer virtual machines with various operating systems at
rather moderate prices. You only have to pay for the time the virtual machines
are actually running, so you can save money by shutting those hosts down when
not needed.

However, running those virtual machines locally costs even less. No additional
public IPv4 addresses are wasted, and, most importantly, a local setup allows
you to test changes to be applied to your productive environment locally
beforehand.

This article shows how to set up three virtual machines ‒ `master`, `node1`, and
`node2`, which later could be used for a Puppet setup with a Puppetmaster ‒
using [libvirt](https://libvirt.org/) on top of
[KVM](https://www.linux-kvm.org/page/Main_Page). [Debian 10
(«Buster»)](https://www.debian.org/releases/buster/) is going to be used both as
the host and guest operating system. The host operating system is installed on a
Dell Latitude E6430 from 2013 with 8 GB or RAM, which is just laying around
here. (This also proofs that you don't need a whole lot of hardware resources
for such a setup.)

# Setting up the Virtualization

Given a fresh Debian setup with the lightweight LXQt desktop, a couple of
packages need to be installed in order to get virtualization to work:

    # apt-get install \
        qemu-kvm \
        libvirt-clients \
        libvirt-daemon-system \
        virtinst \
        bridge-utils

Make sure to activate virtualization in the BIOS. Check if the `kvm` kernel
module is activated:

    $ lsmod | grep ^kvm
    kvm                 835584  1 kvm_intel

If there is a number not equal to 0 in the third column, `kvm` is up and
running.

# Setting up the Virtual Network

Usually a `default` network is pre-defined, which can be checked as follows:

    # virsh net-list --all
     Name      State      Autostart   Persistent
    ----------------------------------------------
     default   inactive   no          yes

The `default` network can be configured to be started up automatically:

    # virsh net-autostart default
    Network default marked as autostarted

Until the next system restart, it is started up manually:

    # virsh net-start default
    Network default started

A bridge interface `virbr0` should have been created:

    # brctl show
    bridge name     bridge id               STP enabled     interfaces
    virbr0          8000.5254005f4e6b       yes             virbr0-nic

Make sure that NAT is activated:

    # sudo sysctl -a | grep 'net.ipv4.ip_forward ='
    net.ipv4.ip_forward = 1

The value of the above property must be `1`.

## Possible Issues

If `iptables` is in use, make sure to forward the traffic from the guests over
the bridge `virbr0`, so that the guests also have internet access:

    # iptables -I FORWARD -i virbr0 -o virbr0 -j ACCEPT

# Setting up the Virtual Machines

Since networking over the bridge interface requires `root` privileges, all
virtual machine files are put into the `/opt/vms` directory, which first needs
to be created:

    # mkdir /opt/vms
    # cd /opt/vms

The network installer for Debian Buster can be downloaded from the official
website:

    # wget https://cdimage.debian.org/debian-cd/current/amd64/iso-cd/\
    debian-10.4.0-amd64-netinst.iso

The `master` virtual machine is now setup using `virt-install`:

    # virt-install \
        --name master \
        --memory 1024 \
        --vcpus=1,maxvcpus=2 \
        --cpu host \
        --cdrom debian-10.4.0-amd64-netinst.iso \
        --disk /opt/vms/master.qcow2,size=8,format=qcow2 \
        --network network=default \
        --virt-type kvm

The machine gets 1 GB of memory and a 8 GB disk. Most importantly, the network
is set to the `default` network.

A window showing the Debian installer appears. Just install the standard system
utilities and the SSH server. The following users and passwords shall be
configured:

- `root`: `topsecret`
- `user`: `secret`

After the setup is finished, just let the system boot, and login as `root`. Then
shut the virtual machine down:

    # shutdown -h now

The two additional guest nodes can be created by cloning the `master` virtual
machine just set up:

    # virt-clone --original master --name node1 --file node1.qcow2
    # virt-clone --original master --name node2 --file node2.qcow2

Now start up all the nodes:

    # virsh --connect qemu:///session start master
    # virsh --connect qemu:///session start node1
    # virsh --connect qemu:///session start node2

# Configuring the Virtual Network

In order to conveniently access the guests, static IPs should be assigned to
them. The network configuration can be edited as follows:

    # virsh net-edit default

An editor showing an XML configuration appears:

    &lt;network&gt;
      &lt;name&gt;default&lt;/name&gt;
      &lt;uuid&gt;fecb90d5-9b46-48f6-8b93-e57032f8ba6a&lt;/uuid&gt;
      &lt;forward mode='nat'/&gt;
      &lt;bridge name='virbr0' stp='on' delay='0'/&gt;
      &lt;mac address='52:54:00:63:d3:70'/&gt;
      &lt;ip address='192.168.122.1' netmask='255.255.255.0'&gt;
        &lt;dhcp&gt;
          &lt;range start='192.168.122.2' end='192.168.122.254'/&gt;
        &lt;/dhcp&gt;
      &lt;/ip&gt;
    &lt;/network&gt;

The `dhcp` section needs to be extended with static IP definitions, which map
the MAC addresses of the guest's virtual network interfaces to the static IP
addresses to be used.

The MAC addresses of the virtual machines can be extracted from their
configuration as follows:

    # virsh dumpxml master | grep -i '&lt;mac'
        &lt;mac address='52:54:00:db:07:7c'/&gt;
    # virsh dumpxml node1 | grep -i '&lt;mac'
        &lt;mac address='52:54:00:a4:77:a9'/&gt;
    # virsh dumpxml node2 | grep -i '&lt;mac'
        &lt;mac address='52:54:00:51:e8:ef'/&gt;

Using those MAC addresses, new static host definitions can be created as
follows:

    &lt;host mac='52:54:00:db:07:7c' name='master' ip='192.168.122.2'/&gt;
    &lt;host mac='52:54:00:a4:77:a9' name='node1' ip='192.168.122.3'/&gt;
    &lt;host mac='52:54:00:51:e8:ef' name='node2' ip='192.168.122.4'/&gt;

The XML network definition should now look as follows (the `uuid` and `mac
address` of the host will vary):

    &lt;network&gt;
      &lt;name&gt;default&lt;/name&gt;
      &lt;uuid&gt;fecb90d5-9b46-48f6-8b93-e57032f8ba6a&lt;/uuid&gt;
      &lt;forward mode='nat'/&gt;
      &lt;bridge name='virbr0' stp='on' delay='0'/&gt;
      &lt;mac address='52:54:00:63:d3:70'/&gt;
      &lt;ip address='192.168.122.1' netmask='255.255.255.0'&gt;
        &lt;dhcp&gt;
          &lt;range start='192.168.122.2' end='192.168.122.254'/&gt;
          &lt;host mac='52:54:00:db:07:7c' name='master' ip='192.168.122.2'/&gt;
          &lt;host mac='52:54:00:a4:77:a9' name='node1' ip='192.168.122.3'/&gt;
          &lt;host mac='52:54:00:51:e8:ef' name='node2' ip='192.168.122.4'/&gt;
        &lt;/dhcp&gt;
      &lt;/ip&gt;
    &lt;/network&gt;

After saving the configuration, the network `default` needs to be restarted:

    # virsh net-destroy default
    # virsh net-start default

The guest virtual machines must also be restarted so that they will get the new
IP addresses assigned:

    # virsh shutdown master
    # virsh shutdown node1
    # virsh shutdown node2

    # virsh --connect qemu:///session start master
    # virsh --connect qemu:///session start node1
    # virsh --connect qemu:///session start node2

The virtual machines should now be accessible through SSH:

    $ ssh user@192.168.122.2
    $ ssh user@192.168.122.3
    $ ssh user@192.168.122.4

Make sure that the network communication is working between the guests:

    [user@master]$ ping node1
    [user@master]$ ping node2

Also make sure to define the proper hostname in `/etc/hostname`, for it is still
`master` for the two guests that have been cloned from the initial image:

    [root@node1]# echo 'node1' &gt; /etc/hostname
    [root@node2]# echo 'node2' &gt; /etc/hostname

## Adding Some Comfort

Consider adding the following definitions to `/etc/hosts`:

    192.168.122.2   master
    192.168.122.3   node1
    192.168.122.4   node2

So that you can access your virtual machines by their host names:

    $ ssh user@master
    $ ssh user@node1
    $ ssh user@node2

In order to login to the guests without typing a password, create an SSH key
locally without any passphrase:

    $ ssh-keygen -t rsa -b 4096 -f ~/.ssh/id_vms_rsa

Make sure that your `~/.ssh` folder has the access mode `700`, and the contained
files all have the access mode `600` (thanks to [meillo](http://marmaro.de/) for
pointing that out):

    $ chmod 700 ~/.ssh
    $ chmod 600 ~/.ssh/*

Copy the public key to the hosts using `ssh-copy-id` (thanks to meillo again for
hinting that utility to me):

    $ ssh-copy-id -i ~/.ssh/id_vms_rsa user@master
    $ ssh-copy-id -i ~/.ssh/id_vms_rsa user@node1
    $ ssh-copy-id -i ~/.ssh/id_vms_rsa user@node2


Check that the SSH connection now works without any password:

    $ ssh -i ~/.ssh/id_vms_rsa user@master
    $ ssh -i ~/.ssh/id_vms_rsa user@node1
    $ ssh -i ~/.ssh/id_vms_rsa user@node2


# Conclusion

Three virtual machines running Debian GNU/Linux have been installed on a
rather old laptop running Debian GNU/Linux itself. Those virtual machines can be
comfortably accessed without any passwords through SSH, and are able to
communicate with one another over a virtual network.

It took me almost a day ‒ and gave me some additional grey hair ‒ to get all
this information together from various sources. After I figured out how to
create the setup described above, it only took me about two hours to reproduce 
everything on another laptop (including the setup of the laptop itself) and to
write this article.

Since I did the try-and-error part on Arch Linux, this article can also be used
on that distribution, and probably many others as well. Only the packages to be
installed will probably vary on other distributions.

I plan to describe the setup of a local Puppet environment based on the setup
described above in a forthcoming article.
</content>
    </entry>
    <entry>
        <title>Table-Driven Test Design</title>
        <link href="https://paedubucher.ch/articles/2020-07-22-table-driven-test-design.html"/>
        <author>
            <name>Patrick Bucher</name>
        </author>
        <id>https://paedubucher.ch/articles/2020-07-22-table-driven-test-design.html</id>
        <updated>2020-07-22T22:00:00Z</updated>
        <content type="text/markdown; charset=UTF-8">
Many universities teach programming in Java. Writing unit tests is one of the
subjects being taught. Many professional Java programmers, but also university
professors, suggest to build those test cases according to a pattern. _Given,
When, Then_ is a common pattern, and so is _Arrange, Act, Assert_. Both patterns
prescribe the following structure for a test case:

1. _Given_/_Arrange_: An environment (in the broadest sense) is built up.
2. _When_/_Act_: The function or method to be tested is invoked.
3. _Then_/_Assert_: The result of the function or method is checked against some
   expectation.

Such a test case might look as follows (Java):

    public void testAddition() {
        // Given/Arrange
        Calculator calc = new Calculator();
        int a = 3;
        int b = 5;

        // When/Act
        sum = calc.add(a, b);

        // Then/Assert
        assertEqual(8, sum);
    }

A rule often taught is the so-called _single assert rule_ from Robert C. Martin,
[whom I refuse to call «Uncle
Bob»](http://marmaro.de/apov/txt/2016-04-27_schaedlicher-kult.txt). It states
that there should be only one assertion per test case. One can argue whether or
not this rule is useful.

# Unclean Code

However, in my experience this rule leads to a consequence I do not like ‒ and
which also doesn't fit into the _Clean Code_ philosophy (or _cult_, I daresay):
The programming language being used to write test code is a small subset of the
implementation language, often degenerating into a sheer sequence of statements
(imperative programming).

Even though using a subset of a language is often a sensible approach (just
think about C++, or `with` and `eval` in JavaScript, or `unsafe` in Go, etc.),
using a subset of a language that doesn't even contain core features from
structured programming (decisions, loops, data structures) does not sound
sensible to me, except when programming in a purely functional style.

How should an additional test case to cover, say, negative numbers, be added to
the one above? The _single assert rule_ wants us to write an additional test
case:

    public void testAdditionWithNegativeNumbers() {
        // Given/Arrange
        Calculator calc = new Calculator();
        int a = -1;
        int b = 3;

        // When/Act
        sum = calc.add(a, b);

        // Then/Assert
        assertEqual(2, sum);
    }

Who would _type_ in that code, which is almost identical to the one above? Such
code is rather _copied_ than written again. (Why don't I hear somebody shouting
_«Clean Code!!!!11»_ now?)

# Structured Programming to the Rescue

Let's violate the _single assert rule_ for a minute and bring back structured
programming. Let's write a unit test in C!

    typedef struct {
        int a;
        int b;
        int expected;
    } addition_test_case;

    void test_addition()
    {
        addition_test_case tests[] = {
            {3, 5, 8},
            {-1, 3, 2},
        };
        int n = sizeof(tests) / sizeof(tests[0]);
        for (int i = 0; i &lt; n; i++) {
            addition_test_case test = tests[i];
            int actual = add(test.a, test.b);
            if (actual != test.expected) {
                printf(&quot;add(%d, %d): expected %d, got %d\n&quot;,
                        test.a, test.b, test.expected, actual);
                exit(1);
            }
        }
        printf(&quot;test_addition: %d tests passed\n&quot;, n);
    }

This test case, which does not make use of any unit testing framework, was
designed in a _table-driven_ manner. I first got to know the concept of
_table-driven test design_ when learning Go by reading [The Go Programming
Language](http://www.gopl.io/) (p. 306) by Alan A. A. Donovan and the great
Brian W. Kernighan.

However, the concept must predate Go, for I can at least remember one article by
Rob Pike, who later designed Go, mentioning table-driven test design.
(Ironically ‒ or not so ironically ‒ that article was a critique of
object-oriented programming, as far as I can remember.)

# Table-Driven Test Design

Let's break down the parts that make up a table-driven test design.

First, a single test case is defined using a structure that contains all the
input parameters, and the expected result of the test:

    typedef struct {
        int a;
        int b;
        int expected;
    } addition_test_case;

Second, an array ‒ the test _table_ ‒ containing all the test definitions is
defined (_Given_/_Arrange_):

    addition_test_case tests[] = {
        {3, 5, 8},
        {-1, 3, 2},
    };

Third, the test table is processed using a _loop_ (structured programming,
remember that?):

    int n = sizeof(tests) / sizeof(tests[0]);
    for (int i = 0; i &lt; n; i++) {
        // omitted
    }

For every test case, the result is computed (_Act_/_When_):

    addition_test_case test = tests[i];
    int actual = add(test.a, test.b);

Fourth, the result is validated against the definition (_Then_/_Assert_):

    if (actual != test.expected) {
        printf(&quot;add(%d, %d): expected %d, got %d\n&quot;,
                test.a, test.b, test.expected, actual);
        exit(1);
    }
    printf(&quot;test_addition: %d tests passed\n&quot;, n);

An error message is printed if the `actual` value is not equal to the `expected`
value (in case `add` was implemented incorrectly):

    add(3, 5): exptected 8, got 666

Note that this test terminates after the first error. No assertions are used.
The lack of a test framework is compensated by manually defined error and
success messages.

Yes, I'm well aware of the fact that there are unit testing libraries in C. The
point is that this C code covering two test cases is only slightly longer than
the Java code to cover the same amount of test cases would be. (Using Python or
Go rather than C would have shaved off some additional lines.)

Now let's add a third and a fourth test case:

    addition_test_case tests[] = {
        {3, 5, 8},
        {-1, 3, 2},
        {13, 17, 30}, // new
        (-100, 100, 0}, // new
    };

No code was copied. No existing code was modified. Only _two_ lines of code were
added to define _two_ additional test cases. The table-driven test is
_extensible_.  Robert C. Martin would love it, wouldn't he?

# Comparing Apples to Rotten Tomatoes

So why isn't everybody writing table-driven tests instead of triple-A copy-paste
tests?

First, some programming languages make it harder to define data structures as
literals. Languages like JavaScript, Python, or Go are quite good at that. Even
C, as shown above, can be quite concise when it comes to defining static data
structures. Java recently got better at that, but up to version 8, defining a
static map structure was done by adding single elements subsequently. (Why don't
I hear _«DRY principle!!!1»_ now?)

Second, the unit testing framework plays an important role. In C, (at least as
shown above), and in Go (as it is done using the standard library), no
assertions are used. The programmer instead performs the checks manually and
reacts with a reasonable error message. The programmer is supposed to _program_
the tests.

Some unit testing frameworks that do make use of assertions also allow to add
custom error messages to every `assert` call. Other frameworks, such as
[Jest](https://jestjs.io/), just will tell you _on which line_ an assertion
failed. This is not very useful when having assertions within a loop, for the
programmer does not know which test case failed. At least for Jest, writing pure
sequential assertion code is a necessity, and the _single assert rule_ looks
quite reasonable from that perspective.

The [PyTest](https://docs.pytest.org/en/latest/) framework, for example, has
table-driven test design built-in, by providing the static test definitions
through a decorator, which is basically an annotation in Java lingo. (Check
`@pytest.mark.parametrize` for details.) However, this approach makes it
impossible to include information into the test table that needs prior
construction within the test function.

More recent versions of JUnit also allow for parametrized tests (check out the
`@ParametrizedTest` and `@ValueSource` annotations). The restrictions stated
above for PyTest also apply here. Again, the poor programmer is put into
straightjacket, for he's not supposed to _program_, but only to _test_.

My favourite test framework is from the Go standard library, which on one hand
gives the programmer total flexibility, and on the other hand provides an useful
API to construct small but powerful test runners. Checkout the
[testing](https://golang.org/pkg/testing/) package for details. (And read [The
Go Programming Language](https://gopl.io) by all means, even if you don't need
to learn Go. You'll pick up a lot about computer science in this book.)

# Single Assert Rule Revisited

The discussion about testing frameworks and programming languages (and text
editors, and tabs vs. spaces) could be extended here ad nauseam. But let's
review the _single assert rule_ instead, which could be interpreted from two
perspectives:

1. Runtime: `assert` should only be called once per execution of every test
   function/method.
2. Code: There should only be one reference to `assert` in every test
   function/method.

While the first interpretation makes table-driven design impossible, the second
interpretation might be closer to the rule's original intention: Each test case
should only verify one aspect of the function/method being called.

I'll therefore continue to happily violate the first interpretation of the rule,
for the advantages of table-driven test design (extensibility, flexibility, more
concise code) outhweigh the indiscriminate application of some hand-wavy
statements about «doing only one thing» by far. Please let me just _program_
those tests…

As an additional example, check out my test cases for some time formatting
routines
([test_timefmt.c](https://github.com/patrickbucher/countdown/blob/master/test_timefmt.c)).
Here, the test table can be used in two directions: One function uses the left
value as input and the right value as the expected outcome, while the other
function does the opposite. Here, _two_ new test cases are defined by adding
_one_ (very short) line of code.

Am _I_ allowed to shout _«Clean Code!»_ and _«DRY principle!»_ now, by the way?
</content>
    </entry>
    <entry>
        <title>Optimierung und Externalisierung</title>
        <link href="https://paedubucher.ch/articles/2020-07-04-optimierung-und-externalisierung.html"/>
        <author>
            <name>Patrick Bucher</name>
        </author>
        <id>https://paedubucher.ch/articles/2020-07-04-optimierung-und-externalisierung.html</id>
        <updated>2020-07-04T15:00:00Z</updated>
        <content type="text/markdown; charset=UTF-8">
Ich habe diesen Frühling _Heute schon einen Prozess optimiert?_ von Gunter Dueck
gelesen. Der Autor beschreibt in diesem Buch, wie in Deutschland (und im
ähnlichen Stil wohl auch in anderen Ländern) derzeit Prozessoptimiertung überall
das Gebot der Stunde ist. Historisch gesehen habe man das Wirtschaftswachstum
seit dem zweiten Weltkrieg vor allem Prozessoptimierungen im zweiten
Wirtschaftssektor (Industrie) zu verdanken. Die Autos, die wir heute fahren,
unterscheiden sich nicht grundlegend von denjenigen, die vor 50 Jahren
produziert worden sind. Ihre Herstellungsweise hat sich jedoch radikal
verändert und läuft heute grösstenteils automatisch ab.

In der Industrie sind wir mittlerweile an die Grenzen der Optimierung und des
Wachstums geraten. Grosses Wachstum gibt es nur noch im Dienstleistungssektor.
Das Problem, das Dueck beschreibt, bezieht sich auf die Dienstleistungen. Denn
hier wird genau nach dem gleichen Prinzip verfahren wie in der Industrie:
Prozessoptimierung, was das Zeugs hält! Doch sind optimierte Dienstleistungen
wirklich das, was sich der Kunde wünscht?

McDonald's ist das Paradebeispiel für Prozessoptimierung in der Gastronomie. Ich
esse sehr selten dort, und das praktisch nur, wenn es keine Alternativen gibt,
und/oder wenn ich betrunken bin. Die Bedienung erfolgt hocheffizient. Dank der
neuen Bestell- und Bezahlterminals muss man nicht einmal mehr lange an der Kasse
anstehen und sich dort mit dem Personal unterhalten. Der Bestellprozess ist
mittlerweile soweit durchoptimiert, wie es der Herstellungsprozess in der Küche
schon längstens ist.

Doch möchte ich auch in einem «richtigen» Restaurant so bedient werden? Ich gehe
gerne zwischendurch in der Mittagspause mit Bekannten ausgedehnt in einem
Restaurant essen. Dort steht neben dem guten Essen auch die Unterhaltung im
Mittelpunkt. So eine Mittagspause ist oft bereichernd und entspannend, quasi ein
Kurzurlaub vor dem Nachmittag.

Merke ich jedoch, dass die Bedienung sichtlich gestresst ist, kann ich mich beim
Restaurantbesuch kaum entspannen. Ich wähle und bestelle mein Essen sehr schnell
und versuche, die Bedienung nicht unnötig lange aufzuhalten, denn ansonsten
könnte das Ärger mit dem Vorgesetzten geben, was bloss für noch mehr Stress und
schlechte Laune sorgt. Ein Mittagessen in einem Restaurant, das
Prozessoptimierung betreibt, geht zwar schneller, ist aber kein sehr angenehmes
Erlebnis. Man könnte auch gleich zu McDonald's gehen.

Ein anderes Beispiel ist die Zustellung von Paketen. In den 90er-Jahren kam
einmal täglich ein Postbote vorbei, der auf einem kleinen Anhänger Pakete
mitführte. Für ein Dorf mit den weit ausserhalb gelegenen Bauernhöfen waren
meistens ein oder zwei Postboten verantwortlich. Zu dieser Zeit gab es
wesentlich weniger Pakete, jedoch mehr Briefe, Zeitungen, Zeitschriften usw.

Diese Postboten haben immer einen sehr entspannten Eindruck auf mich gemacht.
Oft konnte ich beobachten, dass sich der Postbote nach der Brief- und
Paketzustellung noch mit den Nachbarn unterhielt, bis er zum nächsten Haus
weiterzog. Offensichtlich hatte man damals noch Zeit…

Heutzutage ist Effizienz angesagt. Der Paketbote rennt aus seinem Kastenwagen
und will siene Ware möglichst schnell loswerden. Das ist auch nötig, denn seine
Route wurde zuvor nach tayloristischen Methoden vermessen. Die Post weiss, wie
lange der Bote für welche Anzahl Pakete maximal benötigen darf. Wird diese
Zielvorgabe nicht eingehalten, hat der Bote mit negativen Konsequenzen zu
rechnen.

Manche Paketzusteller, denn es gibt ja mittlerweile Konkurrenz zur Post,
klingeln sich so oft bei einem Mehrfamilienhaus durch. Schliesslich muss die
Sendung nicht unbedingt dem Empfänger übergeben, sondern nur in das Gebäude
hineingebracht werden. Der Bote klingelt also bei allen Hausbewohnern, und
unterbricht dabei möglicherweise eine Vielzahl von Personen bei ihrer
Beschäftigung. In den letzten Monaten könnte das durchaus Büroarbeit (in meinem
Fall Softwareentwicklung) gewesen sein, zumal viele Leute im Home-Office tätig
sind. Wie schädlich solche Unterbrechungen sein können, weiss ich als
Programmierer nur zu gut.

Ergebnis: Durch die Unterbrechungen sind die Leute weniger produktiv. Ihre
Arbeitgeber verlieren Arbeitsleistung und damit Geld, müssen ihre Angestellten
aber genau gleich entlöhnen. Der Paketzusteller spart hingegen einen Bruchteil
seiner Personalkosten, da der Zustellungsprozess mittels Durchklingeln optimiert
worden ist. Der Paketzusteller externalisiert seine Kosten ‒ das Umfeld hat
diese zu bezahlen.

Diese Prozessoptimierung führt nicht nur zu schlechteren Dienstleistungen ‒ das
Paket wurde unsanft beim Eingang abgeworfen, und nicht dem Empfänger überreicht
‒ sondern auch zu externalisierten Kosten. Denn der entstandene Schaden taucht
nicht in der Bilanz des Paketzustellers auf, jedenfalls nicht sofort. (Und
sollten die Versandhändler wegen schlechter Rückmeldungen der Logistikfirma ihre
Aufträge entziehen, dürfte diese zum Ausgleich wiederum mit weiteren
Prozessoptimierungen reagieren.)

Ich bin keinesfalls gegen die Automatisierung von mechanischen Abläufen, denn
diese ist als Softwareentwickle mein täglich Brot, ja meine
Existenzberechtigung. Es gibt Aufgaben, die der Computer schneller und präziser
ausführen kann als ein Mensch. Die zwischenmenschlichen Interaktionen sollten
jedoch nicht optimiert werden, denn diese machen oftmals die Qualität einer
Dienstleistung aus. Solche Optimierungen führen oft bloss zu Frust auf beide
Seiten ‒ und eben zu externalisierten Kosten, von denen wir sonst schon viele
haben (Umweltverschmutzung, Lärmbelastung, Littering usw.)

Fazit: Wir sollten beim Optimieren von Prozessen nicht nur darauf achten, dass
dabei die Dienstleistung und der zwischenmenschlicher Umgang nicht
beeinträchtigt werden. Wir sollten auch darauf achten, dass wir unsere
Einsparungen nicht unseren Mitmenschen als externalisierte Kosten aufbürden.
</content>
    </entry>
    <entry>
        <title>Meine Linux-Distributionen</title>
        <link href="https://paedubucher.ch/articles/2020-06-28-meine-linux-distributionen.html"/>
        <author>
            <name>Patrick Bucher</name>
        </author>
        <id>https://paedubucher.ch/articles/2020-06-28-meine-linux-distributionen.html</id>
        <updated>2020-06-28T22:00:00Z</updated>
        <content type="text/markdown; charset=UTF-8">
Ich verwende seit 2005 hauptsächlich Linux als Betriebssystem. Dabei habe ich
schon Erfahrungen mit verschiedenen Distributionen sammeln können. Meistens
hatte ich eine Hauptdistribution, die ich praktisch auf all meinen Rechnern
installiert war. Dies ändert sich jetzt vielleicht. Doch der Reihe nach…

# Mandrake: Wie alles begann

Meine ersten Erfahrungen mit Linux habe ich im Jahr 2004 gemacht. Alles begann
damit, dass eMule (das damals wichtigste File-Sharing-Tool, das einen
Stellenwert hatte, wie es heute BitTorrent hat) auf dem Windows-Rechner der
Familie nicht mehr richtig funktionierte. Irgendetwas musste ich am
Betriebssystem kaputt gemacht haben.

Eine mögliche Lösung wäre es gewesen, den Rechner neu mit Windows XP
aufzusetzen. Das konnte ich aber nicht so einfach tun, da auch andere
Familienmitglieder Dateien auf dem Rechner hatten. So musste ich immer um
Erlaubnis bitten, wenn ich den Rechner neu aufsetzen wollte. Ausserdem dauerte
es oft Tage, bis wieder alles funktionstüchtig war.

Da ich eine zweite Festplatte hatte, die ich sonst für nichts brauchte, wollte
ich stattdessen einen Dual-Boot einrichten. So gab ich _Mandrake Linux_ (heute
_Mandriva_) eine Chance. Die Installation lief problemlos ab, und auch der Dual
Boot mit Windows klappte problemlos. Meine Familie konnte weiter standardmässig
nach Windows booten.

Die KDE-Oberfläche war für mich einfach bedienbar. Da ich bereits auf Windows
diverse OpenSource-Programme (OpenOffice.org, VLC Media Player, Firefox)
verwendete, kam ich recht schnell mit dem Betriebssystem zurecht. eMule lief
tatsächlich unter Mandrake. Das Problem war aber, wie ich die heruntergeladenen
Dateien vom Rechner wegkopieren sollte.

Der USB-Stick (Kapazität: 128 MB), den ich an einem überbetrieblichen Kurs
(Computer zusammenbauen) erhalten hatte, wurde nicht automatisch erkannt. Und
das mit dem `mount`-Befehl, was von der Google-Suche ausgespuckt worden war,
überforderte mich dann doch noch. Der Zugriff auf die Windows-Festplatte (NTFS)
funktionierte (out of the box) leider nur lesend. So werde ich mir wohl die
heruntergeladenen Dateien auf CDs gebrannt haben, denn die Brennsoftware
funktionierte problemlos.

Ansonsten verlor ich bald das Interesse an Mandrake und bootete nur noch nach
Windows.

# SuSE: Linux als neues Zuhause

Es muss wohl Ende 2004 oder Anfang 2005 gewesen sein, als ich mir zum ersten mal
SuSE installierte. Wahrshceinlich war es Version 9.2 oder 9.3. Wieder
installierte ich es auf der zweiten Festplatte neben Windows. Doch dieses mal
sollte ich dabei bleiben.

Im Sommer 2005 wechselte ich nach zwei Jahren Lehrlingsaustausch bei der [Data
Unit AG](https://www.dataunit.ch) in die Softwareentwicklung bei [Bison Schweiz
AG](https://www.bison-group.ch). Nach zwei eher Microsoft-geprägten Jahren
sollte ich nun also ein Java-Entwickler werden. In der Schule arbeiteten wir mit
C#. Doch unser Lehrer in den Programmierfächern, Roland Bucher, der beide
Programmiersprachen kannte, war so flexibel, dass er uns die Wahl der
Programmiersprache frei liess. So rückte ich ab von C# und beschäftigte mich
bereits im zweiten Lehrjahr, also bevor ich den Arbeitsplatz wechselte, mit
Java.

Es muss kurz vor diesem Wechsel gewesen sein, als ich auf
[Heise.de](https://www.heise.de) einen Artikel über die Zukunftsstrategie von
Microsoft gelesen hatte. Dabei kündigte der damalige CEO Steve Ballmer an, dass
Microsoft so etwas wie _full spectrum dominance_ in der IT erreichen wollte. Das
Forum zu dieser News-Meldung war damals voller ablehnender Beiträge. Microsoft
wurde zu dieser Zeit vom unsympathischen Monopolisten zum absoluten Hassobjekt,
und das nicht nur für mich. Für mich war klar, dass ich von Microsoft und damit
von Windows weg musste.

Es kam dazu, dass ich die Dokumentation [The
Code](https://www.youtube.com/watch?v=XMm0HsmOTFI) gesehen hatte. Nun
interessierte ich mich nicht nur für GNU/Linux als Betriebssystem, sondern für
die Freie-Software-Bewegung als Ganzes. Für mich war eine neue Welt aufgegangen.
Leute wie Richard Stallman, Linus Torvalds und Alan Cox waren meine neuen Idole.

Im Herbst 2005 hatten wir in der Lehre unsere Zwischenprüfungen (pardon:
Teilabschlussprüfungen). Hierfür habe ich mit einer Gruppe von fünf
Klassenkameraden einige Zusammenfassungen geschrieben. Diese sind immer noch
in einem [Archiv](https://github.com/patrickbucher/archive/tree/master/pdfs) auf
GitHub zu finden. Wir nannten uns damals «Team Eichhof». (Das würde ich heute
auch nicht mehr machen…) All diese Dokumente wurden in OpenOffice.org
geschrieben. Ich war der einzige von uns sechs, der das verwendete. Ich weiss
nicht einmal mehr genau, wie ich die Beiträge meiner Kollegen eingebunden hatte.
Wahrscheinlich habe ich sie aus den Word-Dokumenten der Kameraden rauskopiert.

Die meiste Zeit war ich nun auf Linux unterwegs, wobei ich diese
Zusammenfassungen natürlich auch unter Windows hätte bearbeiten können. Wichtig
war, dass mein jeweils aktuelles Arbeitsverzeichnis nun auf der Linux-Festplatte
lag. Beim Dual Boot wählte ich nun immer seltener Windows aus.

Sollten die Zwischenprüfungen problemlos ablaufen, und sollte ich alles
bestehen, wollte ich mir meinen ersten eigenen Computer zur Belohnung kaufen.
Natürlich würde ich mir den selber zusammenbauen, und bloss die Komponenten dazu
kaufen. Wichtig war, dass die Komponenten alle gut von Linux unterstützt wurden.
Das war damals beispielsweise bei WiFi-Karten gar nicht selbstverständlich. Und
da der Computer in meinem Zimmer stehen sollte, war ein Ethernet-Kabel leider
keine Option.

Ein Berufsschulkollege, der schon seit frühem Jugendalter mit Linux arbeitete,
und auch bereits seine eigene Firma hatte, war hierfür ein guter
Ansprechpartner. Ich bestellte die Hardware bei ihm. (Die Prüfungen waren
übrigens sehr gut gelaufen.) Ich staunte sehr, dass er mir die Komponenten mit
seinem eigenen Firmenauto lieferte.

Den Computer hatte ich bald zusammengebaut. Doch leider liess sich SuSE Linux
darauf nicht installieren ‒ oder zumindest funktionierte das WiFi nicht, so
genau kann ich mich nicht mehr darain erinnern. Auf jeden Fall gab es ein
Problem mit SuSE. So habe ich einen Plan B gebraucht.

# Ubuntu: Ein gelungener Umstieg

Zu dieser Zeit wurde gerade _Ubuntu_ einigermassen populär. Ich war zwar auf
SuSE ein begeisterter KDE-Benutzer und hätte darum auch zu _Kubuntu_ wechseln
können. Ich wollte aber doch lieber das «Original» einmal ausprobieren.

Ubuntu liess sich problemlos installieren. Ich weiss nicht mehr, ob es _Breezy
Badger_ (5.10, am 12. Oktober 2005 erschienen) oder die Vorgängerversion _Hoary
Hedgehog_ (5.04, am 8. April 2005 erschienen) war. Auf jeden Fall funktionierte
alles auf Anhieb, auch das WiFi.

An GNOME gewöhnte ich mich sehr schnell. Es war übersichtlicher und eleganter
als KDE. Es funktionierte alles so, wie es musste. Aus dieser Zeit ist mir
ansonsten eher wenig geblieben.

Ab und zu musste ich wohl auch noch am Windows-Rechner arbeiten, denn in der
Berufsschule wurde immer noch der Microsoft-Stack unterrichtet. _Microsoft SQL
Server_ habe ich mit Sicherheit einmal verwenden müssen. Geblieben ist mir davon
wenig. Die gleichen Übungen hätte man auch mit MySQL oder PostgreSQL machen
können.

2006 kaufte ich mir dann sogar einen eigenen Laptop. Der Lehrlingslohn war ja
mit dem dritten Lehrjahr bedeutend angestiegen. Das HP-Notebook hatte einen
verspiegelten Bildschirm. (Diesen Fehler würde ich heute nicht mehr machen.)
Doch Ubuntu lief darauf problemlos. Ich konnte den Laptop auch in die Schule
mitbringen und darauf arbeiten. Aber ans Netzwerk durfte ich ihn nicht
anschliessen, aus Sicherheitsgründen, versteht sich. Eine externe USB-Festplatte
diente zum Dateiaustausch.

So bin ich bis zum Lehrabschluss bei Ubuntu geblieben. Für die
Lehrabschlussprüfungen haben wir wieder in der gleichen Gruppe wie zwei Jahre
zuvor Zusammenfassungen geschrieben. Dieses mal nicht mehr als «Team Eichhof»,
aber wiederum mit OpenOffice.org. Die Zusammenfassung für die Allgemeinbildung
hatte ich selbständig mit LaTeX verfasst. (Diese war Jahre später noch einem
Lehrling hilfreich, sodass sich dieser per E-Mail bei mir bedankte.)

# Debian: Ubuntu für Erwachsene

2009 kaufte ich mir gleich zwei Computer. Einerseits einen Dell OptiPlex als
Computer für mein Zimer, und andererseits ein Lenovo Thinkpad (mit grosszügigem
Studentenrabatt) für mein Informatik-Studium.

Ich weiss nicht mehr, ob Ubuntu auf einem der beiden Rechnern nicht
funktionierte. Auf jeden Fall stieg ich in dieser Zeit auf Debian um, das den
Ruf hatte, schwer installierbar zu sein. Tatsächlich waren es einfach ein paar
Klicks mehr im Setup-Menü als bei Ubuntu.

Auf meinem Laptop hatte ich einen Dual Boot eingerichtet, da ich ja im
Informatikstudium weiterhin würde Windows verwenden müssen. (Daran hat sich bis
heute kaum etwas geändert.)

Von Ubuntu her waren mir viele Konzepte für Debian schon bekannt, zumal ja
Ubuntu auf Debian basiert. Den Paketmanger `apt-get` verwendete ich auch über
die Kommandozeile, und kaum noch über ein grafisches Tool, dessen Name mir
entfallen ist.

Ich arbeitete nun schon seit etwa fünf Jahren mit Linux, war aber nur ein
Anwender, und keinesfalls ein Profi. Wenn ich etwas auf der Kommandozeilen
machen musste, dann kopierte ich mir diese Befehle von einer Webseite, und
hoffte, dass sie funktionieren würde. Ich war auch weiterhin in der alten
Windows-Routine verhaftet, dass ich das Betriebssystem komplett neu
installierte, wenn etwas grundsätzliches nicht mehr funktionierte. Verstanden
habe ich vom System sehr wenig.

Zu dieser Zeit verlor ich auch die Lust an der Informatik. Der Grund dafür
dürfte eine Kombination aus meiner Situation in Beruf und Hochschule gewesen
sein, wobei auch der Mangel an Freizeit über mehrere Jahre (Berufsmatura,
Studium) mit Lektionen am Samstag, an den Abenden und Lernen am Wochenende auch
eine Rolle gespielt haben dürfte.

Ich entschloss mich dazu, mein Informatikstudium abzubrechen (bzw. offiziell
bloss zu unterbrechen), und die Matura nachzuholen. Ich wollte lieber
Geistes- und Sprachwissenschaften studieren, als mich noch länger mit der
Informatik zu beschäftigen. Zunächst wollte ich aber mein Französisch aufbessern
und ging im Sommer 2010 für einige Wochen nach Paris.

Auf diese Zeit geht auch meine Aversion gegen Bloatware zurück. Ein
Schlüsselerlebnis dürften für mich die Vorträge von [meillo](http://marmaro.de/)
beim Chaos Computer Club Ulm gewesen sein. Schliesslich war es der Window
Manager [dwm](http://dwm.suckless.org/), der mich nachhaltig auf einen anderen
Pfad bringen sollte: Weg vom GUI, hin zur Kommandozeile!

Zunächst verwendete ich weiterhin den GNOME-Login-Bildschirm. Ich schaffte es,
`dwm` als zweite Option (neben dem GNOME-Desktop) zu konfigurieren. So konnte
ich notfalls immer noch auf GNOME ausweichen. Meine grafische Oberfläche war
aber nun `dwm`. Dies hat sich bis heute nicht geändert.

Ich verwendete dieses Setup einige Jahre lang auf meinem Laptop und meinem
Heimrechner. Nun machte ich auch Fortschritte auf der Kommandozeile. Ich
verwendete aber immer noch grösstenteils die Konfigurationstools des Systems.
Für die Netzwerkverbindung war beispielsweise WICD im Einsatz.

In der Zwischenzeit war in meinem Leben einiges passiert: Ich absolvierte die
Passerelle, hatte ein einjähriges Gastspiel in Fribourg, wo ich Slavistik und
Germanistik studierte ‒ und kehrte 2012 dann doch wieder in die Informatik
zurück. Meine Lust am Programmieren hatte ich wohl wiederentdeckt.

In diesen Jahren hatte ich mir auch ein Netbook angeschafft: eine Gattung
Geräte, die von den Tablets verdrängt worden sind. Es muss auf diesem Netbook
gewesen sein, wo ich zum ersten mal ein Betriebssystem ohne GUI installiert
habe. Seither startete ich `dwm` direkt von der Kommandozeile, einen
Login-Screen hatte ich nicht mehr. Diese Installation dokumentierte ich in einem
Artikel namens [Lean Debian](https://web.archive.org/web/20150217043316/http://paedubucher.ch/docs/lean-debian.html).

# Arch: Das vorläufige Ende einer Reise

2016 entschied ich mich dazu, mein Informatik-Studium an der Hochschule Luzern
wieder aufzunehmen und also doch noch zu beenden. Im Sommer hatte ich eine
Aktion entdeckt: einen ultraschwachen Acer-Laptop für 199 Franken mit 32 GB
internem Speicher, der dafür aber extrem leicht und energieeffizient war: der
ideale Laptop fürs Studium!

Die Debian-Installation scheiterte dabei leider. Ich stand wieder vor dem
gleichen Problem, das mich schon früher hat die Distribution wechseln lassen.
Doch mit Debian war ich doch so zufrieden…

Ich probierte verschiedenste Distributionen aus. Einige davon basierten auf _Arch
Linux_. Damit funktionierte alles auf Anhieb, ich hatte aber immer die grafische
Benutzeroberfläche dabei. So wagte ich mich an die manuelle Installation des
«richtigen» Arch Linux heran, wofür ich seither eine personalisierte
[Dokumentation](https://github.com/patrickbucher/docs/blob/master/arch-setup/arch-setup.md)
führe.

Die ganze Sache lief doch recht problemlos ab, sodass ich Arch gleich noch auf
meinem «richtigen» Laptop installierte. (Ich wollte damals diesen Laptop für
Windows brauchen, war aber jetzt zu begeistert von Arch.) Dabei musste ich wohl
vergessen haben, das Mounten der `/boot`-Partition in `/etc/fstab` festzuhalten,
sodass sich der Laptop nach dem nächsten Kernel-Update nicht mehr aufstarten
liess.

Ich verfluchte Linux wie kaum jemals zuvor ‒ und wie seither niemals wieder.
Denn der Fehler war ganz klar auf meiner Seite. Endlich lernte ich etwas übers
System. Das Problem löste ich nicht durch eine komplette Neuinstallation,
sondern indem ich das System mit dem USB-Stick startete und das Mounten der
`/boot`-Partition korrekt konfigurierte. Für mich war das ein Meilenstein.

Im Studium habe ich mich dann weitgehendst an Linux gehalten. Ausnahmen waren
Prüfungen mit dem _Safe Exam Browser_, der eben nur unter Windows und macOS das
System komplett blockieren konnte. In den Modulen _C# in Action_ und
_Microcontroller_ stand auch gezwungenermassen Windows-Einsatz auf dem Programm,
sodass es kaum ein Zufall ist, dass ich diese beiden Module abgebrochen habe.

In der Zwischenzeit arbeitete ich in einer Firma mit macOS. Auf meiner neuen
Stelle kann ich komplett mit Linux arbeiten. Neben Arch Linux auf dem Laptop
kommt auf den Servern Ubuntu zum Einsatz.

# Ausprobiert: Alpine Linux, OpenBSD, FreeBSD

Wenn ich mit Docker-Containern arbeite, ist oft das schlanke _Alpine Linux_
meine Wahl für das Base-Image. Auf einem Heimrechner oder auf einem Laptop habe
ich es bisher noch nicht ernsthaft verwendet. Das dürfte wohl mit der etwas
älteren Kernel-Version zusammenhängen. Auch auf Servern verwende ich es nicht,
da es von vielen Cloud-Anbietern nicht angeboten wird. Dort verwende ich Debian
‒ oder Ubuntu, wenn ich auf neuere Packages angewiesen bin. (Lokal kann man
schon einmal Debian Testing verwenden, das läuft dermassen stabil.)

Weiter habe ich dieses Jahr einige kleinere Ausflüge in die BSD-Welt
unternommen. OpenBSD scheint mir wie geschaffen zu sein für meine Ansprüche:
alles ist minimal, standardmässig sinnvoll konfiguriert und sicher. FreeBSD ist
mir in der Firma begegnet, wo ein Backup-Server (mit ZFS als Dateisystem) damit
läuft.

Für meinen privaten Einsatz konnte sich aber noch keines der beiden Systeme
gegen Arch durchsetzen. Gerade bei Laptops läuft Linux mittlerweile so gut, dass
die BSDs eher ein Rückschritt in vielerlei Hinsicht wäre.

Seit einigen Monaten betreibe ich einen kleinen Server in der Cloud auf Debian.
Hier wäre vielleicht OpenBSD eine sinnvolle Alternative, die ich gelegentlich
prüfen sollte. Überhaupt möchte ich mich gelegentlich stärker mit den BSDs
befassen als mit Linux.

Für die «Hardcore»-Distributionen wie _Gentoo_ und _Linux from Scratch_ konnte
ich mich bisher noch nicht begeistern. Es wären wohl beides gewinnbringende
Übungen.

Im Moment stehen für mich aber andere Themen an, z.B. die funktionale
Programmierung. So bleibe ich vorerst bei Arch Linux, und lasse mich von der
Zukunft überraschen… OpenBSD und FreeBSD laufen mir ja nicht weg.
</content>
    </entry>
    <entry>
        <title>Hallo, Welt!</title>
        <link href="https://paedubucher.ch/articles/2020-06-28-hallo-welt.html"/>
        <author>
            <name>Patrick Bucher</name>
        </author>
        <id>https://paedubucher.ch/articles/2020-06-28-hallo-welt.html</id>
        <updated>2020-06-28T19:00:00Z</updated>
        <content type="text/markdown; charset=UTF-8">
Dies ist ein Demo-Artikel auf meiner neuen Webseite
[paedubucher.ch](http://paedubucher.ch). Die Seite ist mithilfe eines einfachen
statischen Webseiten-Generators erstellt, der auf
[GitHub](https://github.com/patrickbucher/paedubucher.ch) verfügbar ist. Wie
dieser funktioniert, werde ich gerne einmal ausführlicher erklären.

Ich möchte in Zukunft auf meiner Webseite mehr schreiben, auf Deutsch und auf
Englisch, je nach Lust und Laune ‒ und Thema.
</content>
    </entry>
</feed>
