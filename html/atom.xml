<?xml version="1.0" ?>
<feed xmlns="http://www.w3.org/2005/Atom">
    <title>paedubucher.ch</title>
    <subtitle>paedubucher.ch Article Feed</subtitle>
    <link href="https://paedubucher.ch/atom.xml" rel="self"/>
    <link href="https://paedubucher.ch/"/>
    <id>https://paedubucher.ch/</id>
    <updated>2023-01-28T21:54:56.399544Z</updated>
    <entry>
        <title>Six Months of SICP</title>
        <link href="https://paedubucher.ch/articles/2023-01-28-six-months-of-sicp.html"/>
        <author>
            <name>Patrick Bucher</name>
        </author>
        <id>https://paedubucher.ch/articles/2023-01-28-six-months-of-sicp.html</id>
        <updated>2023-01-28T22:00:00Z</updated>
        <content type="text/markdown; charset=UTF-8">
Last summer, I decided to work through SICP, at least through the first two
chapters, and probably also through the third. I wrote about my progress [after
the first month](./2022-08-30-one-month-of-sicp.html). Back then, I established a daily
habit, but was also a bit pessimistic whether or not I'd stick to it. I worked
three days for the company, teached two days a week, and on the weekends, I had
to prepare the lessons for the next week. I also knew that I was moving to a new
place in October. So how could I possibly continue with my daily efforts?

To this day, I didn't miss a single day. Even on the day of our move, I was able
to [do an
exercise](https://github.com/patrickbucher/sicp/blob/master/diary.md#2022-10-04-tu).
I installed MIT Scheme, Emacs, and Geiser on a laptop a couple of days before,
and I pushed my commit through a hotspot provided by my smartphone. I kept using
this setup until a proper internet connection was established in my new flat.

Fortunately, I wasn't sick for a single day, besides the occasional headaches.
But since I worked on SICP as the first thing in the morning, nothing could
really get in its way. I made it a priority, and so it worked out fine. However,
I sometimes had to work on SICP in the train on my way to work or back home, but
I did at least one commit every day. Sometimes it was just an entry into my
diary that I read a couple of pages, but usually I also committed code. On some
exercises, I had to work many days, sometimes about a week or so. But the most
important thing was to start the editor, to look at the code, and at least to
try it.

# Challenges

The second chapter had fewer mathematical proofs and exercises where I needed to
trace long chains of function calls. It was very hard, nonetheless.

- Some examples and exercises were about image processing. They were not too
  difficoult, after all, but required quite some additional work. First, the
  setup of the image library wasn't documented. This is actually quite
  understandable, because the implementation from the 80s and 90s probably
  wouldn't have worked on a modern machine anyway. I figured out a way to do the
  exercises in Racket. Its Scheme dialect is basically a superset of MIT Scheme.
  Having figured out my setup and the way the API worked, I went through the
  exercises rather quickly. DrRacket isn't great if you're used to Vim and
  Emacs, but it worked quite nicely.
- The last part of chapter 2 was quite a bit annoying. In fact, I considered
  putting away SICP on multiple occasions. There's a myth about SICP: that it
  only introduces state in the third chapter; after more than 200 pages, that
  is. Indeed, state isn't introduced until the third chapter, but the exercises
  in the last part of chapter 2 heavily rely on state: A lookup table for
  generic procedures can only be managed using state. Figuring out the workings
  of the `set!` procedure hasn't been an issue, but the whole arithmetics
  package that was developed was quite involved. Different examples and
  exercises provided different ways of doing computations on different types,
  and they didn't always play well together. I had to skip exercise 2.92, which
  was a sacrifice I had to do in order to finish the chapter. I am very glad
  that I choose this tradeoff, which allowed me to keep going.

So chapter 2 offered quite some challanges; the first due to the short lifespan
of technology, the second one, I daresay, due to some didactic imperfections. I
learned a lot about functional programming in the chapter, and writing tail-call
optimized recursive functions really became second nature to me.

I've spent six months on two chapters and worked through 216 pages. The book has
five chapters on 610 pages, i.e. three chapters on almost 400 pages are left. If
I'd continue at the current pace, I could finish SICP roughly at the end of the
year, if not later.

# What's Next?

My initial plan was to do at least the first two chapters, but probably also the
third one. The subjects of the third chapter—state, scope, data structures,
concurrency, and streams—are important concepts that well transfer to other
functional programming languages and environments. It's also one of the biggest
chapters with 140 pages.

The fourth chapter is about evaluation and the inner workings of LISP. The fifth
chapter is about register machines and simulations. Those two chapters might
lead to the often quoted LISP enlightment. However, I might consider a break
after chapter 3. With this much Scheme exposure under my belt, I probably could
pick up SICP again years later on chapter 4.

Chapter 3 might take three months, so I have plenty of time to consider my
future plans. My long-term plan is to stick to functional programming, and I'd
like to pursue my path on three different tracks:

## LISP

Having finished at least three of the five chapters of SICP, I'll probably leave
Scheme behind and try out different LISPs:

1. _Clojure_: I already used it for some toy examples (computing soccer league
   tables, simulating the Game of Life), and it yielded very concise code.  This
   would be a practical choice that could also be used for writing web
   applications.
2. _Racket_: Its Scheme dialect comes with nice additions such as hash tables
   and support for concurrent programming, and might be even useful for very
   practical things.

## ML

I had some exposure to Standard ML and Haskell before, and I consider its type
system to be extremely helpful, especially when I look at the messy arithmetic
package I was developing in chapter 2 of SICP. There are a lot languages in the
ML family, of which I consider the following the most interesting:

1. _Elm_: A domain-specific language for writing web frontends that compile to
   JavaScript. I worked through some basic examples, but the clean way of
   handling state really impressed me. I'd like to rewrite some of my old
   JavaScript toy programs in Elm.
2. _Haskell_: This I consider to be the holy grail of functional programming.  I
   really want to figure out how to write real-world applications with a pure
   functional programming language.
3. _OCaml_: This is supposed to be a more relaxed version of Haskell, which is
   quite close to Elm, and provides a compiler that produces rather fast
   binaries.

## Erlang

This was my initial motivation for functional programming, because it is also
about my second pet subject: concurrency. So I have to learn it!

1. _Erlang_: Even though Elixir is considered to be the primary choice nowadays,
   one needs to know the host language when dealing with the hosted language.
   The Prolog-like syntax doesn't bother me at all; and the boiler-plate code
   doesn't look too scary.
2. _Elixir_: For web applications, I'd probably use Elixir instead of Erlang.
   Elixir comes with some really nice additions, such as nice tooling and the
   pipe operator.

---

So I have plenty of ideas, but I should stick to one subject at a time, to which
I devote some daily practice. It worked quite well with SICP so far. And so it
will for my next endeavour, hopefully!
</content>
    </entry>
    <entry>
        <title>Jahresrückblick 2022 und Ideen für 2023</title>
        <link href="https://paedubucher.ch/articles/2022-12-22-jahresrueckblick-2022-und-ideen-fuer-2023.html"/>
        <author>
            <name>Patrick Bucher</name>
        </author>
        <id>https://paedubucher.ch/articles/2022-12-22-jahresrueckblick-2022-und-ideen-fuer-2023.html</id>
        <updated>2022-12-22T14:30:00Z</updated>
        <content type="text/markdown; charset=UTF-8">
Letztes Jahr habe ich einen
[Jahresrückblick](./2021-12-17-jahresrueckblick-2021.html) geschrieben und kurz
darauf einige [Ideen für 2022](./2021-12-31-ideen-fuer-2022.html)
aufgeschrieben. Was ist ‒ ersteres wiederholend ‒ aus letzterem geworden?

# Ideen umgesetzt?

Vorweg: die drei Themen bzw. Programmiersprachen, die ich mir notiert habe ‒
Elixir, Go und JavaScript ‒ haben mich alle beschäftigt. Oberflächlich
betrachtet habe ich also 2022 das gemacht, was ich mir vorgenommen hatte. Doch
wie sieht es bei genauerer Betrachtung aus?

## Elixir

Tatsächlich habe ich mich von Januar bis April immer wieder mit Elixir befasst,
wie die Commits im entsprechenden
[Repository](https://github.com/patrickbucher/elixir-basics) zeigen. Das Buch
[Elixir in
Action](https://www.manning.com/books/elixir-in-action-second-edition) habe ich
jedoch nur etwa zu zwei Dritteln durchgearbeitet. Ich bin bei den eigentlich
interessanten Teilen (Fehlerbehandlung) hängen geblieben.

Die 
[Elixir-Implementierung meines
SuperLeague-Programms](https://github.com/patrickbucher/superleague-polyglot/tree/master/superleague-elixir)
habe ich überarbeitet und dabei deutlich verbessern können. Soweit das Positive.

Mit Phoenix und dem Neuschreiben meiner
[Reversi-Simulation](https://github.com/patrickbucher/revergo) in Elixir ist es
jedoch nichts geworden.

Das einzige grössere Beispiel war eine nebenläufige
[Primzahlensuche](https://stackoverflow.com/q/71456020/6763074), die dann nicht
so recht skalieren wollte. Mit Go habe ich das Problem hingegen auf Anhieb lösen
können. Das dürfte dann sogleich der Ausstiegspunkt bzw. der Umstiegspunkt auf
Go gewesen sein.

## Go

Mit Generics habe ich mich nur im kleinen Rahmen befasst. Im Zusammenhang mit
der funktionalen Programmierung konnte ich Generics für eine kleine
[filter/map/reduce-Library](https://github.com/seantis/go-functools) nutzen.
Auch habe ich mir [Monaden in Go](./2022-11-05-towards-monads-in-go.html)
angeschaut, jedoch ohne generische Sprachkonstrukte.

Auf der Arbeit habe ich ein Memory-leakendes Logging-Tool (in Python
implementiert) durch ein einfacheres Tool in Go ersetzt. Das habe ich in Etappen
gemacht; mit dem Ergebnis bin ich recht zufrieden. Ich habe es schon mehrmals
verbessert und erweitert, was recht einfach ging.

Mit [Cloud Native
Go](https://www.oreilly.com/library/view/cloud-native-go/9781492076322/) und [12
Factor Apps](https://12factor.net/) habe ich mich nur kurz befasst, da ich für
das Cloud-Modul an der Berufsschule kaum über die Grundlagen herausgekommen bin.

Auch [gin](https://github.com/gin-gonic/gin) habe ich mir nicht angeschaut,
wobei ich ohnehin eher zu [Fiber](https://gofiber.io/) oder
[Gorilla](https://www.gorillatoolkit.org/) tendieren würde.

An privaten Projekten sind doch einige kleinere Sachen in Go zusammengekommen:

- [checklinks](https://github.com/patrickbucher/checklinks): ein
  Kommandozeilenwerkzeug, der die Links einer Webseite überprüft, um tote Links
  finden zu können
- [openbsd_autoinstall](https://github.com/patrickbucher/openbsd_autoinstall):
  ein minimalistischer HTTP-Server, der eine `install.conf` für OpenBSD liefert,
  was etwa für das Bauen von Images mit [Packer](https://www.packer.io/)
  sinnvoll sein kann
- [dfdegoregexp](https://github.com/patrickbucher/dfdegoregexp): eine kleine
  Einführung in reguläre Ausdrücke mit Go, für das deutsche Debianforum
  geschrieben
- [meow](https://github.com/patrickbucher/meow): ein kleines Monitoring-Tool als
  Anschauungsmaterial für den Berufsschulunterricht
- [huffman](https://github.com/patrickbucher/huffman): Textkompression mit
  Huffman-Bäumen als Lernprojekt

Für den Berufsschulunterricht habe ich auch eine Reihe von
[Videos](https://www.youtube.com/@m346pb/videos) mit Go-Bezug aufgenommen.

Immerhin…

## JavaScript

Ab Februar habe ich an der Berufsschule ein Praxismodul zum Thema
Web-Entwicklung unterrichtet. Hierfür habe ich mich wieder einmal etwas mit
JavaScript befasst. Dabei habe ich eher [Node.js](https://nodejs.org/en/) als
[Deno](https://deno.land/) verwendet, obwohl mir letzteres besser gefällt. Es
wurde jedoch November, bis ich mein SuperLeague-Programm mit
[JavaScript/Deno](https://github.com/patrickbucher/superleague-polyglot/tree/master/superleague-javascript)
geschrieben habe.

Neben kleineren Programmierbeispielen für den Unterricht habe ich das [Game of
Life](https://github.com/patrickbucher/js-game-of-life) in JavaScript
geschrieben. Ein Überraschungserfolg wurde mein kleines
[Jass-Spiel](https://github.com/patrickbucher/jassete), das auch vom
[Schweizer
Jassverzeichnis](https://jassverzeichnis.ch/online-jass-wettspiel-im-jass-stuebli/)
verlinkt worden ist, ohne dass ich es dort gemeldet hätte. Man braucht nur
eine genügend schmale Nische zu finden…

Zum Thema JavaScript habe ich auch einige
[Videos](https://www.youtube.com/@ipt6web-entwicklung264/videos) für den
Berufsschulunterricht aufgenommen. Beruflich habe ich JavaScript sonst kaum
verwendet.

---

Soviel zu den gesteckten Zielen.

# Und sonst so?

Es muss so im April gewesen sein, als ich Elixir erneut habe fallen lassen. Ich
muss mich wohl etwas mit Clojure beschäftigt haben, wovon die entsprechende
[SuperLeague-Implementierung](https://github.com/patrickbucher/superleague-polyglot/tree/master/superleague-clojure)
zeugt. (Das gleiche habe ich noch in
[C](https://github.com/patrickbucher/superleague-polyglot/tree/master/superleague-c)
und in
[Racket](https://github.com/patrickbucher/superleague-polyglot/tree/master/superleague-racket)
gemacht, doch von letzterem später mehr…)

Einige neue Bücher auf meinem Regal zeugen auch von dieser Beschäftigung mit
Clojure. Statt eines gründlich durchzuarbeiten, habe ich verschiedene Bücher
angelesen. So richtig vorwärtsgekommen bin ich dabei nicht. Frustriert von
meiner fehlenden Konzentration ‒ und im Hinblick auf ein zweites Halbjahr mit
nur sehr wenig Freizeit, habe ich dann Clojure wieder fallen lassen, jedoch
Scheme wieder aufgenommen.

## SICP: der zweite Versuch

Ich habe mir folgendes überlegt: Wenn ich
[SICP](https://mitpress.mit.edu/9780262510875/structure-and-interpretation-of-computer-programs/)
durcharbeite, profitiere ich dabei sicherlich auch für Clojure, sowie für andere
(funktionale) Programmiersprachen. Und wenn ich täglich daran arbeite ‒ und sei
es auch nur eine gelesene Seite oder eine angefangene Übung ‒ bleibe ich
sicherlich nicht stehen. Gerade das zweite Halbjahr würde mir die Vorbereitung
des Berufsschulunterrichts sehr viel Aufwand bereiten. So bleibe ich zumindest
nicht stehen in meiner Beschäftigung mit der funktionalen Programmierung.

So schaue ich nun auf über vier Monate [täglicher
Beschäftigung](https://github.com/patrickbucher/sicp/blob/master/diary.md) mit
SICP zurück. Sogar am [Tag meines
Umzugs](https://github.com/patrickbucher/sicp/blob/master/diary.md#2022-10-04-tu)
konnte ich eine Übung lösen, die ich dann per Smartphone-Hotspot auf GitHub
gepusht habe. Ja, ich habe es dieses mal durchgezogen!

Gelernt habe ich dabei so einiges. Einige Frustrationen konnte ich
gewinnbringend überwinden, in dem ich etwa auf Racket ausweichen musste, um die
Beispiele mit der Bildverarbeitung im zweiten Kapitel testen zu können. (Aus
dieser Beschäftigung stammte auch die SuperLeague-Implementierung.)

Kapitel 2 könnte ich noch dieses Jahr beenden. Kapitel 3 möchte ich sicherlich
auch noch durcharbeiten. Kapitel 4 und 5 hingegen könnten warten, und ich könnte
mein gewonnenes Wissen über die funktionale Programmierung vielleicht einmal
praktisch anwenden.

---

Für die Berufsschule habe ich mich noch etwas mit Packer befasst, um eine
Ubuntu-VM für den Unterricht automatisch bauen zu können. Packer war auch auf
der Arbeit ein Thema. Weiter habe ich mich wieder einmal etwas mit Redis und
MinIO befasst, auch das für die Berufsschule. Meine Shell-Skripts für die
[Gitea-Administration](https://github.com/patrickbucher?page=1&amp;tab=repositories)
habe ich in Python umgeschrieben und erweitert, was sich durchaus gelohnt hat.

Auf der Arbeit habe ich keine grösseren Würfe zu verzeichnen. Ich habe vor mich
hingewerkelt und das eine oder andere verbessert. Ein Werkzeug zur Auslosung
interner Restore-Tests hat mich zur kurzen Beschäftigung mit
[GraphQL](https://graphql.org/) gebracht. Auch [Podman](https://podman.io/) war
kurz ein Thema. Mit Ruby und Vagrant hatte ich mich auch kurz beschäftigt,
jedoch zu wenig nachhaltig. Ansible ist auch wieder unter den Tisch gefallen,
Kubernetes genauso, wobei ich mir letzteres gar nicht vorgenommen hatte.

Mit zwei Freunden halte ich einen unregelmässizen Lesezirkel zu Gerald Weinbergs
_Psychology of Computer Programming_ ab. Das Buch ist ein wahrer Schatz, und zu
jedem Kapitel fallen jedem Teilnehmer verschiedenste bestätigende Beispiele ein.
Wir sind aber in einem halben Jahr nicht ganz durchgekommen. Ich freue mich
aber, den Lesezirkel 2023 weiterzuführen; auch mit einem weiteren Buch.

Sonst gibt es von 2022 nicht mehr viel zu berichten. Gelesen habe ich nicht sehr
viel. Das [Cryptonomicon](https://www.nealstephenson.com/cryptonomicon.html)
zieht sich seit Sommer hin. Wenigstens wäre nach dem Umzug meine Bibliothek
wieder einigermassen aufgeräumt. Den Kindle habe ich wieder aktiviert, mir
darauf aber v.a. leichte Lektüre zugeführt. Fremdsprachen waren kein Thema
dieses Jahr. Immerhin lese ich das meiste auf Englisch.

Zum Jahresende hat mir dann der [Adventskalender 2022 vom
Debianforum](https://wiki.debianforum.de/Adventskalender_2022) noch etwas Arbeit
und noch viel mehr Freude beschert. Zwei meiner «Türchen» kamen aus dem Umfeld
des Berufsschulunterrichts (Redis und S3/MinIO); die anderen beiden von der
Arbeit (`spiped`) bzw. aus der Freizeit (Huffman-Codierung).

Das Unterrichten mit Vor- und Nachbereitung hat in der zweiten Jahreshälfte
nicht nur zwei volle Tage, sondern auch den grössten Teil des Wochenendes
eingenommen. Im Frühling habe ich nur halbsoviele Lektionen, bei gleicher
Bezahlung. (Die Pensen werden auf das ganze Schuljahr hochgerechnet; d.h. ich
hatte jetzt ein Semester lang etwas über 120% gearbeitet und kann Ende Januar
auf ca. 80% reduzieren.) Das Gröbste habe ich überstanden, und 2023 dürfte es
etwas mehr Luft geben.

# Ideen für 2023

Was soll ich also mit der Zeit anfangen?

Bis Ende April stehen einige Servermigrationen an; d.h. nicht nur VMs, sondern
auch ein physischer im Serverraum. Weiter sollte ich mich mit
Container-Registries befassen und vielleicht Podman noch etwas genauer
anschauen. Kubernetes wäre auch wieder einmal interessant, zumal ich es bisher
v.a. aus der OpenShift-Perspektive kenne, und sich Kubernetes seit meiner
letzten Beschäftigung damit sicherlich weiterentwickelt hat. Auch mein
Python-Wissen sollte ich gelegentlich etwas vertiefen (z.B. mit asyncio oder der
ganzen Packaging-Thematik, ansonsten reizt mich daran derzeit wenig).

SICP wird mich sicherlich noch einige Monate beschäftigen, wenn nicht das ganze
Jahr. Ansonsten reizt mich derzeit so einiges: Elixir, Rust, Perl (kein
Tippfehler!), Clojure, Elm, Svelte und Racket. Dabei sehe ich im Moment bei
Elixir und dem Web-Framework Phoenix das grösste Potenzial. (Und wenn ich
konsequent sein will, sollte ich mir vorher einmal gründlich Erlang anschauen.)
Viele dieser Sprachen und Technologien (Perl ausgenommen) helfen dabei, Software
besser zu entwickeln, indem man auf unkontrollierte Seiteneffekte verzichtet.

Bei ben Betriebssystemen begnüge ich mich derzeit mit Linux. FreeBSD und OpenBSD
spielen bei mir derzeit (leider) keine Rolle. Mein Backup- und Dateiserver, auf
dem FreeBSD mit ZFS läuft, liegt immer noch in einer Umzugskiste. Im Frühling
soll ich zudem eine Linux-Einführung an der Berufsschule geben ‒ für die Lehrer,
nicht für die Schüler. Themen wie systemd, nftables und Netzwerke allgemein
sollte ich gelegentlich auch etwas vertiefen. Dazu kommen auch diverse
Cloud-Themen, u.a. auch Datenschutz und anbieterspezifische Themen.

Im zweiten Halbjahr steht dann die didaktische Grundausbildung an, die mich wohl
jeweils samstags beschäftigen wird. Geniesse ich also die grösstenteils freien
Wochenenden im ersten Halbjahr.
</content>
    </entry>
    <entry>
        <title>Towards Monads (in Go)</title>
        <link href="https://paedubucher.ch/articles/2022-11-05-towards-monads-in-go.html"/>
        <author>
            <name>Patrick Bucher</name>
        </author>
        <id>https://paedubucher.ch/articles/2022-11-05-towards-monads-in-go.html</id>
        <updated>2022-11-05T15:23:00Z</updated>
        <content type="text/markdown; charset=UTF-8">
&gt; As soon as you understand Monads, you're no longer able to explain Monads.

— Somebody on the Internet

Having read `n` tutorials on Monads, one must come to the conclusion that
tutorial `n+1` needs to be written. Not so much in order to explain monads to
others, but rather to gain more clarity. So here I write down my current
understanding of Monads, and my few readers get the chance to correct my
misconceptions about them. I'll be using example code in Go for this purpose,
because Haskell programmers won't bother to visit my site anyway.

# Basic Arithmetic in Go

Functors are about composing functions, Monads are about composing _lifting_
functions. For a start, we need some functions to compose. Everybody knows
function composition from basic arithmetic:

```
3 - 2 + 1 * 4
```

We have _numbers_ that are chained together by _operators_.

First, let's define a type alias for numbers:

```go
type Number float64
```

Second, we need a type for operators:

```go
type Operator func(Number) Number
```

But wait, why does the Operator only take a single number? Because we use
closures to implement them:

```go

func Adder(i Number) Operator {
	return func(n Number) Number {
		return n + i
	}
}

func Subtractor(i Number) Operator {
	return func(n Number) Number {
		return n - i
	}
}

func Multiplier(i Number) Operator {
	return func(n Number) Number {
		return n * i
	}
}

func Divider(i Number) Operator {
	return func(n Number) Number {
		return n / i // NOTE: divide by zero possible!
	}
}
```

An `Operator` function is defined as some operation to be done on another
number. For example, a `Multiplier` for the value `3` can be created. This
function then can be applied to another number, say, `2`, which then is
multiplied with `3` (`2 * 3 = 6`).

Notice how elegantly the `NOTE` comment in the function `Divider` points to a
possible problem to be resolved later on in the text. Yes: we could divide by
zero, which is the problem to be solved using Monads.

Here's how to use those operators:

```go
func main() {
	var n Number = 1
	addTwo := Adder(2)
	subOne := Subtractor(1)
	mulThree := Multiplier(3)
	divFour := Divider(4)

	// ((((1 + 2) * 3) - 1) / 4) = 2
	fmt.Println(divFour(subOne(mulThree(addTwo(n)))))
}
```

We start with `n=1`, add `2` to it (`=3`), multiply it with `3` (`=9`), subtract
`1` from it (`=8`), and divide it by `4`, which produces the result `2`.

# Composing Functions

Nested function can be tedious to read. If we'd like to re-use a formula, it's
better to _compose_ those functions, which is achieved by another function
called `Compose`:

```go
// f(g(n))
func Compose(f, g Operator) Operator {
	return func(n Number) Number {
		x := g(n)
		y := f(x)
		return y
	}
}
```

Two functions `f` and `g` can be composed as `f(g(x))`. The composition of those
functions is itself a function, let's call it `h`. Calling `h(x)` is the
equivalent of calling `f(g(x))`. (I call `g` the _inner_ and `f` the _outer_
function, and `f` is called with the result from the inner function call.)

Let's compose the functions frome before:

```go
func main() {
	var n Number = 1
	addTwo := Adder(2)
	subOne := Subtractor(1)
	mulThree := Multiplier(3)
	divFour := Divider(4)

	f := Compose(mulThree, addTwo)
	g := Compose(subOne, f)
	h := Compose(divFour, g)

	// ((((1 + 2) * 3) - 1) / 4) = 2
	fmt.Println(h(n))
}
```

Which produces the same result as before: `2`.

## Bad Composition

Let's mess up this example by introducing some nefarious operation: divide
something by zero (I'll be leaving out the boiler-plate `main` code from here):

```go
divZero := Divider(0)
f = Compose(mulThree, addTwo)
g = Compose(subOne, f)
h = Compose(divZero, g) // NOTE: this would break everything
fmt.Println(h(n))
```

This, of course, does not produce a numeric result, but `+Inf`. One could make a
mathematical argument on the correctness of this result (which I can't), or
refer to the [IEEE 754](https://standards.ieee.org/ieee/754/6210/) standard for
floating-point arithmetic (which I did), but further composition is clearly hurt
by such a result.

If the definition of `Number` is changed to `int32`, for example, the programm
will break:

```
panic: runtime error: integer divide by zero
```

Which definitively hurts composition.

So we need a safe way to divide numbers, which either produces a proper result
to be used for further computations (or as the final result), or some kind of an
error, so that further computations downstream won't be attempting to compute
with.

# Wrapping Results

A new type `Result` is created, which can represent the result of a successful
computation (`Val`) or the reason why this computation failed (`Err`):

```go
type Result struct {
	Val Number
	Err error
}
```

The operators are supposed to deal with such results. More precisely: The
operators still accept values of type `Number`, but produce values of type
`Result`:

```go
type LiftingOperator func(Number) Result
```

Those operators are supposed to _lift_ an ordinary `Number` into another
context: the `Result`.

A `Result` is used like a union in C: either `Val` is set (indicating a properly
performed computation), or `Err` is set (indicating some problem). The
implementations of the operations addition, subtraction, and multiplication are
straightforward:

```go
func LiftingAdder(i Number) LiftingOperator {
	return func(n Number) Result {
		return Result{n + i, nil}
	}
}

func LiftingSubtractor(i Number) LiftingOperator {
	return func(n Number) Result {
		return Result{n - i, nil}
	}
}

func LiftingMultiplier(i Number) LiftingOperator {
	return func(n Number) Result {
		return Result{n * i, nil}
	}
}
```

The result of the computation is wrapped in `Result` with no error and returned.

The division, however, might produce either a `Result` with an actual value
(`Val`), or a `Result` only consisting of an error (`Err`):

```go
func LiftingDivider(i Number) LiftingOperator {
	return func(n Number) Result {
		if i == 0 {
			return Result{0.0, errors.New(&quot;divide by zero&quot;)}
		}
		return Result{n / i, nil}
	}
}
```

## Composing Lifting Functions

Those lifting functions can be composed, too, but require a modified
implementation of the `Compose` function from before, called `ComposeLifting`:

```go
func ComposeLifting(f, g LiftingOperator) LiftingOperator {
	return func(n Number) Result {
		x := g(n)
		if x.Err != nil {
			return Result{0.0, fmt.Errorf(&quot;call inner function: %w&quot;, x.Err)}
		}
		y := f(x.Val)
		if y.Err != nil {
			return Result{0.0, fmt.Errorf(&quot;call outer function: %w&quot;, y.Err)}
		}
		return Result{y.Val, nil}
	}
}
```

If the result of the inner function `g` turns out be be erroneous, the outer
function `f` is never called, but the error returned from `g` is wrapped and
returned. The same is done for the result of the outer functon `f`. Only if both
`g` and `f` produce proper results (represented by a value) can the composed
function itself return a result with a value set.

Let's compose some _lifting_ functions:

```go
addLiftingTwo := LiftingAdder(2)
subLiftingOne := LiftingSubtractor(1)
mulLiftingThree := LiftingMultiplier(3)
divLiftingFour := LiftingDivider(4)

fl := ComposeLifting(mulLiftingThree, addLiftingTwo)
gl := ComposeLifting(subLiftingOne, fl)
hl := ComposeLifting(divLiftingFour, gl)
fmt.Println(hl(n))
```

This program produces the output `{2 &lt;nil&gt;}`, i.e. the result `2` was computed,
and no error (`&lt;nil&gt;`) occured.

Let's compose some computations including the nefarious division by zero:

```go
addLiftingTwo := LiftingAdder(2)
subLiftingOne := LiftingSubtractor(1)
mulLiftingThree := LiftingMultiplier(3)
divLiftingZero := LiftingDivider(0)

fl := ComposeLifting(mulLiftingThree, addLiftingTwo)
gl := ComposeLifting(subLiftingOne, fl)
hl := ComposeLifting(divLiftingZero, gl)
fmt.Println(hl(n))
```

Which produces the following output:

```
{0 call outer function: divide by zero}
```

Since there is an error, `0` is _not_ the result, but a placeholder. The error
message tells us pretty well what went wrong.

The function composition is also not hurt if we put the nefarious division by
zero in the middle:

```go
addLiftingTwo := LiftingAdder(2)
divLiftingZero := LiftingDivider(0)
subLiftingOne := LiftingSubtractor(1)
mulLiftingThree := LiftingMultiplier(3)

fl := ComposeLifting(divLiftingZero, addLiftingTwo)
gl := ComposeLifting(subLiftingOne, fl)
hl := ComposeLifting(mulLiftingThree, gl)
fmt.Println(hl(n))
```

However, a longer error message is produced:

```
{0 call inner function: call inner function: call outer function: divide by zero}
```

The code can be found on
[GitHub](https://gist.github.com/patrickbucher/70b8f1ebe91ea2f50cb829c7bad657a7).

# Conclusion

Functions operate on values and can be composed to new functions. Lifting
functions operate on values, too, but lift them in a context for some purpose
like error handling. This lifted context is called a Monad. Lifting functions
can be composed and make the aspect, for which the lifting happens, transparent
to the user of an API. Monads are about composing lifting functions.

Note: I didn't bother to mention [Monad
Laws](https://wiki.haskell.org/Monad_laws).

</content>
    </entry>
    <entry>
        <title>One Month of SICP</title>
        <link href="https://paedubucher.ch/articles/2022-08-30-one-month-of-sicp.html"/>
        <author>
            <name>Patrick Bucher</name>
        </author>
        <id>https://paedubucher.ch/articles/2022-08-30-one-month-of-sicp.html</id>
        <updated>2022-08-30T18:43:00Z</updated>
        <content type="text/markdown; charset=UTF-8">
When I finally got my bachelor's degree in Computer Science, I set another goal
for my further professional development: I wanted to learn functional
programming. I got a glimpse of Prolog and Racket during my studies, and used
higher-order functions with lambda expressions in Java and JavaScript once in a
while. I also peaked into Common LISP on multiple occasions, but I never made it
a priority.

# Erlang? No, Haskell and MOOCs!

Now I wanted to dig deeper, and Erlang was the language that interested me the
most. The main reason for this choice was the concurrency model based on actors
and messaging, and the Prolog syntax didn't bother me at all.

For some reason I got sidetracked and ended up learning a bit of Haskell. I
worked halfway through [Programming
Haskell](https://www.cambridge.org/ch/academic/subjects/computer-science/programming-languages-and-applied-logic/programming-haskell-2nd-edition),
which taught me a lot about functional programming. I skipped the second part;
probably because I got sidetracked again: I remember playing around with OpenBSD
and FreeBSD at that time. But I also spent quite some time with machine learning
in the fall and winter with Coursera MOOCs, and only got back to functional
programming when I worked through the Scala MOOC in early 2021.

I also started a MOOC called [Programming Languages Part
A](https://www.coursera.org/learn/programming-languages), which uses Standard ML
as a teaching language. This was the only MOOC I did _not_ finish—and the last
one I started to this day. I just had enough of learning with videos, it was
just too tiresome for me. So I decided to pick up a book; _the_ book: [Structure
and Interpretation of Computer
Programs](https://mitpress.mit.edu/9780262510875/structure-and-interpretation-of-computer-programs/).

# Elixir and Clojure

Unfortunately, I got stuck in the middle of the first chapter. Up to then, I
worked through all the exercises with great care. But the math probably was too
difficult for me, and I rather wanted to do something practical with functional
programming. So I left SICP unfinished and wanted to learn Erlang—finally—but
got sidetracked again: this time to its fancy little brother, Elixir.

I picked up a book—[Learn Functional Programming with
Elixir](https://pragprog.com/titles/cdc-elixir/learn-functional-programming-with-elixir/)—that
left out some of the best parts of Elixir, but had a lot of silly examples in
it. I quite liked Elixir, but the book killed my motivation. And learning some
LISP was still on my bucket list. Therefore, I tried another hosted language:
Clojure. I also had to learn some Clojure for my job in order to adjust the
[Riemann](http://riemann.io/) configuration being used to ingest metrics. I
worked through [Getting
Clojure](https://pragprog.com/titles/roclojure/getting-clojure/), which I
consider a great introduction to the language.

# Wasteful Meandering

I didn't have time for functional programming in spring/summer 2021, because I
was preparing lessons for a teaching side-job I got into on short notice.
However, I managed to work through a small eBook called [Functional Programming in
Python](https://leanpub.com/functionalprogramminginpython), whose lessons I can
apply in my day job—at least partially (no monads). Teaching was also what
occupied most of my time in fall and winter. So 2021 passed without much further
effort.

In the first half of 2022, I found some time again to improve my functional
programming skills. I picked up Elixir again; this time with the excellent book
[Elixir in
Action](https://www.manning.com/books/elixir-in-action-second-edition). However,
the book's focus is on concurrency and error handling in the second half, and I
felt that I first needed to sharpen my skills in functional programming
techniques before digging into the applications.

Back to Clojure I was, and this time I bought a lot of literature. [Clojure for
the Brave and True](https://www.braveclojure.com/) is quite witty and informal,
but focused too much on games for my taste. [Programming
Clojure](https://pragprog.com/titles/shcloj3/programming-clojure-third-edition/)
really was what I was looking for—but it was too fast-paced for me, and I
noticed that I wasn't really learning the mechanisms of the language, and I got
stuck once more.

Frustrated as I was, I went on a walk to overthink my meandering learning path
that lead me nowhere. After two years, I was able to write some of my stock
programs (Connect Four, computing league tables from game results) in Clojure
and Elixir under heavy googling, but that pretty much was it. Having turned 35
this summer, I realized that I should not waste more of my time like this. No
matter what programming language I am learning, I just need to stick with it in
order to make some real progress.

# Back to SICP

So I decided to go back to SICP, and I started completely from scratch. I
deleted my old repository from GitHub, and even from my backup server. On the
30th of July 2022, I pledged myself to work on SICP every day from now on; even
if I only read a single page or think about an exercise for five minutes. And
finally, I [managed to do
so](https://github.com/patrickbucher/sicp/blob/master/diary.md).

After a month, I'm quite far into chapter 1, which I want to finish in
September. The upcoming move to another place and my teaching duty certainly
will disrupt my streak in early October, but until then my «daily SICP» habit
will be strong enough to smoothly continue after being settled in my new home.

My plan is to at least work through chapters 1 and 2. Chapter 3, dealing with
subjects such as concurrency, streams and delayed evaluation, looks promising
for my further journey. Chapters 4 and 5 I might consider later; chapters 1-3
probably will be a strong-enough foundation to take Clojure or Elixir/Erlang
into consideration again. But for the rest of 2022, I'll focus on SICP.

## Lessons Learned

What lessons did I learn in the last 30 days?

First, hard exercises require pen and paper. The computer is just too
distracting, and formulas are way easier to understand drawn out on a sheet of
paper rather than mangled into a text file. Seeing something on paper, as
opposed to holding it in your mind, frees up resources for actually thinking
about it.

Second, if my mind goes blank when working on an exercise, then there are just
too many things I haven't understood yet. Probably I didn't fully comprehend the
examples earlier in the chapter. So back to the examples, back to pen and paper;
and only back to the exercise after I carefully worked through the examples.

Third, being unsure about my first step into a mathematical proof, I just cannot
find the willpower to step through it completely. I first need a little hint
whether or not my initial idea is valid. After looking at a single line of a
sample solution, I can find the confidence to work through the complete proof.

Fourth, thinking hard about a problem for a while, then leaving it for a day or
night, and going back to it in the evening or on the next day helps a lot. I
often got stuck in the wrong place and missed the actual problem. Getting out of
the problem and back in after a couple of hours gives you a new perspective. So
far, I was able to solve all the problems within 24 hours, but with multiple
attempts at it.
</content>
    </entry>
    <entry>
        <title>Bumbling Boomer Bob and the Clean Code Cult</title>
        <link href="https://paedubucher.ch/articles/2022-08-27-bumbling-boomer-bob-and-the-clean-code-cult.html"/>
        <author>
            <name>Patrick Bucher</name>
        </author>
        <id>https://paedubucher.ch/articles/2022-08-27-bumbling-boomer-bob-and-the-clean-code-cult.html</id>
        <updated>2022-08-27T10:00:00Z</updated>
        <content type="text/markdown; charset=UTF-8">
If you are around my age, work as a programmer, and took some classes on the
subject since 2009, you probably were subjected to Robert C. Martin's book
_Clean Code: A Handbook of Agile Software Craftsmanship_. (As of this day, the
[first
edition](https://www.informit.com/store/clean-code-a-handbook-of-agile-software-craftsmanship-9780132350884)
from 2008 hasn't been updated. Clearly, _Clean Code_ must be a—nay,
_the_—perfect book, which does not require improvements or adjustments after all
those years).

I bought the book out of personal interest, probably in 2009. As many others, I
first was fascinated by it, but never managed to read more than one or two
chapters. I also rarely bothered to read specific sections offering solutions to
problems I faced on a daily basis. Probably the solutions offered weren't too
helpful, even though I worked as a Java programmer during that time, and _Clean
Code_ is very much about the kind of Java we wrote back then. So _Clean Code_
collected dust on my bookshelf.

# Clean Code and the Clean Code Cult

When I studied computer science at the local technical college, I was exposed to
_Clean Code_ again. Or rather it was shoved down the students throats as a
gospel by disciples of the _Clean Code Cult_, as I like to call them.

The members of the _Clean Code Cult_ strengthen their belief by joining up for a
ritual called _Clean Code Shaming_, where they superficially look at a piece of
code they don't understand on first sight, and then just yell _«Clean Code!!!1»_
at its author in order to give proof of their superiority and sophistication.

Remember: Code you initially don't understand is _always_ just bad code and
certainly not a chance to improve your understanding of programming, especially
if pointless techniques like _Memoization_ or _Lexical Closures_ are used, i.e.
techniques you haven't been exposed to yet.

My friend [meillo](http://marmaro.de/) pointed out the [cult-like nature of
_Clean Code_](http://marmaro.de/apov/txt/2016-04-27_schaedlicher-kult.txt)
roughly at that time when its disciples came after me. A leader being called
«Uncle Bob», a scripture that doesn't require a second edition after many years
(but spawns sequels such as _The Clean Coder_, _Clean Architecture_, _Clean Agile_,
and _Clean Craftsmanship_), disciples willing to align themselves into
[grades](https://clean-code-developer.com/grades/) and wear
[bracelets](https://clean-code-developer.com/die-initiative/bracelets/) for self
castigation: If you still don't think that _Clean Code_ is a cult, just talk to
one of its disciples, point out a contradiction in the _Clean Code_ book, and
witness the angry reaction caused by your blasphemous remark.

But wait, a contradiction in _Clean Code_? That's impossible! Or maybe not?

# Bumbling Boomer Bob on Function Arguments

Working as a programmer for almost twenty years, I am still a layperson when it
comes to the exegesis of _Clean Code_, because I obviously still haven't been
englightened by this masterpiece yet. Trying to slay a straw man—and failing to
do so!—will hopefully bring me back to the Right Path, so that I can finally
abondon my wrongthink and give up on my hellish ends.

Let's hear what The Englightened has to say about function arguments (_Clean
Code_, Chapter 3, p. 40):

&gt; The ideal number of arguments for a function is zero (niladic). Next comes one
&gt; (monadic), followed closely by two (dyadic). Three arguments (triadic) should
&gt; be avoided where possible. More than three (polyadic) requires very special
&gt; justification—and then shouldn’t be used anyway.

This is misleading on so many levels that I need to dissect it in multiple
paragraphs.

Uncle Bob™ uses the terms _niladic_, _monadic_, _dyadic_, _triadic_, and
_polyadic_ for functions with arities of 0, 1, 2, 3, and n, respectively. There
certainly is a _qualitative_ difference betweeen 0, 1, and n (nothing,
something, and many things), the difference between, 2, 3, and n is only of a
_quantitative_ nature. But judging by the terms being used, the author sees a
_qualitative_ difference between all those arities, to wit (_Clean Code_, p.
42):

&gt; A function with two arguments is harder to understand than a monadic function.

And:

&gt; Functions that take three arguments are significantly harder to understand
&gt; than dyads.

This difference is clearly just of a _quantitative_, not of a _qualitative_
nature, because adding another argument only makes the function «harder to
understand», no matter if you go from one to two, or from two to three
arguments. You just move up one step on a continuum.

Having been exposed to Haskell for a couple of hours, I'd expect to read about
_Curried Functions_ here: functions of arity `n` that return a function of arity
`n-1` when being invoked with a single argument. But obviously those Haskell
guys must be stupid, because they also bother with _Partial Function
Application_, which only makes sense when you have multiple arguments, i.e.
_diadic_, _triadic_, or even—Bob forbid—_polyadic_ functions!

This must also the reason why
[SICP](https://mitpress.mit.edu/9780262510875/structure-and-interpretation-of-computer-programs/)
makes for such a bad introductory textbook, because the Professors Abelson and
Sussman clearly haven't read _Clean Code_ when coming up with this abomination
(_Structure and Interpretation of Computer Programs_, Exercise 1.32, Chapter 1,
p. 61):

    (accumulate combiner null-value term a next b)

Six arguments, are you kidding me? Polyadic _ad nauseam_! Bob hates it.

SICP is such a horrible book that it even required a second edition. It even was
[adapted](https://mitpress.mit.edu/9780262543231/structure-and-interpretation-of-computer-programs/)
from MIT Scheme to JavaScript recently, whereas _Clean Code_ clearly would
withstand such blasphemous attempts, being firmly grounded in the Java culture
of the mid-2000s.

(If you didn't figure out where my exegesis went from serious to sarcastic, stop
reading this text and just forget about it. Put on the _Clean Code_ bracelet of
the day and refactor that cryptic `this.x += 3;` statement to a _clean_
`increaseXByThree();` niladic method instead.)

But Bumbler Bob is here to help (_Clean Code_, p. 43):

&gt; When a function seems to need more than two or three arguments, it is likely
&gt; that some of those arguments ought to be wrapped into a class of their own.
&gt; (p. 43)

I wonder how many arguments a constructor for such a class might require.
Certainly, the introduction of the Builder Pattern would be The Right Solution™
for this issue. Much clearer than having a function with four arguments.
(Behold, the englightment is kicking in!)

## Niladic (Im)purity

Boomer Bob is clearly familiar with the concept of _Pure Functions_, otherwise
he wouldn't object so strongly against side effects (_Clean Code_, p. 44):

&gt; Side effects are lies. Your function promises to do one thing, but it also
&gt; does other hidden things. Sometimes it will make unexpected changes to the
&gt; variables of its own class. Sometimes it will make them to the parameters
&gt; passed into the function or to system globals. In either case they are devious
&gt; and damaging mistruths that often result in strange temporal couplings and
&gt; order dependencies.

Unless Bobby-O considers lies good, he clearly speaks out _against_ side effects
here, as he spoke out _in favour_ of functions without arguments before. So we
should all be writing side-effect free functions without arguments. But what can
such a function return?

- Nothing
- A constant value
- A random value

_Unless_ the function also operates on global variables or on the properties of
an object. But then those functions (or methods) are _not_ really side-effect
free, because their semantics is influenced by side-effects of other
functions/methods.

In order for a function to do something useful without side-effects, function
arguments are needed. The amount of arguments needed is determined by the
_domain_ of the function, i.e. by what the function is actually supposed to
do. Shall a function compute the solutions to a quadratic equation, the
arguments `a`, `b`, and `c` are needed. Shall a function draw an arc on a
canvas? You need to define the coordinates of the circle's center (`x` and `y`),
its radius (`r`), start and end angle, and whether or not the arc shall be drawn
clockwise or counter-clockwise.

### Complexity: Inherent and Accidental

Admittedly, _Clean Code_ offers some useful advice to make such APIs easier to
understand, e.g.:

- Instead of having two paramters `x` and `y`, a `Point` abstraction might come
  in handy. (see _Argument Objects_ on p. 43)
- Instead of passing flag arguments (clockwise/counter-clockwise), provide two
  functions. (see _Flag Arguments_, p. 41)

But Boomer Bob entirely misses the point: The difference between _inherent_ and
_accidental_ complexity. Solving a square equation _requires_ three arguments.
An arc is _defined_ by its mid-point, radius, angles, and curve orientation.
This is complexity _inherent_ to the problem at hand.

The thickness, colour, and opacity of an arc being drawn on a canvas all have to
do with a specific application of the concept. So while I consider it good
advice to wrap drawing details (thickness, colour, opacity) into an argument
object, or to use a `Point` abstraction instead of two loose `x` and `y`
arguments, there's no reasonable way to deal with arcs using _niladic_ or
_monadic_ functions, save for _Curried Functions_, which clearly aren't on
Babbling Bob's mind here.

You might also separate the computation of an arc from actually drawing it.
Here, the computation returns the coordinates to be drawn, which you can pass
into the `draw` method of the canvas object, maybe together with the drawing
details. A pair of _monadic_ methods for setting coordinates and drawing details
on that object won't make anything clearer, but only introduce more side
effects: _accidental_ side effects this time, which change the state of the
object without any palpable benefit. Calling the `draw` method to actually draw
on the canvas is the only _desired_ side-effect: the complexity introduced
thereby being of an _inherent_ nature.

### More Arguments Can Make For Better Abstractions

Another example: Consider a `reduce` function. (This is a higher-order function,
but obviously not at the height of _Clean Code_, for such concepts are not
mentioned in The Masterpiece.) Consider the following interfaces:

    reduce(combine, values)

and

    reduce(combine, values, initialValue)

Where `combine` is itself a function with the following interface:

    combine(accumulator, x)

The _dyadic_ `reduce` function must assume a value out of the given `values`
(usually its first element) as the initial value to be used for the
`accumulator`. So the _dyadic_ `reduce` can only return something of the same
type as the elements of `values` have. E.g. `values` is the integer array
`[1,2,3]`, and `combine` sums up the `accumulator` with the current value
argument `x`, then `reduce` must return an integer.

The _triadic_ `reduce` function can accept any initial value, as long as the
`combine` function is capable of dealing properly with that type of value. E.g.
`reduce` can be used to partition the integer array of `[1,2,3,4]` into two
arrays of odd and even numbers. The `initialValue` then could be a tuple of two
empty arrays: `([],[])`. Those arrays are filled by the `combine` function: odd
numbers in the first array, even numbers in the second array: `([1,3],[2,4])`.

Not only is a _triadic_ `reduce` function more powerful than the _dyadic_ one,
it is also more _general_, i.e. a higher abstraction. If you want to do repeated
modifications to a vector in Clojure, you need something like a triadic reduce
function! But I guess that _Clean Coder_ Bob figured this out in recent years,
judging by his [delight in
Clojure](https://blog.cleancoder.com/uncle-bob/2019/08/22/WhyClojure.html).

# Conclusion

I ranted away half of my Saturday morning on roughly half a page of Bob's
Timeless Wisdom. The problem is not that _Clean Code_ is a book with advice
that aged poorly, because it was written from a mid 2000s-Java perspective. The
problem is its uncritical fellowship taking this advice at face value, because
their perspective is too narrow.

I'm convinced that Robert C. Martin would write a totally different book on the
subject nowadays than he did back in 2008. But a second edition of _Clean Code_
would have very little in common with the first edition still being in print.
Rewriting the entire book probably would be less work than re-editing it. And
its fellowship would feel cheated by reading a book full of advice contrary to
its original.

If you feel offended by this text, please take an hour to watch Brian
Kernighan's lecture on [The Elements of Programming
Style](https://www.youtube.com/watch?v=8SUkrR7ZfTA) and let his advice sink in.
A former professor of mine once «improved» Kernighan's code from _The C
Programming Language_ (second edition, again…), for the reason you might guess:
_Clean Code!!!1_

So reconsider your habit of yelling _«Clean Code!!!1»_, _«Train Wreck!!!1»_,
or _«SOLID!!!1»_ (what does the «L» stand for, again?) at other programmers
without first having tried to understand their code and familiarized yourself
with the concepts being used therein. Try out a functional programming language
or two, e.g. Haskell and Scheme, and consider their up- and downsides compared
to, say, Java or C#. Then read _Clean Code_ again (or: _actually_ read it), but
with the grain of salt extracted from your recent encounters with different
ideas and concepts. Read it critically, not as a gospel, and you'll extract some
real value out of it: by carefully considering each advice and its proper area
of application—as limited as that might be.
</content>
    </entry>
    <entry>
        <title>Git with Multiple E-Mail Addresses</title>
        <link href="https://paedubucher.ch/articles/2022-07-26-git-with-multiple-e-mail-addresses.html"/>
        <author>
            <name>Patrick Bucher</name>
        </author>
        <id>https://paedubucher.ch/articles/2022-07-26-git-with-multiple-e-mail-addresses.html</id>
        <updated>2022-07-26T12:30:00Z</updated>
        <content type="text/markdown; charset=UTF-8">
# Different Git Server, Different E-Mail Address

When you use Git on a daily basis, chances are that you use it with multiple
remote servers—and do so with different email addresses. I personally use:

- My work email address (let's say `patrick@work.xy` for the sake of brevity)
  for my employer's Git server (`git.work.xy` to keep it short).
- My private email address (`patrick@home.xy`) for GitHub (`github.com`).
- The school's email address (`patrick@school.xy`) for the Git server used for
  teaching (`git.school.xy`).

Only using each Git server on a computer dedicated for some line of work
(company, private, school) is impractical for different reasons. I keep personal
notes on discoveries I make when working for my job or for school, which I store
in a private repository. Changing laptops just to write down that command line I
already googled seven times is not practical, and just would make me to google
it for the eighth time.

Furthermore, I prefer to work on my stationary PC running Arch Linux (which I
use as a daily driver, [by the
way](https://knowyourmeme.com/memes/btw-i-use-arch)) for school-related work,
especially when it comes to making up programming examples; I'm just less
efficient working on my Windows laptop.

# Two Problems, One Solution

So private, work- and school-related repositories will end up on the same
computer, which requires solving two problems:

1. How shall the repositories be organized on the file system?
2. How to make sure to commit using the right email address for every repository
   based on its area of origin (work, school, private)?

Many developers I know just use a folder `~/projects` or `~/repos`, wherein they
store _all_ of their repositories. This not only causes issues when storing
multiple repositories of the same name (e.g. different `dotfiles` or `meta`
repositories for each area, which can be solved using prefixes like `work-` or
`school-`), but also makes it harder to solve the second issue, as you'll see
shortly.

Therefore I organize my repository folders in a different way, a lot like those
Git servers's URLs are organized:

- `~/git.work.xy` for my work-related repositories,
- `~/github.com` for my private repositories, and
- `~/git.school.xy` for my school-related repositories.

I also use a second folder level, emulating the repositories actual URLs:

- `~/git.work.xy/[customer]` for the repositories of the same customer at work,
- `~/github.com/[username]` for the repositories of different users and
  organizations, and
- `~/git.school.xy/[course]` for the repositories of different courses I teach.

If you think that this messes up your home directory (following some Freedesktop
standard enforced by `xdg-user-dirs(1)` with folders like `~/Documents` and
`~/Videos`), feel free to add another level on top, such as `~/Repositories` or
`~/Projects`.

This leads to deeper file system hierarchies, but makes finding repositories
very easy and straightforward. It also helps solving the email address issue,
which I tackle using [conditional
includes](https://git-scm.com/docs/git-config#_conditional_includes) in my
`~/.gitconfig`.

# Conditionally Overwriting the E-Mail Address

For every computer I use, there's some main email address, e.g.
`patrick@work.xy` on my employer's laptop, or `patrick@home.xy` on my PC at
home. For the latter case, my `~/.gitconfig` starts as follows:

```
[user]
    name = Patrick Bucher
    email = patrick@home.xy
```

(I also use the `signinkey` option to sign my commits with the proper GPG key,
but using different ones for each E-Mail address is straightforward, so I won't
list those options here.)

What I need to do for every area of work is to overwrite my email address. So I
create additional Git config files in my home folder: `~/.gitconfig-work` and
`~/.gitconfig-school` (`~/.gitconfig` is for private GitHub repos on this
particular computer, i.e. the default shown above).

Then each config is included depending on its path from the `~/.gitconfig` file:

```
[includeIf &quot;gitdir:~/git.work.xy/&quot;]
    path = ~/.gitconfig-work
[includeIf &quot;gitdir:~/git.school.xy/&quot;]
    path = ~/.gitconfig-school
```

Those files referenced then just need to overwrite the `email` option of the
`[user]` section, e.g. for `~/.gitconfig-work`:

```
[user]
    email = patrick@work.xy
```

Or for `~/.gitconfig-school`:

```
[user]
    email = patrick@school.xy
```

## Another Problem, Same Solution

Now consider that your employer also has some repositories on GitHub
(`github.com/employer`). In this case, you can further overwrite your email
using an additional conditional include in `~/.gitconfig`:

```
[includeIf &quot;gitdir:~/github.com/employer/&quot;]
    path = ~/.gitconfig-work
```

This works as intended, as this demonstration shows:

```
$ cd ~/git.school.xy/cs-101/intro
$ git config user.email
patrick@school.xy

$ cd ~/git.work.xy/acme/config
$ git config user.email
patrick@work.xy

$ cd ~/github.com/patrick/dotfiles
$ git config user.email
patrick@home.xy

$ cd ~/github.com/employer/dotfiles
$ git config user.email
patrick@work.xy
```

I won't do any demo commits, proving my point; the `user.email` setting _will_
be used for each commit.

So you no longer have to remember running `git config user.email [wh@ev.er]`
after cloning a repo (which you will forget) or commit with the wrong E-Mail
address (which you'll regret).

</content>
    </entry>
    <entry>
        <title>«Fire and Forget» oder «Fork and Join»?</title>
        <link href="https://paedubucher.ch/articles/2022-04-30-fire-and-forget-oder-fork-and-join.html"/>
        <author>
            <name>Patrick Bucher</name>
        </author>
        <id>https://paedubucher.ch/articles/2022-04-30-fire-and-forget-oder-fork-and-join.html</id>
        <updated>2022-04-30T11:35:00Z</updated>
        <content type="text/markdown; charset=UTF-8">
In der nebenläufigen Programmierung ist _Fire and Forget_ ein Prinzip, bei dem
untergeordnete Aufgaben gestartet werden und anschliessend nicht mehr auf deren
Beendigung gewartet wird. Man kann davon ausgehen, dass der Task dann irgendwann
zu Ende sein wird, und das Ergebnis wird dann schon stimmen. Eine Beispiel
hierfür wäre eine Datensicherung, die von einem lang laufenden Server gestartet
wird. Sofern der Server nicht neu gestartet wird, dürfte der Vorgang dann schon
zu Ende laufen.

Beim Prinzip _Fork and Join_ wird ein Task (bzw. werden mehrere Tasks)
nebenläufig gestartet. Man wartet aber anschliessend auf ihr Ergebnis, um den
nächsten Verarbeitungsschritt erledigen zu können. Ein Beispiel hierfür wäre
etwa eine nebenläufige Sortierung eines grossen Arrays, das zunächst in
Unterarrays aufgeteilt (Fork) wird, und dessen sortierte Unterarrays dann
zusammengeführt werden (Join), wie etwa beim Merge Sort.

Beide Modelle haben ihre Berechtigung. Man sollte sich aber dessen bewusst sein,
welches Modell für eine gestellte Aufgabe das richtige ist. Und genau hier
beginnt das Problem: Nicht unbedingt ein technisches, sondern ein alltägliches:
Wie einfach ist es, mit einer neuen Sache zu beginnen, und wie schwierig ist es,
die Sache dann auch wirklich zu einem Abschluss zu bringen!

Egal ob _Fire and Forget_ oder _Fork and Join_: Es muss in beiden Fällen eine
(Unter)aufgabe gestartet werden. Beim Programmieren sind das Prozesse oder
Threads im weitesten Sinne. Im Alltag könnte das z.B. eine E-Mail sein, mit der
eine Diskussion eröffnet wird; die Zuweisung einer Aufgabe, deren Ausführung man
später kontrollieren sollte; das Anlesen eines Buches, dass dann irgendwo
herumliegt, und sich offenbar nicht von alleine fertigliest; das Erteilen einer
Hausaufgabe, für deren Kontrolle man dann doch keine Zeit hat.

Beim Programmieren ist das Prinzip _Fire and Forget_ wesentlich einfacher
umzusetzen als _Fork and Join_. Bei ersterem startet man einfach eine Aufgabe,
die dann nebenläufig oder gar parallel abläuft. Am einfachsten ist dies wohl mit
der Programmiersprache Go zu demonstrieren. Der synchrone Funktionsaufruf

    performLongRunningJob()

kann einfach durch Voranstellen des Schlüsselwort `go` in einen nebenläufigen
Funktionsaufruf verwandelt werden:

    go performLongRunningJob()

Schwieriger wird es bei Funktionen, an deren Ergebnis (bzw. Rückgabewert) man
interessiert ist. Der Funktionsaufruf

    result := performLongRunningJob()

lässt sich nicht so einfach nebenläufig ausführen; hier ist ein Channel zur
Kommunikation des Rückgabewertes nötig:

    resultChan := make(chan interface{})
    performLongRunningJob(resultChan)
    result := &lt;-resultChan

Hier dürfte das `async`/`await`-Muster anderer Programmiersprachen syntaktisch
einfacher sein, aber eine Erkennis bleibt: _Fork and Join_ ist anspruchsvoller
als _Fire and Forget_ ‒ und für viele Anwendungsfälle der einzig zielführende
Weg.

Unsere Zivilisation erlaubt uns vielerorts das unkompliziertere _Fire and
Forget_ zu betreiben. Den Kehricht werfen wir in einen Gebührensack und diesen
dann in einen Container. Der Hauswart kümmert sich darum, dass der Container am
Sammeltag am Strassenrand steht, und die Müllabfuhr holt dann alles ab. Dahinter
verbirgt sich eine gewaltige logistische Leistung, denke man nur an den Umfang
des Entsorgungskalenders, der einem einmal jährlich per Post zugestellt wird.
(Hierfür soll es ja mittlerweile Apps geben, welche die ganze Lieferkette noch
wesentlich komplizierer machen, zumal noch IT darin involviert ist.)

Der _Forget_-Teil ist hier aber oftmals subjektiver Natur; d.h. wir denken nicht
länger an die vorabends in den Müll geworfene Chipstüte. Die Müllabfuhr und die
Kehrichtverbrennungsanlage muss sich aber noch darum kümmern. Dort setzt das
Vergessen erst ein, wenn der Abfall verbrannt ist. Die dabei ausgestossenen Gase
und der sich an der Verbrennungsanlage absetzende Russ sind dann wiederum die zu
lösenden Probleme anderer Industriezweige (etwa von Zertifikatshändlern und
Kaminfegern).  Solange aber die Müllverbrennungsanlage ordnungsgemäss gewartet 
wird, braucht man auch hier nicht mehr an den konkreten Müll zu denken: _Fire
and Forget_.

Komplizierter ist die Sache bei der Entsorgung von Elektronikartikeln oder
Fahrzeugen. Sobald diese nicht mehr leistungsfähig genug sind oder nicht mehr
den strengen Regelwerken unserer fortgeschrittenen Zivilisation
(Sicherheitsanforderungen, Strassenverkehrsordnung) entsprechen, werden sie
nicht etwa entsorgt, sondern in einen anderen Weltteil mit weniger hohen
Leistungs- und Sicherheitsanforderungen exportiert. Elektronik wird teilweise in
offenen Feuern verbrannt, um an die Kupferkabel zu gelangen.  Fahrzeuge hingegen
werden teilweise noch jahrelang betrieben, bevor sie dann zwecks Metallgewinnung
auch zerlegt werden. Zwischen _Fire_ und _Forget_ können hier also Jahrzehnte
liegen; Gesundheitsschäden durch eingeatmete Verbrennungsgase können gar ein
Leben lang nachwirken, wenn nicht sogar auf Folgegenerationen.

Wir machen es uns also oft etwas zu leicht mit dem _Fire and Forget_-Ansatz, wo
wir wieder zurück beim Programmieren wären: Sollte ein asynchron gestarteter
Backup-Vorgang nicht vielleicht doch abgewartet werden, dass man Erfolg und
Misserfolg zum Schluss zurückmelden könnte? Wird dies ausgelassen, und wird der
lang laufende, übergeordnete Serverprozess (Elternprozess) häufig (etwa aufgrund
eines Memory Leaks) neu gestartet, dürfte der lange andauernde Backup-Vorgang
(Kindprozess) nicht erfolgreich ablaufen. Bemerkt wird dies erst dann, wenn man
durch Datenverlust zu einem Restore gezwungen wird: wann es also bereits schon
zu spät ist. Erscheint uns hier _Fire and Forget_ zunächst als legitim, kommen
wir bald zur Erkenntnis, dass _Fork and Join_ wohl doch die bessere, wenn auch
schwierigere Lösung gewesen wäre.

Mein wachsender Stapel angelesener Bücher und der weniger werdende, da durch
Elektronik eingenommene Platz in meiner Wohnung weisen mich immer öfters darauf
hin, dass ich es mir mit _Fire and Forget_ wohl etwas zu einfach mache.

Bei Elektronikartikeln haben wir vor vielen Jahren die beim Kauf zu entrichtende
vorgezogene Entsorgungsgebühr eingeführt. So kann man ausgediente Geräte einfach
zur Verkaufsstelle zurückbringen, die den Artikel dann der ordnungsgemässen
Entsorgung zuführen sollte: Brennende Haufen von Elektroschrott auf
afrikansichen Müllkippen sollten dann eigentlich nicht mehr von unserer
Elektronik befeuert werden. («Sollten», denn aus Sicht des Kunden haben wir es
hier ja mit _Fire and Forget_ zu tun; ich persönlich habe die
Elektronik-Entsorgungskette noch nie versucht nachzuverfolgen.)

Bei persönlichen Interaktionen, etwa beim Vorgesetztenverhalten am Arbeitsplatz
oder beim Lehrer-Schüler-Verhältnis in Schulen, lässt sich das Problem nicht so
einfach lösen: Vorgesetzter und Lehrer können nicht beim Erteilen der Aufgabe
eine vorgezogene Kontroll- und Korrekturgebühr (gemessen in Zeiteinheiten)
entrichten, wodurch dann dieser abschliessende Vorgang garantiert durchgeführt
wird. Stattdessen muss hier auf einem virtuellen Zeitkonto eine entsprechende
Ressource in der Zukunft reserviert werden, die dann nicht von einem anderen
Vorgang eingenommen werden darf. Wir müssen also mit dem Erteilen einer Aufgabe
auch immer ein Versprechen für die Zukunft machen, um deren Erledigung
kontrollieren zu können. Hält man sich nicht an dieses Versprechen, oder gibt
man es zu Beginn gar nicht ab, verwenden wir das bequemere _Fire and Forget_ wo,
wir stattdessen _Fork and Join_ machen müssten.

So wäre ich an der These dieses Artikels angekommen: Viele Probleme der modernen
Gesellschaft ‒ Umweltverschmutzung, Zeitnot, Führungsversagen, die oft zitierte
Bildungskatastrophe ‒ sind dadurch mitverschuldet, dass man sich zu oft _Fire
and Forget_ begnügt, wo man mit _Fork and Join_ arbeiten müsste. Etwas
überspitzt könnte man den Begriff _Verantwortung_ mit dem Prinzip _Fork and
Join_ gleichsetzen: Man löst nicht nur einen Vorgang aus, sondern kümmert sich
dann auch um dessen Folgen. Oder genauer: beim Auslösen eines Vorgangs ist man
sich nicht nur dessen bewusst, dass man zu einem späteren Zeitpunkt die Folgen
desselben zu bewältigen haben wird, sondern auch, dass man für verschiedene
eingetretene Szenarien einen Plan bereithalten muss. (Hat der Schüler die
Aufgabe zufriedenstellen gelöst, wird er dafür gelobt; andernfalls fordert man
ihn zur Nacharbeit auf: ein erneuter _Fork and Join_-Vorgang wird initiiert.)

Auch hier ist ein Blick auf die Softwareentwicklung erleuchtend, zumal
gewisse Programmiersprachen bei der Auswertung von Funktionsergebnissen ein
_erschöpfendes_, d.h. alle Fälle berücksichtigendes _Pattern Matching_
erfordern; bzw. man explizit angeben muss, dass man für gewisse Ausgänge keine
Reaktion vorgesehen hat, um so von Compilerwarnungen verschont zu bleiben. Als
Programmierer muss man somit über die Probleme nachdenken, bevor sie eingetreten
sind. Zurück in den Alltag übersetzt, könnte man sich also bei einer Handlung
folgende Fragen stellen:

1. Genügt das Prinzip _Fire and Forget_, oder benötige ich einen Plan, wie ich
   mit den Konsequenzen meiner Handlung umgehen muss (_Fork and Join_)?
2. Ist mein _Pattern Matching_ erschöpfend, oder habe ich auf jetzt schon
   erwartbare Ausgänge meiner Handlung gar keinen Plan?

Die Welt ist kein Computer und die Menschen sind keine Prozesse. Dennoch bin ich
gespannt, ob diese Analogie bei der Analyse ersterer hilfreich sein wird.
</content>
    </entry>
    <entry>
        <title>Weltverbesserung und Denkfaulheit</title>
        <link href="https://paedubucher.ch/articles/2022-04-13-weltverbesserung-und-denkfaulheit.html"/>
        <author>
            <name>Patrick Bucher</name>
        </author>
        <id>https://paedubucher.ch/articles/2022-04-13-weltverbesserung-und-denkfaulheit.html</id>
        <updated>2022-04-13T19:08:00Z</updated>
        <content type="text/markdown; charset=UTF-8">
&gt; Explanations exist; they have existed for all time; there is always a
&gt; well-known solution to every human problem — neat, plausible, and wrong.

‒ H. L. Mencken

Im Geografieunterricht am Gynmasium kann ich mich an zwei Lehrer erinnern: Der
erste war eher ein Haudegen; hatte ein enormes Wissen, versteckte dieses aber
teilweise hinter seiner jovialen Art. Bei ihm im Unterricht haben wir
angeschaut, was die Fakten sind. In New York herscht _dieses_ und in den Tropen
_jenes_ Klima. Das eine Klima eignet sich gut für den Maisanbau, im anderen
betreibt man besser extensive Viehzucht. Der Aralsee trocknet aus, weil die
Abflüsse und die Verdunstung grösser sind als die Zuflüsse. Auf diese Weise
wurden aber nicht nur Fakten gelernt, sondern auch Zusammenhänge, z.B. zwischen
Baumwoll- und Fischereiindustrie in der Region: beide sind auf die gleiche
Ressource angewiesen, nutzen sie aber mit einem unterschiedlichen Zeithorizont.
(Die Akteure haben einen unterschiedlichen
[_Buxton_-Index](https://amateurearthling.org/2009/04/03/dijkstra-the-buxton-index-and-the-prisoners-dilemma/).)

Der zweite Geografielehrer war jünger und hatte einen ganz anderen
Unterrichtsstil. Der Frontalunterricht wechselte sich mit vielen Einschüben mit
Gruppen- und Partnerarbeiten ab. Ich habe das immer gehasst, weil diese
ständige Umkonfiguration des Schulzimmers für sehr viel Unruhe sorgte. Man war
mehr mit Tischerücken und Organisation beschäftigt als mit der Sache an sich.
Doch auch die Inhalte wurden anders vermittelt: Statt zu erfahren, was Sache
ist, wurden Vorschläge erarbeitet, wie man etwas verbessern könnte.

Zum Beispiel: Der 37jährige Maniokbauer Otoktok Tschabobo aus Tschibuti hat
Probleme seine Waren auf dem lokalen Markt abzusetzen, weil sie dort preislich
nicht mit den in industrieller Landwirtschaft erzeugten Produkten mithalten
können. Die industrielle Landwirtschaft wird subventioniert, und Herr Tschabobo
erhält nichts davon. Nun war es an den Schülern, Lösungen auszuarbeiten. ‒ Die
Probleme des Welthandels, die von den Mächtigen und Gutausgebildeten nicht
gelöst werden können oder wollen, sollen also von 15jährigen Schülern gelöst
werden: Manch einer (oder vielmehr: eine) scheint diesen Auftrag wohl etwas _zu_
ernst zu nehmen.

Die Vorschläge lauteten immer ähnlich: Importzölle, Subventionen, Verbote, usw.
Der Aralsee trocknet aus? Baumwollanbau verbieten, Subventionen für weniger
wasserintensive Anbauprodukte sprechen, Gesetze gegen übermässige Wasserentnahme
verabschieden, Swimming-Pools gleich ganz verbieten. Eine Ferienregion wird mit
hässlichen Hotelkomplexen verschandelt? Strafsteuern für das nicht
Wünschenswerte, Subventionen für das Wünschenswerte, Verbote, Infokampagnen,
eine Kommission einsetzen ‒ so lauteten meistens die Lösungen.

Die Generation, die als erste in den zweifelhaften Genuss dieses Unterrichts
kam, der nicht auf die persönliche Bildung, sondern auf die Weltverbesserung
abzielte, gelangt nun langsam selber an die Schalthebel der Macht. So erinnern
mich die politischen Nachrichten immer öfters an den Geografieunterricht von
damals: Zu wenig  Frauen im Verwaltungsrat (was auch immer das heissen möge)?
Strafsteuern für Firmen mit «zu» tiefer Frauenquote, Subventionen für Firmen mit
erfüllter Frauenquote.  Diskrikimierungsverbote.  Eine Kommission wird
eingesetzt, die die Missstände untersuchen soll. Dann noch eine Infokampagne für
mehr «Sichtbarkeit» und _awareness_, was auch immer das bedeuten möge. Ein
verregneter Sommer? Elektromobilität subventionieren, Benziner abschaffen,
Ölheizungen verbieten, Fleisch verteuern.  Nicht gleich jetzt, aber vielleicht
bis ins Jahr Zweitausendbisdannbinichnichtmehrinderverantwortung.

Es wird auf naivste Art in komplexe Systeme eingegriffen. Und mit «komplex»
meine ich nicht einfach «kompliziert», «vielschichtig» oder «gross». Ein System
ist komplex, wenn in diesem nicht-lineare Rückkoppelungseffekte (_non-linear
feedback loops_) auftreten, also selbstverstärkende Effekte. Solche naiven
Interventionen können ebenfalls zu solchen Effekten führen, nur dummerweise oft
nicht zu den gewünschten, sondern zu teils noch viel schlimmeren und
ungewünschten.

Beispiel Elektroauto: Der süffisante CTO lobt sich gerade vor versammelter
Meute seiner Unterlinge: Er sei heute mit dem Tesla zur Arbeit gefahren, denn
ihm sei es wichtig, dass seine Kinder in einer Welt aufwachsen, in der alle
Menschen genug Ressourcen zum Leben haben. (Ich Umweltsünder habe keinen Tesla
und keine Kinder und bin zu Fuss zur Arbeit gekommen.) Elektroautos sollen ja
schliesslich den Klimawandel stoppen ‒ oder gar rückgängig machen? Man
verbrennt ja schliesslich kein Benzin mehr. Und der Strom kommt ja von der
Sonne, denn der CTO hat ja schliesslich auch Solarpanels auf seinem
Einfamilienhaus, womit er die Welt mit jedem Sonnenstrahl aufs Neue rettet.
(Ich Umweltsünder habe keine Solarpanels, da meine Zweizimmermietwohnung gar
kein eigenes Dach hat. Stattdessen ziehe ich bewässerungsintensive Petersilie
auf meinem Balkon!)

Doch woher kommt die Batterie für den Tesla? Wie viel Energie braucht die
Herstellung des leichten Karbons für die Karosserie, womit das hohe Gewicht der
Batterie ausgeglichen werden soll? Und woher kommt der Strom tatsächlich?
Nein, lieber CTO, wenn du deinen Tesla nachts an dein Hausnetz anschliesst,
ist es nicht die Sonne, die deine Elektroautobatterien lädt. (Bedeutet «CTO»
eigentlich _Compulsive Tesla Owner_?)

Dennoch: Elektroautos gehören gefördert, Benziner verboten! In der Stadt soll
es kostenlose e-Scooter geben, die dann nachts mit dieselbetriebenen Lieferwagen
eingesammelt werden müssen. Die Geografieklasse von 2001 macht ja jetzt
schliesslich die Gesetze, und die haben ja schliesslich ihr ganzes Schulleben
lang gelernt, wie man die Probleme dieser Welt löst. Der Geografielehrer hat
die Lösungen damals schliesslich für gut befunden, und der muss es ja wissen,
beschäftigte er sich doch sein ganzes Berufsleben lang mit der Weltverbesserung
und der Unterweisung darin.

Was haben solche gut gemeinten «Lösungen» nun für unerwünschte nicht-lineare
Effekte? Diese sind oft nicht so einfach vorherzusagen und äussern sich erst
nach vielen Jahren, wenn ein entsprechender und oft irreversibler Schaden schon
angerichtet ist. Von der Gewinnung der Rohstoffe, die für Elektroauto-Batterien
benötigt werden, lässt sich jetzt schon nichts Gutes vernehmen. Dass neben dem
bestehenden Tankstellennetz noch eine Ladeinfrastruktur für Elektroautos
errichtet werden muss, ist ein weiterer sekundärer Effekt. Die Probleme mit
fehlenden Parkplätzen und verstopften Strassen werden durch Elektroautos
jedenfalls nicht gelöst.

Das übergeordnete Problem ‒ der Individualverkehr, ja die Idee des beruflichen
Pendelns überhaupt ‒ wird dabei nicht betrachtet. Zu sehr ist man auf konkrete
Technologien fokussiert. Mit Technologie lässt sich schliesslich auch Geld
verdienen.

Und was könnte man stattdessen tun, also jetzt angefangen im Geografieunterricht?

Statt sich gleich auf Lösungen zu stürzen, könnte man sich zunächst einmal
eingängiger mit den Problemen befassen. _Systemdenken_ ist das Stichwort.
Welche Rückkoppelungseffekte existieren in einem bestehenden System? Wie wirken
sich Interventionen auf diese Rückkoppelungseffekte aus? Wird dadurch der
bestehende Effekt geschwächt, oder unwillentlich sogar noch verstärkt? Welche
unerwünschten sekundären Effekte kann eine Intervention zur Folge haben?

Wenn beispielsweise angekündigt wird, dass die Wasserentnahme für den Aralsee
ab nächstem Jahr streng limitiert wird, dürfte das genau das Gegenteil der
erwünschten Wirkung zur Folge haben: Vor lauter Torschlusspanik wird nun noch
mehr Wasser entnommen, das dann versucht wird zu horten. Man hat genau das
Gegenteil des erwünschten Effekts erreicht.

Wenn Diesel- und Benzinautos ab Jahr X nicht mehr zugelassen werden, dürfte
deren Absatz nur noch steigen und schliesslich im Jahr X-1 einen (vorerst)
letzten Höhepunkt erreichen. Die Auftragsbücher sind übervoll, die Nachfrage
nach entsprechenden Bauteilen bei den Zulieferern steigt noch einige Zeit an,
sodass diese ihre Kapazitäten ausbauen und sich mit Rohstoffen eindecken. Tritt
schliesslich das Jahr X ein, fällt die ganze Nachfrage in sich zusammen. (Wir
haben es hier mit langen Lieferketten und also wiederum mit komplexen Systemen
zu tun: Der Hersteller von Kupferkabeln weiss wohl am Ende gar nicht, ob seine
Erzeugnisse dereinst in Autos mit Verbrennungs- oder Elektromotoren landen
sollen.) Die Hersteller weiter hinten in der Lieferkette sitzen nun auf ihren
Lager- und Rohstoffbeständen fest und wissen nicht, was sie mit den ganzen
ausgebauten Kapazitäten anfangen sollen. Der Markt ist nun für einige Jahre
ausgetrocknet.

Es folgt die naive Gegenintervention: Die Gesetzesänderung wird einfach wieder
rückgängig gemacht, sodass die Maschinerie weiterhin am Laufen bleibt und im
Leerlauf dreht. Oder aber die ganze Wertschöpfungskette wird mit Subventionen
und Ausgleichszahlungen eingedeckt, da die ganzen Probleme ja auf eine
Gesetzesänderung einer früheren Regierung zurückzuführen sind, von der die
Nachfolgeadministration schliesslich wenig hält.

Man soll sich also nicht fragen, welche _positiven_ Auswirkungen eine
Intervention haben könnte, sondern welche _negativen_. Vor allem sollte man
aber betrachten, ob diese negativen Folgen überschaubar sind und in Proportion
zu den erwünschten positiven Effekten stehen, oder aber ob man die Möglichkeit
für selbstverstärkende (unerwünschte) Rückkoplungseffekte erkennen kann. Eine
Intervention ist also nicht anhand ihres potentiellen Nutzens zu bewerten,
sondern anhand ihres möglichen Schadens.

Führt man dieses Vorgehen nun in den Schulunterricht zurück, sollten weniger
vorgefertigte und allen bereits bekannte Lösungen diskutiert werden.
Stattdessen soll die Vorstellungskraft traniert werden, indem man die möglichen
Auswirkungen einer Intervention auf ein System durchzudenken versucht. Das ist
intellektuell fordernder und verlangt das Erkennen von Zusammenhängen. Das
Wiederkäuen von bereits bekannten Lösungen, wie man sie etwa aus der Tagespresse
vernimmt, ist natürlich weniger anstrengend. Zudem wähnt man sich mit den
zitierfähigen Expertenmeinungen aus den Massenmedien argumentativ auf der
sicheren Seite; hierzu muss man deren Argumente nicht einmal verstehen können,
man braucht sie nur nachzuplappern. (Der Vorwurf, dass einer nicht an die
Wissenschaft _glaube_ bzw. ihr nicht _vertraue_, ist hier besonders entlarvend.)

Nur wer bereit ist, diese Bequemlichkeit zu opfern, und sich stattdessen im
_problemorientierten_ Denken zu üben, kann schliesslich langfristige und
tragbare Lösungen finden, die sich nicht später bloss als zusätzliche
Folgeprobleme äussern.
</content>
    </entry>
    <entry>
        <title>Ideen für 2022</title>
        <link href="https://paedubucher.ch/articles/2021-12-31-ideen-fuer-2022.html"/>
        <author>
            <name>Patrick Bucher</name>
        </author>
        <id>https://paedubucher.ch/articles/2021-12-31-ideen-fuer-2022.html</id>
        <updated>2021-12-31T13:15:00Z</updated>
        <content type="text/markdown; charset=UTF-8">
Bald ist das Jahr zu Ende, und ich habe in den letzten Wochen und Monaten
verschiedenste Ideen gehabt, womit ich mich 2022 beschäftigen könnte. Einige
Ideen habe ich bald wieder fallen lassen, z.B. diese Webseite nach
[Hugo](https://gohugo.io/) zu migrieren (denn die Sache wird damit nicht
einfacher, eher komplizierter). Andere Ideen haben sich immer wieder von neuem
aufgedrängt, doch konnte ich sie aus Zeitmangel nicht weiterverfolgen.

Ein Mangel an Zeit ist ist doch meistens nur eine Prioritätsfrage: Wenn etwas
wichtig genug ist, wird man es schon angehen. Eine Stunde früher schlafen gehen,
dafür eine Stunde früher aufstehen; und die erste (d.h. beste) Stunde des Tages
auf etwas verwenden, das einem wirklich wichtig ist. Und das ist für mich 2022
folgendes:

# Elixir

2021 habe ich mich ursprünglich mit [Erlang](https://www.erlang.org/)
beschäftigen wollen. Ich habe das nur sehr kurz getan, da mich dann
[Elixir](https://elixir-lang.org/) mehr überzeugt hat. Auch damit konnte ich
mich nur kurz diesen Frühling beschäftigen, da sich die Vorbereitung für meine
Unterrichtstätigkeit dann vorgedrängt hat.

In den letzten beiden Wochen habe ich mir dann wieder Elixir angeschaut und mir
ein paar entsprechende [Notizen](https://github.com/patrickbucher/elixir-basics)
dazu gemacht. Ich komme langsam voran, aber lerne Elixir dadurch gründlicher.

Als nächstes möchte ich ein kleines Beispielprojekt umsetzen. Hierfür nehme ich
ein Problem, das ich schon mehrfach und in verschiedenen Programmiersprachen
gelöst habe, und versuche es einmal idiomatisch mit Elixir zu lösen.

Anschliessend geht es weiter mit Concurrency. Wenn ich das einmal verstanden
habe, möchte ich meine Reversi-Simulation
[revergo](https://github.com/patrickbucher/revergo) mit Elixir und OTP nicht nur
nebenläufig, sondern parallel-verteilt ausprogrammieren. Mal schauen, ob ich mit
mehreren Rechnern die Performance von Go auf einem Rechner übertrumpfen kann.

Dann möchte ich den Fokus auf Web-Anwendungen ausrichten. Das
[Phoenix-Framework](https://www.phoenixframework.org/) sieht für mich
vielversprechend aus.

# Go

Die Einfachheit und konzeptionelle Schönheit von Go begeistert mich immer wieder
aufs Neue. Im Februar soll Go 1.18 erscheinen und parametrische Polymorphie
(Generics) mitbringen. Das ist eine gute Gelegenheit, mich wieder einmal mit Go
zu befassen.

Dann sollte ich mich einmal mit Cloud-Anwendungen befassen, und was Go hier zu
bieten hat. Das Buch [Cloud Native
Go](https://www.oreilly.com/library/view/cloud-native-go/9781492076322/) steht
schon länger in meinem Bücherregal und soll mir zur Vorbereitung auf das
entsprechende Modul in der Berufsschule dienen. (Vielleicht sollte ich vorher
das kürzere Buch [Cloud
Native](https://www.oreilly.com/library/view/cloud-native/9781492053811/) einmal
durcharbeiten.)

Mein Ziel ist es, mit Go Anwendungen als [12 Factor Apps](https://12factor.net/)
entwickeln zu können. Das hat nur zu einem kleinen Teil mit der
Programmiersprache selber zu tun, und vielmehr mit der ganzen Umgebung.

Für ein privates Projekt wollte ich schon länger eine Web-Schnittstelle
anbieten, damit ich die Verwaltung der Benutzereinstellungen nicht von Hand in
einer Quellcodedatei nachtragen muss. Hierfür sollte ich mir gelegentlich
[gin](https://github.com/gin-gonic/gin) anschauen.

# JavaScript

An JavaScript kommt man nicht mehr vorbei; und funktionales JavaScript ist doch
etwas recht schönes. Für das Web-Modul, das ich diesen Frühling unterrichten
soll, habe ich hier noch einiges vorzubereiten.

Neben den beiden Büchern [Eloquent JavaScript](https://eloquentjavascript.net/)
und [Composing Software](https://leanpub.com/composingsoftware) wäre vielleicht
[Deno](https://deno.land/) noch interessant, zumal dies viele Unzulänglichkeiten
von Node.js beseitigen soll. Für den Berufsschulunterricht schwebt mir hingegen
[Express.js](https://expressjs.com/de/) vor, um die Funktionsweise von
RESTful-APIs zu demonstrieren und um damit entsprechende Übungen zu machen.

TypeScript interessiert mich hingegen wenig. JavaScript ist eine dynamische
Programmiersprache; dementsprechend möchte ich mich auch lieber mit dem
entsprechenden Paradigma beschäftigen als wie bei Java und C# aus allem eine
Klasse zu machen.

---

Das dürfte genug sein für 2022. Wenn ich in einem Jahr aus dem Stegreif mit
Elixir programmieren, mit Go 12-Factor-Anwendungen entwickeln und mit JavaScript
RESTful-APIs umsetzen kann, dann wäre mein Jahr 2022 in technischer Hinsicht
gelungen.

Was sonst 2022 noch alles passieren wird, darauf habe ich wohl nur wenig
Einfluss. Konzentriere ich mich also darauf, was ich kontrollieren kann: Stilles
Lernen in meinen eigenen vier Wänden; notfalls bei Kerzenlicht.
</content>
    </entry>
    <entry>
        <title>Jahresrückblick 2021</title>
        <link href="https://paedubucher.ch/articles/2021-12-17-jahresrueckblick-2021.html"/>
        <author>
            <name>Patrick Bucher</name>
        </author>
        <id>https://paedubucher.ch/articles/2021-12-17-jahresrueckblick-2021.html</id>
        <updated>2021-12-17T13:00:00Z</updated>
        <content type="text/markdown; charset=UTF-8">
Selten war meine Laune am Jahresende so schlecht. Dass ich mitte Dezember
erschöpft bin und mich nach einer Pause sehne, ist nichts Neues und war zu
erwarten. Leider habe ich aber auch das Gefühl, dass dieses Jahr für mich extrem
unproduktiv war, und ich stehenbleibe, ja gar Rückschritte mache ‒ in allen
möglichen Lebensbereichen.

Ich möchte hier auf mein Jahr zurückblicken, soweit es mein Erinnerungsvermögen
erlaubt, um diesen Eindruck zu prüfen.

Was Fremdsprachen und Literatur betrifft, ist der Rückblick schnell abgehandelt.
Französisch und Russisch habe ich kaum verwendet; Englisch höchstens für die
Informatik.

An Literatur kann ich mich dieses Jahr kaum erinnern. Ayn Rands _Anthem_ habe
ich auf dem Kindle gelesen. In den Sommerferien in Arosa habe ich Michel
Houellebecqs _Unterwerfung_ aus einer Bibliotheksbox gezogen und dann in dieser
Woche gleich durchgelesen, ich musste es ja schliesslich vor meiner Abreise
wieder zurücklegen. Gontscharews _Oblomow_, den ich im Gepäck hatte, blieb dafür
ungelesen. Viele andere Romane habe ich _an-_ aber nicht _durchgelesen_.  Den
Kindle habe ich schliesslich wieder in den Keller verbannt, da ich damit nur
oberflächlich lese.

Was meinen Beruf und die Informatik angeht, weiss ich doch etwas mehr zu
berichten.

# Motiviert mit MOOCs

Ich weiss nicht, ob es Ende 2020 oder Anfang 2021 war, als ich den Kurs [Machine
Learning](https://www.coursera.org/learn/machine-learning?) abgeschlossen habe,
aber mit der [Machine Learning
Specialization](https://www.coursera.org/specializations/deep-learning) habe ich
wohl erst 2021 angefangen. Hier bin ich schnell vorangekommen und war lange Zeit
motiviert ‒ bis es schliesslich in einem Mooc zum Framework
[TensorFlow](https://www.tensorflow.org/) kam, das immer noch in der Version 1.x
verwendet worden war. Der Umstieg auf TensorFlow 2.x muss wohl ein grösseres
Problem sein, zumindest ist er mir nicht einmal für die Kursbeispiele gelungen.

Immerhin habe ich selber noch einige Beispiele entwickelt:
[x-o-classifier](https://github.com/patrickbucher/x-o-classifier) zur optischen
Unterscheidung der Zeichen `x` und `o`,
[digit-detection](https://github.com/patrickbucher/digit-detection) als
erweitertes OCR-Beispiel für die Ziffern von 0 bis 9, die [Titanic
Challenge](https://github.com/patrickbucher/titanic-challenge) und die [House
Prices](https://github.com/patrickbucher/house-prices) von Kaggle (wenn auch
mit mässigem Erfolg). Weiter sind mir kleinere Beispiele und eine
Zusammenfassung des ersten Kurses in meinem
[machine-learning-Repository](https://github.com/patrickbucher/machine-learning)
geblieben.

Für mich war das Thema Machine Learning dann vorerst erledigt. Ein vorbestelltes
und endlich im November eingetroffenes Buch ‒ [Math for Deep
Learning](https://nostarch.com/math-deep-learning) ‒ erinnerte mich an meine
verflogene Motivation.

# Funktionale Programmierung

Weiter ging es mit der funktionalen Programmierung, wozu ich den recht
gelungenen
[Scala-Kurs](https://www.coursera.org/learn/scala-functional-programming) von
Martin Odersky durchgearbeitet habe. Den meiner Meinung nach am besten
aufgebauten Kurs zur funktionalen Programmierung, [Programming Languages, Part
A](https://www.coursera.org/learn/programming-languages), konnte ich leider
nicht abschliessen: das ständige Lernen von Videos hat mich zu sehr erschöpft,
und ich wollte in meiner Freizeit weg vom Bildschirm.

So machte ich mich an
[SICP](https://mitpress.mit.edu/sites/default/files/sicp/full-text/book/book.html)
heran, worin ich leider im ersten Kapitel steckengeblieben bin. Immerhin habe
ich die Übungen bis dahin [seriös
gelöst](https://github.com/patrickbucher/sicp/tree/master/ch01), sodass ich hier
bei späterer Gelegenheit wieder einsteigen kann.

Eigentlich wollte ich mich 2021 mit Erlang befassen, bin aber dann aus
pragmatischen Gründen doch auf [Elixir](https://elixir-lang.org/) eingeschwenkt.
Ich versuchte das Buch [Learn Functional Programming with
Elixir](https://pragprog.com/titles/cdc-elixir/learn-functional-programming-with-elixir/)
durchzuarbeiten, gab aber auf Seite 129 (von 198) auf, wovon der Kommentar ganz
unten in meinen
[Notizen](https://github.com/patrickbucher/learning-elixir/blob/master/learn-functional-programming-with-elixir/notes.md)
zeugt. Die Beispiele waren mir wohl einfach zu kindisch und praxisfern. Ich
versuchte es dann noch mit [Elixir in
Action](https://www.manning.com/books/elixir-in-action-second-edition), doch
hatte ich im Frühling dann andere Prioritäten.

Im Sommer arbeitete ich noch ein kleines eBook zur funktionalen Programmierung
in Python durch: [Functional Programming in
Python](https://leanpub.com/functionalprogramminginpython), wozu ich auch gleich
meine [persönliche
Zusammenfassung](https://github.com/patrickbucher/docs/blob/master/python/funcprog.md)
mit eigenen Beispielen schrieb. Einiges davon konnte ich dann gewinnbringend im
Arbeitsalltag einsetzen; Monaden gehören jedoch nicht dazu…

# Arbeit

In der Firma habe ich mich die ersten vier Monate v.a. mit der Migration von
Servern von Ubuntu 16.04 auf 20.04 befasst. U.a. musste ich einen Mailserver und
den ganzen Monitoring-Stack migrieren, was wohl das Schwierigste an der Aufgabe
war.

Für die Ingestion der Metriken via [Riemann](http://riemann.io/) musste ich, um
von Python 2 wegzukommen, von einer bestehenden Datenbank (deren Name ich
bereits vergessen habe) auf [InfluxDB](https://www.influxdata.com/) wechseln.
Hierzu durfte ich sogar etwas Clojure lernen, wobei mir [Getting
Clojure](https://pragprog.com/titles/roclojure/getting-clojure/) von Russ Olsen
geholfen hat. Ich benötigte nur ca. zwei Kapitel. Im Frühling arbeitete ich das
Buch dann in meiner Freizeit komplett durch, wovon mein Repository
[learning-clojure](https://github.com/patrickbucher/learning-clojure/) zeugt.

Insgesamt migrierte ich ca. 20 Server mit verschiedensten Anwendungen und
Technologien. Von Mailservern verstehe ich praktisch immer noch so wenig wie
zuvor; da es sich hier aber um etablierte Software handelt (Sendmail, Dovecot),
war die Umstellung nicht allzu schwierig und auch ohne tieferes Wissen zu
bewältigen.

Lehrreich war auch die Ablösung von [Bitbucket](https://bitbucket.org/product),
das ja seit Herbst nur noch in der Cloud und nicht mehr _on-premise_ läuft,
durch [Gitea](https://gitea.io/en-us/) ‒ eine deutlich schlankere Lösung, die in
[Go](https://go.dev) geschrieben ist.

Das wichtigste Projekt war jedoch der
[Flugzeugkonfigurator](https://www.seantis.ch/success-stories/aircraft-configurator-pilatus/),
mit dem man neu auch die Innenausstattung von Flugzeugen (PC-12 und PC-24)
konfigurieren kann. Obwohl mir die Programmierung grafischer Oberflächen
überhaupt nicht liegt, konnte ich auf Basis erstklassiger Renderings, die mir
vom Kunden geliefert worden sind, doch ein recht ansprechendes Ergebnis
erzielen.

Das Projekt hat mich ab Mai praktisch bis Ende Jahr beschäftigt und war äusserst
vielfältig: Neben Parsern in [PLY](https://www.dabeaz.com/ply/) ‒ und
entsprechenden Interpretern, wobei mein bescheidenes Wissen in LISP doch recht
hilfreich war ‒ habe ich viel Test-Driven Development mit
[PyTest](https://docs.pytest.org/) und einiges an Verarbeitung von Excel-Dateien
mit [OpenPyXL](https://openpyxl.readthedocs.io/en/stable/) gemacht. Auch die
PDF-Generierung mit [ReportLab](https://www.reportlab.com/opensource/) spielte
dabei eine wichtige Rolle, wobei solche Aufgaben wegen der manuellen Testerei
und Feinjustierung oft zermürbend sind. Viele Konstrukte erforderten rekursive
Verarbeitung; solche Programmieraufgaben machen mir immer am meisten Freude.

Den Frontend-Code, wozu ich Ende letzten Jahres v.a. JavaScript verwendet hatte,
konnte ich dieses Jahr reduzieren, indem ich viel Funktionalität ins Backend
verschob. Das war ein Erfolgserlebnis, zumal die Codebasis heute viel besser
aussieht als vor einem Jahr, obwohl sie geschätzt um die Hälfte angewachsen ist.
Auch die Testabdeckung liegt mittlerweile bei über 90%, wobei ich den
JavaScript-Code im Frontend nicht mitzähle. Hier habe ich leider noch keine
vernünftige und einfache Möglichkeit gefunden, den Code automatisch zu testen.

# Lehrlingsausbildung und Berufsschule

Seit diesem Sommer bin ich in der Firma verantwortlich für die
Lehrlingsausbildung. Ich durfte bereits von Januar bis August einen Praktikanten
betreuen und so erste Erfahrungen auf diesem Gebiet sammeln. Die Aufgabe,
jemandem das Programmieren beizubringen, der noch keine oder nur sehr wenig
Erfahrungen auf diesem Gebiet hat, ist sehr interessant ‒ und sehr fordernd.
Hier experimentiere ich mit verschiedenen Ansätzen, wobei ich nach fast einem
halben Jahr mehr offene Fragen als Antworten habe.

Im Frühling hat sich dann per Zufall eine Nebenbeschäftigung als
Berufsschullehrer ergeben: An einer Informationsveranstaltung für die neue
Bildungsverordnung der Informatiklehre wurde ich von meinem früheren
Klassenlehrer in der Berufsschule, der jetzt dort als Fachbereichsleiter in der
Informatik-Abteilung tätig ist, auf offene Lehrerstellen hingewiesen. Da hier
auch 20%-Pensen angeboten worden sind, und beruflich nur zu 80% tätig war,
ergriff ich diese Chance sofort.

So unterrichte ich seit Ende August ein Modul mit der Bezeichnung «Software mit
agilen Methoden entwickeln». Ich habe drei (recht unterschiedliche) Klassen zu
je zwei Wochenlektionen. Diese Aufgabe hat meine zweite Jahreshälfte stark
geprägt. Dies einerseits, weil ich einen grossen Teil meiner Freizeit für die
Vor- und Nachbereitung des Unterrichts verwende; andererseits, weil der Inhalt
dieses Moduls mich beschäftigte:

- Die Berufsschule ist Microsoft-Territorium. So bin ich nicht darum herum
  gekommen, mich seit 2005 das erste mal wieder mit C# zu beschäftigen.
  Besonders interessant finde ich die Programmiersprache nicht; ich sehe sie
  eher als eine Art _Microsoft Java_. Positiv überrascht hat mich jedoch an den
  aktuellen Versionen von .NET (5 und 6), dass man damit mittlerweile sehr gut
  unabhängig von Betriebssystem und IDE entwickeln kann. So darf C# in meinem
  Programmiersprachenrucksack gerne einen kleinen Platz einnehmen.
- Für die Unterrichtsgestaltung war für mich Git das A und O. Ich baute einen
  privaten Gitea-Server auf, dessen API mir bei der Administration von drei
  Klassen mit insgesamt 52 Schülern sehr stark behilflich war. Ich konnte einige
  Wissenlücken zum Thema Git schliessen und habe einige Unterlagen dazu
  erarbeitet, welche ich gelegentlich noch veröffentlichen möchte.
- Damit ich die gleichen Themen nicht immer wieder aufs neue erklären musste,
  habe ich einige Screencasts aufgenommen und diese auf einem eigens dafür
  geschaffenen
  [YouTube-Kanal](https://www.youtube.com/channel/UCPq4iLFbolH2deHLMVKha1A)
  veröffentlicht. Mit [OBS Studio](https://obsproject.com/) geht das mit wenig
  Aufwand, wobei ich auf einen Videoschnitt und besondere Effekte verzichte. In
  der Regel gelingen mir die Videos im ersten Versuch in einer einigermassen
  akzeptablen Qualität. Die Audioqualität meines Headsets war mir jedoch
  eindeutig zu schlecht, zumal meine Aussprache nicht sehr deutlich ist und ich
  mich beim Sprechen oft etwas überschlage. So habe ich mir den [Rode
  Podcaster](https://www.rode.com/microphones/podcaster) gegönnt, womit man doch
  sehr gute Ergebnisse erzielt. Ich trenne die Tonspur jeweils mit
  [`ffmpeg`](https://ffmpeg.org/) vom Video, importiere sie in
  [Audacity](https://www.audacityteam.org/) und mache sie mit dem
  Kompressor-Filter lauter. Diese Tonspur führe ich dann wieder mit dem Video
  zusammen, wiederum mit `ffmpeg`. Das ist die einzige Nachbereitung, die ich an
  meinen Videos vornehme.
- Um mich einmal etwas gründlicher mit dem Thema «Agile» (ich hasse die
  Substantivierung dieses Begriffs) zu befassen, habe ich [Clean
  Agile](https://www.informit.com/store/clean-agile-back-to-basics-9780135781869)
  von Robert C. Martin gelesen. (Der Autor geht mir mit seiner Boomer-Attitüde ‒
  *«If I achieve a test coverage of 95% in my hobby project, then you must be
  able to do so in the projects of your your stressful day-to-day job…»* ‒
  zwar ziemlich auf die Nerven, hat aber einen recht pragmatischen und
  vernünftigen Zugang zum Thema und predigt nicht einen einzelnen Ansatz als den
  einzig Wahren.) Hierzu habe ich auch eine sehr ausführliche
  [Zusammenfassung](https://github.com/patrickbucher/docs/blob/master/clean-agile/clean-agile.pdf)
  geschrieben und für den Unterricht noch auf Deutsch
  [übersetzt](https://github.com/patrickbucher/docs/blob/master/clean-agile/clean-agile-de.pdf).
  Mit der Zusammenfassung bin ich sehr zufrieden, mit der Übersetzung etwas
  weniger; doch sie erfüllt ihre Aufgabe, wobei mich die Lesefaulheit einiger
  Schüler doch etwas überrascht hat.

# Pen and Paper

Beim Zusammenfassen von _Clean Agile_ wollte ich sorgfältig vorgehen und habe
darum wieder einmal mit Tinte und Papier gearbeitet. So habe ich jeweils ein
Kapitel zuerst einmal komplett durchgelesen, und dann die einzelnen Unterkapitel
handschrifltich auf wenige Seiten zusammengefasst. Diese habe ich dann am
Computer eingetippt und daraus ein PDF generiert.

Ich habe dieses Vorgehen gewählt, da ich bei der Lektüre von Gerald Weinbergs
[How Software Is Built](https://leanpub.com/howsoftwareisbuilt/c/99Qmmhd5KHxg)
schlechte Erfahrungen machen musste: So habe ich das eBook am Computer gelesen
und gleich _on the fly_ zusammengefasst. Leider ist dabei eher ein
[Notizkonvolut](https://github.com/patrickbucher/docs/blob/master/weinberg/how-software-is-built.md)
als eine gut lesbare Zusammenfassung von Weinbergs Gedanken entstanden.

So habe ich mich entschieden, in Zukunft öfters den entschleunigten Ansatz mit
Tinte und Papier zu verfolgen. Hierzu habe ich mir auch zwei eher günstige
Füllfederhalter gekauft, mit denen ich einigermassen gerne schrieb, jedoch nicht
komplett zufrieden war. Durch das Lesen einiger von Dijkstras handschrifltich
verfasster [EWDs](https://www.cs.utexas.edu/users/EWD/) konnte ich mich dann
doch zur Anschaffung eines _Montblanc Meisterstück 149_ durchringen: bereut habe
ich es nicht, obwohl ich leider in der zweiten Jahreshälfte bedeutend weniger
zum Schreiben gekommen bin, als ich das gewollt hätte.

# Parerga und Paralipomena

Endlich habe ich mich im Frühling/Sommer einmal gründlich mit TLS befasst: Das
eBook [TLS Mastery](https://www.tiltedwindmillpress.com/product/tls/), das ich
auch sponserte, lag endlich bereit. Ich habe es sehr gründlich durchgearbeitet
und eine
[Zusammenfassung](https://github.com/patrickbucher/docs/blob/master/tls/tls-mastery.pdf)
mit weiteren Beispielen und Anhängen verfasst. Geschrieben habe ich sie wiederum
auf Papier, wobei ich beim Eintippen dann ebensoviel Arbeit durch das
Ausprobieren der Beispiele hatte. Jetzt fühle ich mich nicht nur mit TLS
einigermassen sattelfest, sondern verstehe auch ACME (Let's Encrypt) und habe
zum ersten mal einen kleinen DNS-Server aufgesetzt.

Leider war dann für mich in diesem Jahr was Systemadministration betrifft so
ziemlich die Luft raus. Mit [FreeBSD](https://www.freebsd.org/) und
[OpenBSD](https://www.openbsd.org/) habe ich mich dieses Jahr kaum beschäftigt.
Zwar leistet mein Datei- und Backupserver basierend auf billigster Hardware und
FreeBSD mit ZFS gute Dienste, daran angepasst habe ich jedoch wenig. Immerhin
laufen jetzt darauf verschiedenste Backup-Scripts, unter anderem
[back-my-git-up](https://github.com/patrickbucher/back-my-git-up), wozu ich die
[Suitable-Library](https://github.com/seantis/suitable) meines Arbeitgebers
verwenden konnte.

Mit [Radicale](https://radicale.org/v3.html) habe ich der Firma einen
CalDAV-Server zur Verfügung gestellt. Das ging so einfach, dass ich jetzt eine
private Instanz davon betreibe. So habe ich zum ersten mal in meinem Leben meine
Termine zentral abgelegt, ja sogar mit dem Smartphone synchronisiert. Auch
dieser Kalender wird von meinem Backup-Server gesichert, und zwar per `rsync`.

Somit habe ich eine produktive FreeBSD-Installation im Einsatz, die ich dieses
Jahr auf den Release 13.0 aktualisiert habe. OpenBSD habe ich leider derzeit gar
nicht mehr im Einsatz, was ich demnächst wieder ändern möchte. Habe ich im
Frühling noch auf OpenBSD Elixir gelernt, läuft bei mir mittlerweile wieder
überall Linux und Windows, letzteres wegen meiner Tätigkeit als
Berufsschullehrer. Ich werde meinen über fünfjährigen Desktop wohl Anfang
nächstes Jahr durch ein bereits angeschafftes Gerät von Lenovo ersetzen, da die
USB-Anschlüsse mittlerweile zu viele Probleme machen. Die Hardware kann ich aber
noch sehr gut als OpenBSD-Heimserver verwenden, denn CPU, Memory und die SSD
sind für Experimente durchaus noch gut genug.

Überhaupt sehne ich mich wieder etwas mehr nach Askese in der Informatik: Neben
OpenBSD drängt sich bei mir auch Go wieder etwas in den Vordergrund, zumal die
Einführung von Generics eine erneute Beschäftigung mit der Sprache für mich
nötig macht.  Ausserdem möchte ich Go vielleicht in der Berufsschule für das
kommende Cloud-Modul verwenden, das ich wohl rund um die Idee der [Twelve-Factor
App](https://12factor.net/) konzipieren werde. Da sich
[meillo](http://marmaro.de/) neustens auch mit Go befasst, ergibt sich hier ein
recht interessanten Austausch für mich.

Für eine systematische Auseinandersetzung mit [Puppet](https://puppet.com/)
konnte ich mich dieses Jahr leider nicht motivieren. Auch Themen wie Container
und die Cloud sind bei mir dieses Jahr liegengeblieben. Die geplante
LPIC-Zertifizierung ist an meiner oben bereits erwähnten Müdigkeit im
Zusammenhang mit Videokursen dieses Jahr gescheitert.

Auch für eine gründliche Auseinandersetzung
[SQLAlchemy](https://www.sqlalchemy.org/) konnte ich mich dieses Jahr nicht
motivieren. Dennoch komme ich mittlerweile einigermassen damit zurecht und kann
auch immer mehr explizite Queries durch die Nutzung der ORM-Features loswerden.
Hierdurch verstärkt sich meine ambivalente Haltung gegenüber ORMs: Zum ersten
mal siehe ich ihren positiven Nutzen; dennoch graut mir vor der ganzen
Komplexität, die dadurch erzeugt und damit versteckt wird.

Ansonsten habe ich mich noch kurz mit PHP und JavaScript befasst, dies in erster
Linie für meine Unterrichtstätigkeit im nächsten Frühling, wo ich ein
Praxismodul zum Thema Web-Entwicklung betreuen werde. Mit PHP habe ich innerlich
abgeschlossen und werde wohl eher die Idee von Full-Stack-JavaScript, d.h. nicht
nur im Browser, sondern auch serverseitig mit [Node.js](https://nodejs.org/en/)
oder [Deno](https://deno.land/) verfolgen.

Beinahe hätte ich es vergessen: 2021 habe ich endlich einmal
[plan9](https://9p.io/plan9/) ausprobiert, und zwar auf dem [Raspberry Pi
400](https://www.raspberrypi.com/products/raspberry-pi-400/). Viel ist nicht
daraus geworden, aber die Erfahrung möchte ich nicht missen: Diese Kombination
von Hardware und Betriebssystem versprüht etwas vom Pioniergeist der 80er-Jahre,
den ich nicht miterleben konnte.

In diesem Zusammenhang fallen mir noch einige Bücher ein, die ich in der zweiten
Jahreshälfte gelesen habe:
[Hackers](https://www.oreilly.com/library/view/hackers/9781449390259/), [Free as
in Freedom](https://www.oreilly.com/openbook/freedom/) und [Masters of
Doom](https://doomwiki.org/wiki/Masters_of_Doom). Diese Werke haben gemeinsam,
dass Durchbrüche und Höchstleistungen im Bereich der Programmierung der
körperlichen und sozialen Verwahrlosung dieser Hackertypen gegenübergestellt
werden. Diese Texte waren so für mich Motivation und Warnung zugleich. Mehrmals
wurde in diesem Kontext [Joseph
Weizenbaum](https://en.wikipedia.org/wiki/Joseph_Weizenbaum) zitiert, mit dessen
Werk ich mich auch erneut befassen möchte.

# Ausblick

Beruflich möchte ich mich 2022 einmal etwas gründlicher mit
[Ansible](https://www.ansible.com/) auseinandersetzen. Was Python betrifft,
sollte ich mich nicht nur mit der Sprache, sondern auch mit dem Ökosystem etwas
genauer befassen, z.B. mit dem Packaging und mit PyTest.

In der Berufsschule werde ich mich im ersten Halbjahr v.a. mit Web-Entwicklung,
d.h. mit JavaScript oder TypeScript auseinandersetzen müssen. An
[Express](https://expressjs.com/) habe ich recht gute Erinnerung. Bei der
funktionalen Programmierug mit JavaScript werde ich mich wohl etwas zurückhalten
müssen, sind doch Berufsschüler recht stark von der objektorientierten
Programmierung geprägt. Im zweiten Halbjahr soll dann das Cloud-Modul
stattfinden, wofür mir Go und [Heroku](https://www.heroku.com/) vorschwebt.

Privat möchte ich mich einmal gründlich mit Elixir befassen. Für mich ist das
die aussichtsreichste Plattform, um später einmal damit produktive Anwendungen
entwickeln zu können. Hier interessieren mich nicht nur die technischen Ideen ‒
funktionale und verteilte Programmierung, das Actor-Modell ‒ sondern auch
Frameworks wie [Phoenix](https://www.phoenixframework.org/) und
[Ecto](https://hexdocs.pm/ecto/Ecto.html). Ich bin gespannt, ob diese beiden
Technologien meine Vorurteile gegen Web-Frameworks und ORMs widerlegen können.
Wer weiss; vielleicht zündet die Beschäftigung damit bei mir endlich den Funken
um später meine vagen Pläne für die Selbständigkeit befeuern zu können.

SICP werde ich wohl auch 2022 nicht durcharbeiten können. Gerne hätte ich mich
einmal mit Knuths Werken (_The Art of Computer Programming_, _Concrete
Mathematics_) auseinandergesetzt; doch das muss wohl noch ein paar Jahre auf
meinem Büchergestellt reifen.

Mit Fremdsprachen sieht es auch im nächstn Jahr eher düster aus. Reisen dürfte
wohl immer noch zu umständlich oder zu schlecht planbar sein. Aber an ein
französisches oder russisches Buch könnte ich mich wieder einmal heranwagen.

Überhaupt möchte ich meine Bildschirmzeit etwas besser beschränken, zumal mich
immer häufiger Spannungskopfschmerzen plagen. Den Kindle habe ich immerhin
wieder durch gedruckte Bücher ersetzt. Das ist oftmals weniger komfortabel, aber
doch zielführender.

Auch sollte ich mich nächstes Jahr mehr mit didaktischen und pädagogischen
Fragen befassen, obwohl mir hierzu keine konkreten Themen oder Werke einfallen.
(Neben mir liegt Ivan Illichs [Entschulung der
Gesellschaft](https://www.perlentaucher.de/buch/ivan-illich/entschulung-der-gesellschaft.html),
das mich eher vom Thema weg- als darin einführt.)

An Büchern hat sich mittlerweile hier soviel angesammelt, dass ich mich eher mit
dem Verschenken und Entsorgen als mit dem Anschaffen beschäftigen sollte. Doch
auch dieses Problem werde ich 2022 nicht lösen können.

Sollte sich ab Frühling 2022 nicht mehr alle paar Tage ändern, was man unter
welchen Umständen noch tun darf, möchte ich wieder einmal den Ausstieg aus dem
News-Hamsterrad wagen; idealerweise für immer, damit ich endlich die Ruhe und
Zeit habe, um wieder einmal gründlich lesen zu können. Ja: _Konzentration_ wäre
ein guter Vorsatz fürs neue Jahr, wieder einmal…

# Fazit

Ein anstrengendes, arbeitsintensives und lehrreiches Jahr neigt sich dem Ende
zu. Wie ich schon im Untertitel vermutete, war 2021 dann doch nicht so schlecht,
wie mich das meine derzeitige Laune glauben lassen macht. Von daher hat sich das
Herunterschreiben meiner diesjährigen Leistungen (und das Ausbleiben
ebensolcher) doch gelohnt.

Die Zombieapokalypse ist ausgeblieben, ich und die Leute in meinem Umfeld sind
grösstenteils bei guter Gesundheit und in den Supermärkten erhält man immer noch
geniessbare Nahrungsmittel zu einigermassen stabilen Preisen. Das reduzierte
gesellschaftliche Leben lässt mir Zeit zum Lesen, Lernen und Spazieren. Ich
sollte diese Zeit nutzen; das ist das Beste, was ich tun kann.

# Nachtrag (21.12.2021)

Langsam lichtet sich der Schatten; einige Sachen sind bei meiner Aufstellung
vergessen gegangen:

- Ich habe mich kurz mit der Programmiersprache [Io](https://iolanguage.org/)
  befasst, um in der Berufsschule eine Lücke in der Jahresplanung schliessen zu
  können. Immerhin hat sich ein Schüler über die neue Perspektive auf OOP
  gefreut ‒ und darüber, dass schon sehr grosse Geister Kritik an OOP geübt
  haben. Ein paar Notizen und Übungen dazu kann ich gelegentlich
  veröffentlichen.
- Für die Bewertung einer kleinen Projektarbeit komme ich kaum um Excel herum.
  Ich möchte aber den Schülern ein Dokument mit der Bewertung abgeben können. So
  habe ich mir mit Python, OpenPyXL,
  [Jinja-Templates](https://jinja.palletsprojects.com/en/3.0.x/) und
  [Pandoc](https://pandoc.org/) ein kleines Werkzeug namens
  [gradedocs](https://github.com/patrickbucher/gradedocs) gebaut, das ich gerade
  erfolgreich einsetzen konnte.
- Nach der Lektüre von _Hackers_ habe ich endlich mal das [Game of
  Life](https://en.wikipedia.org/wiki/Conway%27s_Game_of_Life) in JavaScript
  ausprogrammiert. Man kann es unter
  [paeedubucher.ch/gol](https://paedubucher.ch/gol/) ausprobieren
  ([Code](https://github.com/patrickbucher/js-game-of-life) auf GitHub).
- Vor Jahren habe ich einmal bei Nassim Taleb gelesen, dass man Pi berechnen
  könne, indem man Pfeile auf ein Quadrat werfe, und dann schaue, welche
  innerhalb eines davon eingelassenen Kreises mit maximalem Durchmesser landen.
  Nun habe ich das einmal selber ausprogrammiert
  ([paedubucher.ch/simpi](https://paedubucher.ch/simpi/),
  [Code](https://github.com/patrickbucher/simulate-pi) auf GitHub). Es scheint
  tatsächlich einigermassen zu funktionieren.
- Ich wollte mich einmal umschauen, ob es für Go auch ein paar vernünftige
  Bibliotheken zur PDF-Generierung gibt. Ich bin dann auf
  [gopdf](https://github.com/signintech/gopdf) gestossen und habe daraus einen
  kleinen Flash-Card-Generator namens
  [verzettler](https://github.com/patrickbucher/verzettler) geschrieben. Das
  ganze ist ein Proof-of-Concept; mit langen Strings funktioniert es (noch)
  nicht; aber 2022 muss ich ja auch noch etwas zu tun haben…
- Obwohl es mir länger vorkommt, war es diesen Februar, als ich ein paar
  Routinen zum extrahieren von Daten aus dem DOM-Tree zu einer Go-Library
  verpackt habe: [htmlsqueeze](https://github.com/patrickbucher/htmlsqueeze).
  Ich setze das persönlich für ein Projekt ein. Die Weiterentwicklung könnte
  jedoch recht kompliziert werden, sofern ich noch weitere CSS-Selektoren
  (korrekt) umsetzen möchte.

Unproduktiv war das Jahr also nicht.
</content>
    </entry>
    <entry>
        <title>Vom präzisen Sprachgebrauch</title>
        <link href="https://paedubucher.ch/articles/2021-05-09-vom-praezisen-sprachgebrauch.html"/>
        <author>
            <name>Patrick Bucher</name>
        </author>
        <id>https://paedubucher.ch/articles/2021-05-09-vom-praezisen-sprachgebrauch.html</id>
        <updated>2021-05-09T12:30:00Z</updated>
        <content type="text/markdown; charset=UTF-8">
&gt; Besides a mathematical inclination, an exceptionally good mastery of one's
&gt; native tongue is the most vital asset of a competent programmer.

— _Edsger W. Dijkstra_, [EWD 498](https://www.cs.utexas.edu/users/EWD/transcriptions/EWD04xx/EWD498.html)

Sprache ist für mich sehr wichtig, und zwar nicht _obwohl_, sondern _weil_ ich
Informatiker bin. Obwohl ich ohne grössere Probleme in den mathematischen
Fächern durch das Studium gekommen bin, sind meine Fähigkeiten in diesem Fach
nicht besonders stark ausgeprägt. (Meine Mathematiklehrer vom Gynmanisum können
dies sicherlich bestätigen.)

Fremdsprachen lerne ich zwar auch nicht besonders schnell, aber nach längerer
Beschäftigung mit einer Sprache kann ich in dieser doch recht präzise und
flüssig kommunizieren. Noch wichtiger finde ich, dass man sich der Unterschiede
zwischen verschiedenen Sprachen bewusst wird. Denn wortweise Übersetzungen sind
nicht nur stylistisch schlecht, sondern oft schlichtweg falsch. So lässt sich
dieser Satz aus dem Deutschen:

&gt; Ich glaube, ich spinne.

nicht eins zu eins ins Englische übersetzen:

&gt; I believe, I spider.

Dies gilt auch für Programmiersprachen; auch diese sollten _idiomatisch_
gebraucht werden: Was in der Programmiersprache C korrekt und idiomatisch sein
mag:

    for (int i = 0; i &lt; n; i++) {
        sum += values[i];
    }

Sollte beispielsweise in Clojure ganz anders aussehen:

    (reduce + values)

Von jemandem, der bei Sätzen wie dem folgenden nicht leicht mit den Augen rollen
muss:

&gt; Es macht Sinn hier ausserhalb der Box zu denken und eine Extrameile zu gehen,
&gt; damit wir das Produkt rechtzeitig ausrollen können.

Erwarte ich auch keine stilistischen Meisterleistungen im Programmcode, gerade
wenn mehrere Programmiersprachen in einem Projekt zum Einsatz kommen.
(Mangelndes Verständnis für Sprachidiome dürfte wohl auch der Grund sein, dass
JavaScript nach vielen Jahren doch noch ein `class`-Schlüsselwort erhalten hat.)

Doch braucht man gar nicht die stilistische Ebene und die Übersetzungen zwischen
verschiedenen Idiomen heranzuziehen um sprachliche Ungereimtheiten zu finden.
Oft hapert es schon beim Gebrauch einzelner Wörter:

&gt; Das läuft auf einem _physikalischen_ Server, nicht auf einem virtuellen.

Physik ist die Lehre der Körper, und «physikalisch» bezeichnet etwas _die Lehre
der Körper betreffendes_. Das Gegenteil eines virtuellen Servers ist ein
_physischer_ Server: ein Server, der _als Körper_ vorhanden ist. Zu einem
«physikalischen» Server könnte man sich vielleicht einen chemischen oder
biologischen Server als Gegensatz vorstellen. (Ein _physikalischer_ Server wäre
einer, auf dem physikalische Berechnungen durchgeführt werden.) Hier dürfte es
sich nicht einmal um ein Übersetzungsproblem aus dem Englischen («physical»)
handeln, ist doch der _physician_ ein Arzt (der sich mit menschlichen Körpern
beschäftigt) und der _physicist_ ein Physiker (der sich mit der Lehre von
Körpern befasst) ‒ Achtung: Verwechslungsgefahr!

In englischsprachigen Vorträgen wird oft ‒ zu oft! ‒ das Verb «to consume»
verwendet:

&gt; We provide services and libraries that you can _consume_.

Das Wort «konsumieren» (denn dieses Wort lässt sich auf Englisch und Deutsch
gleich verwenden) bedeutet, dass etwas _verbraucht_ wird. In der Informatik ist
dies etwa im Zusammenhang mit Messaging (korrekterweise) zu lesen:

&gt; The producer sends out messages, which are _consumed_ by multiple workers.

Die Nachrichten _müssen_ konsumiert werden, ansonsten würden sie mehrfach
verarbeitet. Einen Dienst (engl. service) oder eine Programmbibliothek (engl.
library) kann man jedoch beliebig oft verwenden, ohne dass diese dabei
verbraucht werden würden. (Bei einem Bezahlservice _verbraucht_ man vielleicht
Tokens oder Credits wenn man den Service _verwendet_.)

Natürlich kann man Ausdrücke wie «physikalischer Server» oder «to consume a
library» pragmatisch deuten und dabei das Gemeinte korrekt vom Gesagten
unterscheiden. Diese Toleranz führt jedoch dazu, dass die Unterschiede
zwischen verschiedenen Begriffen («physisch» und «physikalisch»; «konsumieren»
und «verwenden») verloren gehen und man dadurch weniger klar kommunizieren kann:

&gt; — The message was _consumed_ by all workers.&lt;br&gt;
&gt; — So we have a race condition in the messaging component?&lt;br&gt;
&gt; — No. The workers just use `get` instead of `pop` on the message stack. Works as intended…&lt;br&gt;
&gt; — But then those messages are not _consumed_, but only _used_.&lt;br&gt;
&gt; — So what's the difference?

Oder:

&gt; — Wir brauchen zusätzliches IT-Budget um neue physikalische Server zu beschaffen.&lt;br&gt;
&gt; — Aber wir haben doch gar kein Physik-Departement an unserer Universität!?&lt;br&gt;
&gt; — Was tut denn das zur Sache? Wir brauchen die Server um Word-Dateien zu archivieren.&lt;br&gt;
&gt; — Und was hat denn das mit Physik zu tun?&lt;br&gt;
&gt; — Gar nichts, aber ich will nicht, dass die Dokumente in die Cloud gelangen.&lt;br&gt;
&gt; — Ach, sie meinen einen _physischen_ Server?&lt;br&gt;
&gt; — Natürlich, davon rede ich schon die ganze Zeit!

Diese Dialoge sind natürlich weit hergeholt und dürften ‒ hoffentlich ‒ so nie
zu vernehmen sein. Schliesslich schreibe ich diese Sprachglosse nur, um mich zu
unterhalten. (Man beachte: sprachliches Feingefühl kann eine zusätzliche Quelle
von Ärgernis _und_ Heiterkeit im Alltag sein.)
</content>
    </entry>
    <entry>
        <title>Tests Are Not Specifications</title>
        <link href="https://paedubucher.ch/articles/2021-05-02-tests-are-not-specifications.html"/>
        <author>
            <name>Patrick Bucher</name>
        </author>
        <id>https://paedubucher.ch/articles/2021-05-02-tests-are-not-specifications.html</id>
        <updated>2021-05-02T10:00:00Z</updated>
        <content type="text/markdown; charset=UTF-8">
In _The Clean Coder_, Chapter 7 (_Acceptance Tests_), Robert C. Martin writes
(p. 109):

&gt; But the real reason these tests aren't redundant is that their primary
&gt; function _is not testing_. The fact that they are tests is incidental. […]
&gt; The fact that they automatically verify the design, structure, and behaviour
&gt; that they specify is wildly useful, **but the specification is their true
&gt; purpose**. [Bold emphasis mine.]

OK, boomer, here's your spec:

    assert f(0) == 1
    assert f(-1) == -2
    assert f(+1) == 4
    assert f(+4) == 13
    assert f(-9) == -26

Now implement `f(x)`. The requirements are clear, aren't they?

You're still here? Where's the implementation!? I told you everything you need.
You got test cases! They _are_ the specification; you just told me this, and of
course, _Uncle Bob is always right_, they told me in school.

Still not done? OK, you're fired. Here's the Python implementation. Easy, isn't
it?

    def f(x):
        return (x + 2) * 3 - 5

Those young developers are no good, after all. Let's hire one of those old
programmers, please bring in that old, more-arrogant-than-Uncle-Bob, grumpy
Dijkstra guy ([EWD
981](https://www.cs.utexas.edu/users/EWD/transcriptions/EWD09xx/EWD981.html)):

&gt; At this stage I can give you some behavioural advice. Contrary to what
&gt; misguided—and misguiding—educationists may have told you, don't waste your
&gt; time looking at specific examples. Trying to come to grips with a large set by
&gt; looking at a few—hopefully representative—elements of that set is one of the
&gt; most ineffective ways of spending one's time. Go immediately for the general
&gt; case, or, in other words, **attack the set not by looking at specific elements
&gt; but by analysing the definition of the set.** [Bold emphasis mine.]

So you're not interested in some randomly picked elements of both the problem
and solution set, but in those set's definition? Is that all you need? Well,
here's that definition:

    y = f(x) = (x + 2) * 3 - 5

But I can also provide you with a set of test cases, or _specs_, as they are
called by some developers. Oh, you don't need them? But what is QA supposed to
do all day long then? You mean we can fire them? Wow, that sounds great. You're
hired, Mr. Dijkstra!

Did I hear this right, Mr. Dijkstra, _tests are not specifications_? Now you're
talking…
</content>
    </entry>
    <entry>
        <title>Gendersprache, schlechte Sprache</title>
        <link href="https://paedubucher.ch/articles/2020-09-27-gendersprache-schlechte-sprache.html"/>
        <author>
            <name>Patrick Bucher</name>
        </author>
        <id>https://paedubucher.ch/articles/2020-09-27-gendersprache-schlechte-sprache.html</id>
        <updated>2020-09-27T09:00:00Z</updated>
        <content type="text/markdown; charset=UTF-8">
Neulich bin ich auf einer Webseite auf ein Angebot _«für Benutzer*innen»_
gestossen. Dabei handelt es sich nicht etwa um ein
[Glob-Pattern](https://en.wikipedia.org/wiki/Glob_(programming)), worauf [der
Asterisk](https://www.duden.de/rechtschreibung/Asterisk) schliessen lassen
könnte, sondern wohl um ein
[Gendersternchen](https://de.wikipedia.org/wiki/Gendersternchen). (In der
Linguistik wird dieses Zeichen zur Kenntlichmachung von ungrammatischen 
Konstrukten verwendet. Die Ironie dieses Zufalls dürfte im weiteren Verlauf des
Artikels deutlich werden.)

Was soll nun dieses Gendersternchen bewirken? Es kürzt das umständlichere «für
Benutzer und Benutzerinnen» ab. Oder war es «für Benutzerinnen und Benutzer»?
Wie auch immer die Reihenfolge gewählt wird: der Gebrauch dieses Konstrukts
geht auf ein Missverständnis zurück, nämlich dass mit «Benutzer» immer eine
männliche Person gemeint ist. Schliesslich hat das Wort ja den bestimmten
Artikel «der». Um zu zeigen, _dass_ es sich hier um ein Missverständnis handelt,
betrachten wir folgenden Satz:

&gt; Der Benutzer ist jemand, der eine Anwendung benutzt.

Nun formulieren wir den Satz um, ohne dessen Bedeutung zu ändern:

&gt; Der Benutzer ist _eine Person, die_ eine Anwendung benutzt.

_Der_ Benutzer kann also _die_ Person sein. Folgt man der Argumentation hinter
dem Gendersternchen, nämlich dass «der Benutzer» sich auf ein männliches
Individuum bezieht, müsste sich «die Person» auf eine Frau beziehen. Wie kann
denn nun ein Subjekt gleichermassen männlich und weiblich sein?

Die Antwort ist einfach: die deutsche Sprache unterscheidet zwischen einem
_grammatischen_ Geschlecht (_Genus_) und einem _natürlichen_ oder _biologischen_
Geschlecht (_Sexus_). So kann «der Benutzer» durch «der Mensch», durch «das
Individuum» und durch «die Person» ersetzt werden. Natürlich lässt sich «der
Benutzer» auch durch Namen ersetzen.  Angenommen, Maria und Martin sind
Benutzer, wären die folgenden Sätze grammatikalisch korrekt:

&gt; Maria ist eine Person, _die_ eine Anwendung benutzt.

&gt; Martin ist eine Person, _die_ eine Anwendung benutzt.

&gt; Maria ist ein Mensch, _der_ eine Anwendung benutzt.

&gt; Martin ist ein Mensch, _der_ eine Anwendung benutzt.

&gt; Maria ist ein Individuum, _das_ eine Anwendung benutzt.

&gt; Martin ist ein Individuum, _das_ eine Anwendung benutzt.

Somit kann «der Benutzer» sowohl männliche als auch weibliche Personen
bezeichnen. Dies gilt jedoch nicht für «die Benutzerin», die sowohl
_grammatisch_ weiblich ist, als auch eine Person _natürlichen weiblichen
Geschlechts_ bezeichnet.

Ob jedoch mit «der Benutzer» nur das grammatische oder auch das natürliche
männliche Geschlecht gemeint ist, verrät uns nur der _Kontext_. Wenn sich nun
das eingangs erwähnte Angebot, das sich an _«Benutzer*innen»_ richtet, sich
bloss noch an _«Benutzer»_ richten würde, wäre eine Interpretation nötig. Ist
hier das grammatische oder das natürliche männliche Geschlecht gemeint?

Das Substantiv «Benutzer» ist vom Verb «benutzen» abgeleitet. Es bezeichnet eine
Person, die etwas benutzt. Gleichermassen ist ein «Programmierer» eine Person,
die programmiert, und ein Helfer eine Person, die hilft. Ein Wissenschaftler ist
eine Person, die Wissenschaft betreibt, und ein Fussballer spielt ganz einfach
Fussball. Das natürliche oder biologische Geschlecht der Person (der _Sexus_),
welche diese Handlung verrichtet, ist bei dieser Wortbildung weder relevant,
noch analytisch daraus erfahrbar.

Im Englischen funktioniert die Substantivbildung aus Verben durch Hinzufügen der
Wortendung «er» ebenfalls: _A player is a person that plays a game._ Der
bestimmte Artikel «the» impliziert jedoch keinen Genus, wodurch auch niemand auf
die Idee kommen würde, daraus auf den Sexus zu schliessen.

Ich denke, dass die meisten Leute bei einem Online-Angebot «für Benutzer» den
Kontext dahingehend interpretieren können, dass sich die männliche Form hier nur
auf das grammatische und nicht auch auf das natürliche Geschlecht bezieht.
Warum sollte denn eine Online-Plattform ca. die Hälfte der potentiellen _User_
‒ ein Ausweichen auf Englisch ist hier herrlich bequem, und dürfte für den
inflationären Gebrauch von Anglizismen mitschuldig sein ‒ von seinem Angebot
ausschliessen?  (Vielleicht weil es ein Webseitenbetreib*er*, und keine
Webseitenbetreiber*in* ist?  Doch ich drifte in Sarkasmus ab…) Wer es für
nötig empfindet, sein Zielpublikum mit «Benutzer\*innen» anzusprechen, spricht
diesem die entsprechende Interpretationsfähigkeit implizit ab. (Ein ähnliches
Phänomen ist bei der sogenannten _leichten Sprache_ zu beobachten, bei der man
sich als Leser so vorkommt, als ob man von oben herab wie ein Kind angesprochen
werde.) Somit empfinde ich den Gebrauch gendergerechter Sprache als
bevormundend, ja herablassend.

Um zu der Fehlinterpretation zu gelangen, dass mit dem Wort «Benutzer» Frauen
ausgeschlossen sind, muss man schon eine ganz dicke Genderbrille aufgesetzt
haben, welche die kontextsensitive Interpretation von Sprache erschwert. Wer
hinter jedem «der» eine patriarchalische Unterdrückungsstruktur wittert, dürfte
ohnehin Schwierigkeiten haben, Alltagsformulierungen pragmatisch aufzufassen.
Natürliche Sprache ist ambivalent und muss zwecks Verständnis wohlwollend
interpretiert werden. Wer käme denn auf die Idee, einem Sprecher beim Gebrauch
der Wörter «Sonnenuntergang» und «Sonnenaufgang» die Anhängerschaft an das
ptolemäische Weltbild zu unterstellen? (Und wieder drifte ich in Sarkasmus ab…
Ob es wohl am Thema liegt?)

Wer mit der Genderbrille durch die Welt gehen will, der darf das von mir aus
tun. Ich möchte das nicht. So ist meine Konsequenz des vorher geschilderten
Zusammenhanges, dass ich keine gendergerechte Sprache verwende. Wer
Formulierungen wie «Benutzer*innen» verwenden möchte, darf das gerne tun. Ich
halte es bloss für umständlich und bevormundend. (Dass ich die als Informatiker
beim Asterisk zunächst an ein Glob-Pattern denke, ist eine Berufskrankheit. Ich
kann den Kontext jedoch dahingehend interpretieren, dass hier eben _kein_
Glob-Pattern gemeint ist. Auch kann ich bei der Bezeichnung «der Benutzer»
kontextsensitiv interpretieren, dass hier mit dem bestimmten Artikel «der» nur
das grammatische und nicht das natürliche Geschlecht gemeint ist.)

Sprache ist die Interpretation von Zeichen auf verschiedenen Ebenen. Durch das
Explizitmachen von natürlichen Geschlechtern durch Gendersternchen gewinnen wir
höchstens, dass eine verschwindend geringe Menge von Missverständnissen
ausgeräumt wird; Missverständnisse, die durch eine pragmatische Lesart gar
nicht auftreten würden. (Oder kann mir jemand von einer Informatikerin
berichten, die auf einem Computer über Administratorenrechte verfügt, sich aber
unter der Bezeichung «Administrator des Computers» nicht angesprochen fühlt?)
Die Notwendigkeit von Interpretation könnte in diesem Sinne noch weiter
reduziert werden, indem man etwa auf Metaphern oder sprachlich ambivalente
Figuren aller Art verzichten würde. Dies mag für bestimmte Textarten sinnvoll,
ja notwendig sein: etwa bei Gesetzestexten, Verträgen oder technischen
Dokumentationen. Im Alltagsgebrauch oder für den vorliegenden polemischen Text
führte eine solche Einschränkung jedoch nur zu einer Verarmung der Sprache.

Gendersternchen («Benutzer*innen») und Binnen-I («BenutzerInnen») funktionieren
zudem nicht zufriedenstellend mit allen Fällen. Die Formulierung «der
Anmeldezeitpunkt des Benutzers» (Genitiv) müsste gendergerecht _und_
syntaktisch/grammatikalisch korrekt etwa als «der Anmeldezeitpunkt der
Benutzerin bzw. der Anmeldezeitpunkt des Benutzers» formuliert werden.
Varianten mit Gendersternchen oder Binnen-I funktionieren nicht («der
Anmeldezeitpunkt der/des Benutzer\*in») oder verkommen zu einer syntaktischen
Monstrosität («der Anmeldezeitpunkt der/des Benutzer(in/s)»). In der Praxis
werden solche Formulierungsprobleme mit dem Verzicht auf den Genitiv umgangen.
Die Sprache verliert weiter an Ausdrucksstärke und Variantenreichtum.

Ein weiteres Problem an «gegenderter» Sprache ist, dass Formulierungen wie
«Helferinnen und Helfer» explizit ein natürliches Geschlecht suggerieren, wo
dieses nicht relevant ist. Da «Helferin» immer biologisch weiblich ist, gewöhnt
sich der Leser durch diese Nebeneinanderstellung zum «Helfer» zusehends an die
Interpretation, das bei letzterem das männliche biologische Geschlecht gemeint
sein muss. Da man weibliche oder sachliche Wörter wie «Person» oder «Individuum»
nicht durch Hinzufügen oder Weglassen einer Silbe «gendern» kann, wird das
grammatische Maskulinum zusehends sexualisiert, was hingegen bei Femininum und
Neutrum nicht geschieht.

Apropos Anglizismen: Wer denkt, wie ich es weiter oben angesprochen habe, man
könne dem Gendersternchen entgehen, indem man das deutsche Wort «Benutzer» durch
das englische Wort «user» ersetzt, für den habe ich schlechte Nachrichten! Immer
häufiger sehe ich die Bezeichnung _«User*innen»_, naturgemäss nur auf
deutschsprachigen Webseiten. Gibt es im Englischen keine verräterischen
bestimmten Artikel, verliere ich diese nützliche Ambivalenz beim Übersetzen ins
Deutsche. (Daher kommt es, dass englische Lehnwörter wie «Blog» von
verschiedenen Leuten mit unterschiedlichen bestimmten Artikeln bezeichnet
werden: «das Blog» und «der Blog» sind beide anzutreffen.) So mache ich mir das
Schreiben einfacher, indem ich sowohl auf Gendersprache als auch auf das
Umschwenken von geschlechtsneutralen Anglizismen verzichte. (Um Anglizismen
komme ich als Informatiker ansonsten kaum herum.)

Germanisten und Linguisten mögen meine obige Argumentation gerne zerpflücken.
Ich bin gerne dazu bereit, etwas neues zu lernen. Wie viel Ambivalenz und
Interpretationsspielraum ‒ sei es durch das Weglassen von Gendersternchen oder
durch das Beifügen von sprachlichen Figuren ‒ ich meinen Lesern zumute, ist
jedoch meine persönliche Entscheidung. So bin ich in dieser Beziehung
_konservativ_, zumal ich hinter dem sogenannten _progressiven_ Anpassungsdruck,
der auf unsere Sprache ausgeübt wird, mehr Tendenzen zu einem Neusprech als eine
sprachliche Bereicherung erkenne. Sprachliche Figuren _oder_ Gendersternchen:
beides habe ich nicht im Angebot.
</content>
    </entry>
    <entry>
        <title>Arch Linux Setup with Disk Encryption</title>
        <link href="https://paedubucher.ch/articles/2020-09-26-arch-linux-setup-with-disk-encryption.html"/>
        <author>
            <name>Patrick Bucher</name>
        </author>
        <id>https://paedubucher.ch/articles/2020-09-26-arch-linux-setup-with-disk-encryption.html</id>
        <updated>2020-09-26T13:00:00Z</updated>
        <content type="text/markdown; charset=UTF-8">
I've been using Arch Linux since 2016. I got to understand the system better
since then, and installing Arch nowadays is rather a strain on my fingers than
on my brain. I automated my personal setup procedure to some extent with a
couple of shell scripts, which I'm adjusting as time goes and hardware changes.
I collect those scripts and setup instructions in a [GitHub
repo](https://github.com/patrickbucher/docs/tree/master/arch-setup).

Even though I've often taken my laptop with me, especially for
university, I never made the effort to encrypt my disk or a single partition
thereon. Sensitive data, such as passwords, are stored encrypted using GPG. And
my GPG private key is protected with a strong passphrase; so strong, that I
mistype it once in a while.

Having stored SSH keys on my laptop, however, is a risk in terms of data
protection. I do not protect most of my SSH keys with passwords. (_Not_ having
to use passwords is one of the conveniences SSH gives you, after all.) But some
of those SSH keys are gateways to quite some important data, and therefore I
need to re-consider my security practices.

Protecting SSH keys with a passphrase is very inconvenient, especially if you do
a lot of `git push` operations via SSH. It's possible to use a password
protected SSH key for `git` only, but having to deal with multiple SSH keys is
inconvenient, too. (My rule is to use one SSH key per client machine.) So
encrypting the hard disk or at least some partitions of it might be the better
solution, because it only requires me to enter an additional password once
during startup, which is once or twice a day.

# dm-crypt and LUKS

The Arch wiki provides an article on [how to encrypt an entire
system](https://wiki.archlinux.org/index.php/Dm-crypt/Encrypting_an_entire_system).
Such a setup is based on LUKS (Linux Unified Key Setup), a disk encryption
specification, and dm-crypt, the Linux kernel's device mapper for encrypted
devices.

The wiki mentions different
[scenarios](https://wiki.archlinux.org/index.php/Dm-crypt/Encrypting_an_entire_system#Overview)
for disk encryption. The first option, [LUKS on a
partition](https://wiki.archlinux.org/index.php/Dm-crypt/Encrypting_an_entire_system#LUKS_on_a_partition),
is very simple, but not ideal for me, because I'm not just using one giant root
partition, but separate partitions for `/var`, `/tmp` and `/home`. With this
setup, a process filling up my `/var/log` directory won't prevent me from
writing to `/home` or the root partition, which might be necessary to deal with
the issue.

One could argue that it's sufficient to just encrypt the `/home` partition,
which contains the most sensitive data, after all. However, it's possible that
some program logs sensitive data to `/var/log` or stores it under `/opt`, so
that an encrypted `/home` partition won't prevent sensitive data from being
leaked. An unencrypted root partition also makes it possible to tamper
programs, so that data is leaked later.

It's also possible to encrypt the partitions mentioned seperately, which would
require me to enter my password upon booting for every single partition. This is
highly impractical, and therefore I won't do it that way.

The second option, [LVM on
LUKS](https://wiki.archlinux.org/index.php/Dm-crypt/Encrypting_an_entire_system#LVM_on_LUKS),
is a better fit for my purpose. Here, LUKS provides one single block device, on
top of which the encrypted partitions are created. The other, arguably more
sophisticated scenarios, are mostly useful when dealing with multiple disks, or
if an encrypted boot partition is required. So let's stick to scenario two: LVM
on LUKS.

# Partitioning

My Lenovo Thinkpad X1 has a SSD with roughly 475 GB and 16 GB memory. This is
important when considering partition sizes. I usually create my partitions as
follows:

1. A 256 MB `/boot` partition for the bootloader (FAT 32).
2. A 16 GB (the size of my memory) swap partition.
3. A 128 GB `/` (root) partition (ext4).
4. A 64 GB `/var` partition (ext4).
5. A 8 GB `/tmp` partition (ext4).
6. And a `/home` partition with the remainder of the space, i.e. roughly 260 GB
   in my case (ext4).

The choice of partition sizes is subject to debate. Some prefer to have a swap
partition with twice the size of the physical memory. Some prefer to make the
root, `/var/`, and `/tmp` partitions smaller or bigger. But the above
partitioning scheme was never the cause of any trouble for me, so far.

With those decisions taken, let's proceed to the setup.

# Setup Procedure

So let's go through the setup step by step.

## Prepare the Installation Medium

First, I download the latest Arch amd64 image via BitTorrent [Magnet
link](magnet:?xt=urn:btih:db56a13a6555179990837759ca27274d0be49aca&amp;dn=archlinux-2020.09.01-x86_64.iso)
from the [download](https://www.archlinux.org/download/) page and verify its
checksum against the one on the website:

    $ sha1sum archlinux-2020.09.01-x86_64.iso
    95ebacd83098b190e8f30cc28d8c57af0d0088a0

Then I copy the image on a USB dongle with a capacity of at least 700 MB:

    $ sudo dd if=archlinux-2020.09.01-x86_64.iso of=/dev/sda bs=4M
    $ sync

Then I unplug my USB dongle and plug it into the Thinkpad X1 I'd like to setup,
and boot from it (UEFI boot). Once the installation environment is loaded, let's
continue with the setup.

My Thinkpad X1 has a US keyboard, which I [prefer to
use](https://paedubucher.ch/articles/2020-09-16-compose-key-on-x.html) nowadays.
However, it's possible to change the keyboard layout as follows (for Swiss
German), if wanted:

    # loadkeys de_CH-latin1

Next, I establish a WiFi connection with `frzbxpdb5` being my network SSID, 
`wlan0` being my WiFi device, and `[topsecret]` being my password:

    # iwctl --passphrase '[topsecret]' station wlan0 connect frzbxpdb5

Let's continue with the partitioning of the disk.

## Partitioning

In order to partition the hard disk, one first needs to know the device name,
which can be found using `lsblk`:

    # lsblk

This lists `sda`, the USB dongle, and `nvme0n1`, the 475 GB solid state drive
aforementioned.

### Filling the Disk with Junk Data

As a first step, the disk shall be overwritten with random data.  Even though I
haven't used this laptop productively yet, and so there's no sensitive data
stored on it, it is still a good practice to fill it up once with random data.
This makes it harder for an attacker to distinguish between encrypted data and
random junk.

    # shred --random-source=/dev/urandom --iterations=1 /dev/nvme0n1

In my case, using only one single iteration is sufficient. If there was actual
data stored on the disk, multiple iterations should be considered.

### Creating a Boot Partition

Next, I create a new GPT partition scheme on that disk:

    # parted -s /dev/nvme0n1 mklabel gpt

I'm not going to encrypt my boot partition, so I create it as I do for a regular
setup without encryption (as a FAT 32 boot partition, that is):

    # parted -s /dev/nvme0n1 mkpart boot fat32 1MiB 257MiB
    # parted -s /dev/nvme0n1 set 1 esp on
    # mkfs.fat -F 32 /dev/nvme0n1p1

I leave a gap of 1 MB at the beginning, so no matter what block size my SSD
uses, the boot partition will always be properly aligned. The `esp` flag
identifies the partition as a UEFI system partition.

### Creating a Partition for Encryption

Now comes the crucial part. All the encrypted data is put into a single big
partition (`nvmen1p2`), taking up all of the remaining disk space, which is then
partitioned using a volume manager:

    # parted -s /dev/nvme0n1 mkpart cryptlvm 257MiB '100%'

The encryption is set up on this partition:

    # cryptsetup luksFormat /dev/nvme0n1p2

Enter &quot;YES&quot; if asked for confirmation, and pick a strong passphrase to be
entered twice. (At this point, remember exactly which keyboard layout you're
on!).

To further work with the encrypted partition, let's open it, which requires to
enter the password chosen before:

    # cryptsestup open /dev/nvme0n1p2 cryptlvm

Now a physical volume for the volume mapping needs to be created:

    # pvcreate /dev/mapper/cryptlvm

The partitions are going to be managed in a volume group, which I simply call
`volgrp` for the sake of brevity:

    # vgcreate volgrp /dev/mapper/cryptlvm

Now everything is set up to create the remaining, i.e. the encrypted partitions.

### Creating the Remaining Partitions

Those are created using `lvcreate` by setting a size (`-L`/`-l` parameter) and a
name (`-n` parameter):

    # lvcreate -L 16G volgrp -n swap
    # lvcreate -L 128G volgrp -n root
    # lvcreate -L 64G volgrp -n var
    # lvcreate -L 8G volgrp -n tmp
    # lvcreate -l '100%FREE' volgrp -n home

**Update**: As the user _fra-san_ [pointed
out](https://unix.stackexchange.com/questions/611421/arch-linux-setup-with-encryption-lvm-on-luks)
in a comment on StackExchange, leaving some space open rather than using `-l
'100%FREE'` for the `/home` partition is useful when partition sizes should be
increased later.  Shrinking partitions requires to unmount them, whereas growing
them can happen with the affected partition still being mounted. (Check out
`lvresize` for details.)

The partitions are going to be formatted using the `swap` and `ext4` format,
respectively:

    # mkswap /dev/volgrp/swap
    # mkfs.ext4 -F /dev/volgrp/root
    # mkfs.ext4 -F /dev/volgrp/var
    # mkfs.ext4 -F /dev/volgrp/tmp
    # mkfs.ext4 -F /dev/volgrp/home

For the actual setup, those partitions (and the `/boot` partition created
before) need to be mounted to `/mnt`:

    # mount /dev/volgrp/root /mnt

    # mkdir /mnt/boot
    # mount /dev/nvme0n1p1 /mnt/boot

    # swapon /dev/volgrp/swap

    # mkdir /mnt/var
    # mount /dev/volgrp/var /mnt/var

    # mkdir /mnt/tmp
    # mount /dev/volgrp/tmp /mnt/tmp

    # mkdir /mnt/home
    # mount /dev/volgrp/home /mnt/home

Now the partitions are ready for a regular bootstrap installation. (Setting up
the boot loader will require some more specific instructions to disk encryption
later on.)

## Bootstrap Installation

Now let's install the base system, together with the `lvm2` package:

    # pacstrap /mnt base linux linux-firmware lvm2

In order to get the mounting done automatically upon restart, let's save it in
the `fstab` file:

    # genfstab -U /mnt &gt;&gt; /mnt/etc/fstab

When this is done, let's switch into the installed environment:

    # arch-chroot /mnt

A password needs to be set for the `root` user:

    # passwd

In order to have a WiFi connection after rebooting, let's install a couple of
networking packages (some of those specific to my Intel hardware):

    # pacman -S iw wpa_supplicant dialog intel-ucode netctl dhcpcd

I set the time zone to Zurich (Europe), update and save the system time:

    # ln -sf /usr/share/zoneinfo/Europe/Zurich /etc/localtime
    # timedatectl set-ntp true
    # hwclock --systohc

For language and locale, I simply use `en_US.UTF-8` and `en_US-UTF-8 UTF-8`,
respectively:

    # echo 'en_US.UTF-8 UTF-8' &gt;&gt; /etc/locale.gen
    # locale-gen
    # echo 'LANG=en_US.UTF-8' &gt; /etc/locale.conf

Due to the lack of imagination, I call my laptop simply _carbon_:

    # echo carbon &gt; /etc/hostname

This is a very basic setup. Now let's make sure it can be booted by installing
the boot loader:

## Configuring the Boot Loader

I've always been using the systemd boot loader on Arch Linux, which is quite
simple to configure. First, the computer needs to get a unique id, then the boot
loader can be installed into the `/boot` partition:

    # systemd-machine-id-setup
    # bootctl --path=/boot install

Now comes the tricky part: The UUID of the boot partition needs to be figured
out. `blkid` lists various partitions, but which one to choose? It's the
LUKS partition containing the encrypted volume: `/dev/nvme0n1p2`. Its UUID can
be extracted as follows, and shall be saved into a variable for later use:

    # uuid=$(blkid | grep 'crypto_LUKS' | egrep -o ' UUID=&quot;[^&quot;]+&quot;')
    # uuid=$(echo $uuid | awk -F '=' '{ print $2 }' | tr -d '&quot;')

The first line lists the partitions (`blkid`), extracts the line with the
encrypted partition (`grep`), and further extracts the part of the line defining
the UUID (`egrep`). Don't forget the space in front of `UUID`, otherwise the
`PARTUUID` is extracted, too. In the second line, the definition
(`UUID=&quot;abc...&quot;`) is split at the equal sign, of which the second part
(`&quot;abc...&quot;`) is taken using `awk`. Then the surrounding double quotes are
removed with `tr`. The variable `$uuid` now contains the UUID of the encrypted
partition.

**Update**: As the user _fra-san_ [pointed
out](https://unix.stackexchange.com/a/611507/223188), there's an easier way to
extract the UUID:

    # uuid=$(blkid --match-tag UUID -o value /dev/nvme0n1p2)

Having this information, the entry for the boot loader can be created as
follows:

    # cat &lt;&lt;EOF &gt;/boot/loader/entries/arch.conf
    title   Arch Linux
    linux   /vmlinuz-linux
    initrd  /initramfs-linux.img
    options cryptdevice=UUID=${uuid}:cryptlvm root=/dev/volgrp/root
    EOF

Now a simple bootloader configuration needs to be created:

    # cat &lt;&lt;EOF &gt;/boot/loader/loader.conf
    default arch
    timeout 0
    editor  0
    EOF

## Creating the Ramdisk Environment

As mentioned earlier, there are some further adjustments for disk encryptions to
be made. Let's open `/etc/mkinitcpio.conf` and go to the `HOOKS` definition:

    HOOKS=(base udev autodetect modconf block filesystems keyboard fsck)

Which needs to be extended with `encrypt` and `lvm2`:

    HOOKS=(base udev autodetect modconf block filesystems keyboard fsck encrypt lvm2)

Then the ramdisk environment can be created:

    # mkinitcpio -P

Which generated the `initramfs-linux.img` file referred to from the boot loader
entry created before, as well as a fallback `initramfs-linux-fallback.img`.

## Finishing

Now that everything is set up, leave the `chroot` environment, unmount the new
system's partitions, and shut down the computer:

    # exit
    # umount -R /mnt
    # shutdown -h now

After removing the USB dongle, start the system again.

Early in the boot process, you'll be asked to enter the passphrase for
`/dev/nvme0n1p2`. After doing so, the system will boot. If not, you did
something wrong in the process, because it's working just fine on my Thinkpad X1
Carbon.

I also had issues getting everything right in the first place, but fortunately
got help on the [Linux &amp; Unix
StackExchange](https://unix.stackexchange.com/q/611421/223188). Thanks to the
user _frostschutz_ for pointing out the core issue. The user _Cbhihe_ pointed
out that my partition sizes weren't sustainable, so that I adjusted it
accordingly for this article.

# Conclusion

We set up Arch Linux with encrypted partitions using the variant LUKS on LVM.
The system is split up into multiple partitions, which are all encrypted using
the same password over one common encrypted volume.

The `/boot` partition remains unencrypted, which would allow an attacker to
tamper with my system boot, and, potentially, with my entire setup. So if my
laptop gets lost, there is no 100% guarantee that my system hasn't been
manipulated. However, having all the other partitions encrypted makes
manipulation or data leaks extremely unlikely.

When my laptop gets stolen or lost for an extended period of time, it is still a
good idea to revoke all the SSH and GPG keys. But thanks to encryption, that
procedure won't be an act of emergency, but one that could be carefully planned
for and executed at a convenient time.

The system set up so far is really just a base installation. In order to work on
this computer, further tasks have to be performed, as roughly described in my
[Arch
Setup Notes](https://github.com/patrickbucher/docs/blob/master/arch-setup/arch-setup.md)
on GitHub. Maybe I'll write a follow-up article in the future to cover those
steps more in detail.
</content>
    </entry>
    <entry>
        <title>Basic Printing on OpenBSD</title>
        <link href="https://paedubucher.ch/articles/2020-09-20-basic-printing-on-openbsd.html"/>
        <author>
            <name>Patrick Bucher</name>
        </author>
        <id>https://paedubucher.ch/articles/2020-09-20-basic-printing-on-openbsd.html</id>
        <updated>2020-09-20T18:00:00Z</updated>
        <content type="text/markdown; charset=UTF-8">
I have a roughly ten year old Brother HL-5370DW printer on the shelf next to me.
This printer is mostly used by my wife to print sewing patterns. When I was
studying computer science, I sometimes printed documents I've written for
proofreading. I often was able to find typos that I didn't see on the screen
even after proofreading the document two or three times. However, I didn't
bother to print out my bachelor thesis. Printing 120 pages just for proofreading
just seemed a waste to me. I did my proofreading on the screen extra carefully,
and nobody complained about typos. (Which doesn't mean that there were none.)

Having finished my studies, I hardly ever print out documents. However, I still
prefer to read long texts on paper rather than on the screen. Therefore I often
buy technical books as paperbacks or hardcovers rather than ebooks. And if I buy
an ebook with demanding content, I print out those sections for offline reading.

Having switched to OpenBSD for my private computing shifted my reading habits
more towards manpages. When I need to figure out how something works on
OpenBSD, `apropos(1)` beats Google as a starting point in many cases. Some
manpages are really long, for example `ksh(1)`. I have a book on the Korn Shell
in my basement, which covers `ksh93`.  However, there are some differences
between `ksh93` and OpenBSD's `pdksh`. So reading the manpage not only gives me
more accurate information, but also _less_ to read.

So why not printing out the manpage `ksh(1)`? I can do so even nicely formatted
using PostScript:

    $ man -T ps -O paper=a4 ksh &gt;ksh.1.ps

Now `ksh.1.ps` can be read with `zathura(1)`, given that the package
`zathura-ps` is installed:

    # pkg_add zathura zathura-ps
    $ zathura ksh.1.ps

But why using PostScript and not PDF like anybody else for the last twenty five
years? Because PostScript is the least common denominator and, thus, supported
out of the box by OpenBSD. (For fancier printing options, check out `cups`, but
I'd like to keep it minimalistic for the moment.)

# Printer Setup

I figured out how to configure my printer by reading the section _The lpd
Printing Daemon_ in the 16th chapter of [Absolute OpenBSD (2nd
Edition)](https://nostarch.com/obenbsd2e) (p. 306-307) by [Michael W
Lucas](https://mwl.io/). This is how I applied the configuration to my local
setup.

First, I created the file `/etc/printcap` with the following content:

    lp|brother:\
        :sh=:\
        :rm=192.168.178.52:\
        :sd=/var/spool/output/brother:\
        :lf=/var/log/lpd-errs:\
        :rp=brother

There must be a newline at the end of the file. The line breaks are escaped
using backslashes, except for the last line. The options are defined as follows:

- The first line defines two names for my printer: `lp`, which should always be
  there, and `brother`, which is my arbitrary name for the printer.
- The second line (`sh`) defines that no _burst page_ (summarizing the last
  print job on a special page) should be printed.
- The third line (`rm`) refers to the printer on the network. My FritzBox always
  gives the same IP to my printer. It's also possible to use the printer's
  hostname.
- The fourth line (`sd`) defines the spooler directory for this printer. Print
  jobs are written into that directory.
- The fifth line (`lf`) defines a log file for error messages, which you hopefully
  never need to check.
- The sixth line (`rp`) defines the remote printer name.

Next, the spooler directory needs to be created. It must be owned by the user
`root` and the group `daemon`. Regular users need write access to this directory
in order to print documents:

    # mkdir /var/spool/output/brother
    # chown -R root:daemon /var/spool/output/brother
    # chmod 770 /var/spool/output/brother

Now the printer daemon `lpd` needs to be activated. To do so on system startup,
add the following line to `/etc/rc.conf/local`:

    lpd_flags=&quot;&quot;

Then start the service:

    # /etc/rc.d/lpd restart

**Update (2020-09-21)**: As one reader on
[Hacker News](https://news.ycombinator.com/item?id=24535357#24538879) pointed
out, the last two steps can be performed using `rcctl(8)`:

    # rcctl enable lpd
    # rcctl restart lpd

The manpage says that `rcctl(8)` was introduced in OpenBSD 5.7 back in 2015.
_Absolute OpenBSD (2nd Edition)_ is from 2013 and, thus, older than that. (At
the time of this writing, I'm using Version 6.7.)

Another reader pointed out that setting the access rights to `777` is a bad
practice. That's true, and I actually got the reasoning behind this wrong: I
thought any user must be able to write to the spooler, because any user is
supposed to print. However, it's `lpd` that is writing to the spooler, which of
course runs under the `daemon` group. Therefore, the access rights for
`/var/spool/output/brother` should be set to `770`, not to `777` (as corrected
above).

# Printing Documents

Now the printer is ready to accept jobs. In order to print the PostScript file
generated before, just run `lpr` on the file:

    $ lpr ksh.1.ps

It's also possible to send the PostScript output directly to the printer (this
is Unix, after all), if no preview is needed:

    $ man -T ps -O paper=a4 ksh | lpr

Printing plain text files behaved strange on my setup, but it worked well using
the `pr` formatter with `lpr` as follows:

    $ lpr -p plain.txt

Instead, I also convert plain text files to PostScript, which looks quite nice
on paper. I use `enscript(1)` for this task:

    # pkg_add enscript
    $ enscript plain.txt -o plain.ps
    $ lpr plain.ps

PDFs can also be converted to PostScript using `pdf2ps(1)`, which comes with
GhostScript, i.e. the `ghostscript` package:

    $ pdf2ps document.pdf document.ps

Unfortunately, this doesn't work with all PDFs. But for the time being, I have
enough manpages to read. Printing PostScript works extremely fast, by the way.
When I press return at the end of a `lpr` command, I can see the status LED on
my printer start blinking almost immediately.
</content>
    </entry>
    <entry>
        <title>Compose Key on X</title>
        <link href="https://paedubucher.ch/articles/2020-09-16-compose-key-on-x.html"/>
        <author>
            <name>Patrick Bucher</name>
        </author>
        <id>https://paedubucher.ch/articles/2020-09-16-compose-key-on-x.html</id>
        <updated>2020-09-16T21:00:00Z</updated>
        <content type="text/markdown; charset=UTF-8">
I've been using the Swiss keyboard layout for most of my life. Things changed
when I first had to work on a Mac Book. If the Swiss keyboard layout is not
great for programming on a usual keyboard, because it requires combinations with
Alt-Gr to type in braces and brackets, I consider the Mac version of it outright
horrible, because braces and brackets are located on the digits row. Those
symbols are not even painted onto their respective keys, which makes the
transition for non-Mac users even harder.

# Entering the US Keyboard Layout

So instead of learning an additional inefficient way of typing in special
characters, I decided to adopt to the US keyboard layout. This layout gives you
most characters required for programming with a single key or a combination of
Shift and another key.

One issue of the US layout is that it doesn't provide characters such as _ö_ or
_é_, which I still need to type in for emails, for documentation, and for this
article, of course. Mac OS provides a composing mechanism that lets you type a
double quote followed by a letter like _o_, and then combines those characters
to the German umlaut _ö_.  If you just want to type a double quotation mark,
you need to type Space right after it, so that the quotation mark is not
attempted to be combined with the next character entered. This is annoying, of
course, when programming. So I never really got warm with the Mac keyboard
layout and input method. An external keyboard with US layout didn't help much
in that respect, but at least allowed me to type in those cumbersome
combinations faster and more precisely.

# Switching Keyboard Layouts

Another solution to the goal conflict of typing in German umlauts and special
programming characters as fast as possible is to switch the keyboard layout as
needed. I'm very familier with this approach, because I use the Russian keyboard
layout once in a while. I modified my [dwm](http://dwm.suckless.org/)
configuration so that pressing Super+Tab changes my keyboard layout using the
following script (`switchkb`):

    #!/bin/sh

    layout=&quot;$(setxkbmap -query | grep layout | egrep -o [a-z]{2}$)&quot;
    if [ &quot;$layout&quot; == &quot;ch&quot; ]; then
        setxkbmap ru
    elif [ &quot;$layout&quot; = &quot;ru&quot; ]; then
        setxkbmap us 
    elif [ &quot;$layout&quot; = &quot;us&quot; ]; then
        setxkbmap ch
    fi

This lets me cycle through the keyboard layouts I need very quickly. I also
display the currently active keyboard layout in my status bar using
[slstatus](http://tools.suckless.org/slstatus/), which I described in a
[previous
article](https://paedubucher.ch/articles/2020-09-05-openbsd-on-the-desktop-part-i.html).
Even though this is a feasible solution, I still have to switch mentally between
the Swiss German and the US keyboard layout. Typing in a double quote in a German
email requires me to press another key than typing in that very same character
when programming. This additional mental burden is wearing me down on a usual
working day, which consists of roughly 70% programming and 30% communicating.
The communication part might actually be bigger, but some of the communication
also takes part in English, so that I'd be better off using the US rather than
the Swiss German keyboard layout.

Typing cyrillic letters still requires the Russian keyboard layout, so I won't
get rid of my `switchkb` script and Super+Tab. However, since typing in
punctuation marks using the Russian keyboard layout is almost the same as
typing in those characters on the US keyboard layout, I rather ditch my native
Swiss German layout and embrace the imperialistic option.

# Composing Characters

While being able to write source code faster and not having to distinguish
between English, German, and Russian when typing in punctuation marks sounds
great in terms of _efficiency_, not typing in the typographically correct
representations of German umlauts (_Ae_, _Oe_, _Ue_ instead of _Ä_, _Ö_, _Ü_)
or misspelling French words with accent marks (_depecherent_ instead of
_dépêchèrent_) is bad in terms of communicating _effectively_. So there must be
at least a way to type in those characters somewhat efficiently.

One option would be to type in those characters as hexadecimal Unicode code
points.  I've already learned a few thereof by heart, such as U+2012, U+2013,
and U+2014 for dashes of different lenghts, or simple to remember ones like
U+00ab and U+00bb for guillemets (_«»_). In `vim`, those sequences can be
entered by presing Ctrl+V U followed by the part after U+. In GTK applications,
Ctrl+U enters a mode to enter those codes to be finished with a Space (or maybe
some other character not representing a hexadecimal digit).

However, remembering a lot of Unicode code points is not a very intuitive way
to type and probably leads to a lot of lookups on
[FileFormat.Info](https://www.fileformat.info), which is at least a more
sophisticated way than just googling those character code points, which usually
ends up on FileFormat.Info anyway, or, worse, on a big Wikipedia page
discussing the history and cultural significance of the `LATIN SMALL LETTER C
WITH CEDILLA` (_ç_) before giving me it's code point U+00e7.

## Entering the Compose Key

A better option is certainly the _Compose Key_. (Check out this [Wikipedia
Article](https://en.wikipedia.org/wiki/Compose_key) for a discussion of its
history and cultural significance). Even though not even the nerdiest of nerd
keyboards come with a physical compose key nowadays, the X Window System still
supports the underlying mechanism. And since X is running on both my Arch Linux
and OpenBSD computers, using its compose key mechanism helps me both at work
and for my private computing, the latter consisting mostly of writing articles
about entering special characters on special computing setups and the like.

A good reference for this mechanism is the manual page `Compose(5)` or
`XCompose(5)` on both Arch Linux and OpenBSD. (I guess it also applies to
FreeBSD, but I didn't check yet). However, there's some additional information
required to do a basic setup, so here's a quick guide.

First, a compose key needs to be picked. There are various options, which can be
looked up as follows:

    # OpenBSD
    $ grep 'compose:' /usr/X11R6/share/X11/xkb/rules/base.lst

    # Arch Linux
    $ grep 'compose:' /usr/share/X11/xkb/rules/base.lst

Which shows a list of keys that could be used for the compose key:

      compose:ralt         Right Alt
      compose:lwin         Left Win
      compose:lwin-altgr   3rd level of Left Win
      compose:rwin         Right Win
      compose:rwin-altgr   3rd level of Right Win
      compose:menu         Menu
      compose:menu-altgr   3rd level of Menu
      compose:lctrl        Left Ctrl
      compose:lctrl-altgr  3rd level of Left Ctrl
      compose:rctrl        Right Ctrl
      compose:rctrl-altgr  3rd level of Right Ctrl
      compose:caps         Caps Lock
      compose:caps-altgr   3rd level of Caps Lock
      compose:102          &amp;lt;Less/Greater&amp;gt;
      compose:102-altgr    3rd level of &amp;lt;Less/Greater&amp;gt;
      compose:paus         Pause
      compose:prsc         PrtSc
      compose:sclk         Scroll Lock

Caps Lock is a good choice, but would make it harder to type anonymous hate mail
in all-caps to people not agreeing on my operating system preferences.
Therefore I pick the Menu key, which I didn't even use once in my dark Windows
days. The compose key can be defined in `~/.xinitrc`, ideally together with the
choice of the aforementioned imperialistic keyboard layout:

    setxkbmap us
    setxkbmap -option compose:menu

On my Thinkpad (running Arch Linux), I prefer the PrtSc key, which is placed
between AltGr and Ctrl on the right hand side of the Space bar. I don't often
take screenshots, since the content of my screen is mostly text, which is
better captured using primary selection. (Did I already mention that I use
Arch, btw.?)

    setxkbmap us
    setxkbmap -option compose:prtsc

After restarting X, the compose key is ready for action. Now I can type `Menu &quot;
O` to enter _Ö_ on my PC (OpenBSD), or `PrtSc ' e` to enter _é_ on my Laptop
(Arch Linux). Not all together at the same time, but as a sequence, which is
much more convenient.  Pre-defined sequences can be looked up in the compose
files under `/usr/X11/share/X11/locale/[locale]/Compose` on OpenBSD, or
`/us/share/X11/locale/[locale]/Compose` on Arch Linux, respectively. As locale,
I simply use `en_US.UTF-8` (imperialism, remember?), which gives me a wide
range of sequences (hand-picked examples):

    &lt;Multi_key&gt; &lt;C&gt; &lt;o&gt; 			: &quot;©&quot;   copyright # COPYRIGHT SIGN
    &lt;Multi_key&gt; &lt;R&gt; &lt;o&gt; 			: &quot;®&quot;   registered # REGISTERED SIGN
    &lt;Multi_key&gt; &lt;plus&gt; &lt;minus&gt;     	: &quot;±&quot;   plusminus # PLUS-MINUS SIGN
    &lt;Multi_key&gt; &lt;s&gt; &lt;s&gt;            	: &quot;ß&quot;   ssharp # LATIN SMALL LETTER SHARP S
    &lt;Multi_key&gt; &lt;E&gt; &lt;equal&gt;        	: &quot;€&quot;   EuroSign # EURO SIGN
    &lt;Multi_key&gt; &lt;u&gt; &lt;slash&gt; 		: &quot;µ&quot;   mu # MICRO SIGN
    &lt;Multi_key&gt; &lt;quotedbl&gt; &lt;A&gt;     	: &quot;Ä&quot;   Adiaeresis # LATIN CAPITAL LETTER A WITH DIAERESIS
    &lt;Multi_key&gt; &lt;acute&gt; &lt;E&gt;        	: &quot;É&quot;   Eacute # LATIN CAPITAL LETTER E WITH ACUTE
    &lt;Multi_key&gt; &lt;asciitilde&gt; &lt;N&gt;   	: &quot;Ñ&quot;   Ntilde # LATIN CAPITAL LETTER N WITH TILDE
    &lt;Multi_key&gt; &lt;comma&gt; &lt;c&gt;        	: &quot;ç&quot;   ccedilla # LATIN SMALL LETTER C WITH CEDILLA
    &lt;Multi_key&gt; &lt;v&gt; &lt;z&gt; 			: &quot;ž&quot;   U017E # LATIN SMALL LETTER Z WITH CARON

Those sequences are very intuitive to type, so looking them up will hardly be
needed. This configuration also allows me to type in scientific
transliterations of Russian words, such as _Č_ in _Čechov_ (`Menu &lt; C`) instead
of the German _Tschechow_ or the English _Chekov_. Same with _š_ in _Puškin_
and _Ž_ in _Dr. Živago_.

If those sequences do not suffice for one's particular needs, more sequences can
be defined in the file `~/.XCompose`. When doing so, it is important to also
include the original definitions mentioned above as follows, so that it's not
needed to re-define them all:

    include &quot;%S/en_US.UTF-8/Compose&quot;

The `%S` resolves to the path `/usr/X11R6/share/X11/locale` on OpenBSD and
`/usr/share/X11/locale` on Arch Linux. (`%L` would resolve to the current
locale and, thus, would be shorter, but I prefer the more explicit way using
`%S`.) Additional rules can be defined as follows:

    &lt;Multi_key&gt; &lt;colon&gt; &lt;minus&gt; &lt;parenright&gt; : &quot;☺&quot; U263A # WHITE SMILING FACE

This allows to type the smiley using `Menu : - )` on my OpenBSD machine, and
`PrtSc : - )` on my Arch Linux laptop. The left hand side of the colon defines
the key sequence. The right hand side defines a key symbol (`keysym` in X
lingo), a string, or both, followed by an optional comment. It is also possible
to enter whole strings in that manner:

    &lt;Multi_key&gt; &lt;m&gt; &lt;a&gt; &lt;c&gt; : &quot;fanboy&quot;

Whose effect I leave for you to figure out.

# Update (2020-09-17)

A former [fellow student](https://github.com/chefe) just made me aware of the
keyboard variant `altgr-intl` for the US layout. It can be set as follows:

    $ setxkbmap us -variant altgr-intl

This layout variant offers some useful shortcuts, all to be entered in
combination with AltGr:

- AltGr + q: ä
- AltGr + y: ü
- AltGr + p: ö
- AltGr + e: é
- AltGr + r: ë
- AltGr + s: ß

Unfortunately, an e with grave accent is not easily available, but the
`altgr-intl` variant is still a good alternative for German.
</content>
    </entry>
    <entry>
        <title>OpenBSD on the Desktop (Part II)</title>
        <link href="https://paedubucher.ch/articles/2020-09-12-openbsd-on-the-desktop-part-ii.html"/>
        <author>
            <name>Patrick Bucher</name>
        </author>
        <id>https://paedubucher.ch/articles/2020-09-12-openbsd-on-the-desktop-part-ii.html</id>
        <updated>2020-09-12T15:00:00Z</updated>
        <content type="text/markdown; charset=UTF-8">
A week ago, I've installed [OpenBSD on my
Thinkpad](https://paedubucher.ch/articles/2020-09-05-openbsd-on-the-desktop-part-i.html).
I've been using it now and then, and already have changed a couple of things in
respect to the original setup described in the article. I also installed OpenBSD
on the Dell Optiplex on which I [previously installed
FreeBSD](https:///paedubucher.ch/articles/2020-08-11-freebsd-on-the-desktop.html)
a month before. This means that I'm no longer using FreeBSD on the desktop, at
least not for the moment. However, FreeBSD is running on a disk station I built
earlier this summer. Maybe I'll describe that particular setup (using ZFS) in a
later article.

Except for that storage server, I'd like to use OpenBSD for most of my private
computing. In this article, I describe some GUI tweaks and additional setup
tasks I perfmormed in order to feel more at home on my OpenBSD machines. Some of
the tasks performed are _not_ specific to OpenBSD, but could also be applied to
a Linux setup.

# doas

`sudo` originally came from the OpenBSD community. It is almost as widely used
in the Unix world as SSH, which is the most prominent OpenBSD project.  However,
`sudo` became bigger and harder to configure. Therefore, Ted Unangst came up
with a simpler alternative called `doas`, which stands for _Dedicated OpenBSD
Application Subexecutor_. `doas` is less powerful than `sudo`, but much smaller,
easier to configure, and, thus, more secure. The full rationale can be read in
[Ted Unangst's Blog](https://flak.tedunangst.com/post/doas).

A basic `doas` setup requires to login as root for one last time. The
configuration shall be kept extremely simple. I'd like to permit all users from
the `wheel` group (which is just me on my computers) to use `doas` without
entering the password every time but only once when executing a command that
requires `root` permissions. This is only a single line in `/etc/doas.conf`:

    permit persist :wheel    

Let's check this setup by logging in as a user of the wheel group and trying to
update the packages:

    $ doas pkg_add -u

This works, so bye bye `root` account.

# Fonts for dwm, dmenu, and st

By default, `dwm`, `dmenu`, and `st` use a monospace font of size 10, or
pixelsize 12, respectively, which is hard to read on a screen with a high
resolution. On Linux, I use the the TrueType font DejaVu Sans Mono. For OpenBSD,
I'd rather use something more minimalistic: the [Terminus bitmap
font](http://terminus-font.sourceforge.net/).

As `pkg_info -Q terminus` shows, this font comes in different versions. I prefer
the version with the centered tilde, which I install:

    $ doas pkg_add terminus-font-4.47p0-centered_tilde

Let's reconfigure `st` first, for testing changes doesn't require a restart of
the window manager. I stored my suckless sources in `~/suckless`, so the
font for `st` can be configured in `~/suckless/config.h`. I replace the existing
font configuration

    static char *font = &quot;Liberation Mono:pixelsize=12:antialias=true:autohint=true&quot;;

with

    static char *font = &quot;Terminus:pixelsize=24&quot;;

The options `antialias` and `autohinting` are not needed for a bitmap font, so I
left them away. 24 pixels is rather big, but my screen is big enough to show two
text editors with more than 80 characters per line next to each other, so let's
keep it this way. I rebuild and reinstall `st`, then switch to `dwm`:

    $ doas make install
    $ cd ../dwm

The font configuration in the `config.h` file looks a bit different here:

    static const char *fonts =      { &quot;monospace:size=10&quot; };
    static const char dmenufont =   &quot;monospace:size=10&quot;;

Let's just use the same font as for `st` here:

    static const char *fonts =      { &quot;Terminus:pixelsize=24&quot; };
    static const char dmenufont =   &quot;Terminus:pixelsize=24&quot;;

Note that I'm using `pixelsize` instead of `size` here. (24pt would be much
bigger than 24px.) Then I rebuild and reinstall `dwm`.

    # make install

This configuration appllies also to `dmenu` and `slstatus`, so we're done with
the fonts.

# X Background

By default, the desktop background is a pattern of black and grey dots, which is
a strain to the eye. Even though I rarely look at an empty desktop for long, I'd
rather change this to a solid color. This can be done by adding a command to
`~/.xinitrc`:

    xsetroot -solid black

Right before `dwm` is executed.

# USB Flash Drive

Even though SSH is almost ubiquitous nowadays, a USB flash drive is still useful
when it comes to exchanging data between computers, especially if Windows is
involved, or if the network does not allow SSH.

Block storage devices are accessible through the device nodes `/dev/sd*`,
whereas `*` stands for the number of the disk. The disks can be listed as
follows:

    $ sysctl hw.disknames
    hw.disknames=sd0:ef0268c97ae7a246

Only `sd0` is active, even though I already plugged in my USB dongle. However,
the system already figured out that there is a second disk:

    $ sysctl hw.diskcount
    hw.diskcount=2

The next free disk would have the name `sd1`. The device nodes can be created by
running the `MAKEDV` script in `/dev`:

    $ cd /dev
    $ doas sh MAKEDEV sd1

Let's initialize a new MBR partition schema on `sd1`:

    $ doas fdisk -iy sd1

The new disk layout can be checked using `disklabel`:

    $ doas disklabel sd1
    # /dev/rsd1c
    ...

The first line of the output tells us that there's a partition under
`/dev/rsd1c`. (The `r` refers to «raw», as opposed to «block».) The partition
can be formatted using `newfs` by referring to that partition name:

    $ doas newfs sd1c

This creates a default FFS (Fast File System) partition, which is useful to
exchange data between BSD operating systems. The formatted partition is then
ready to be mounted:

    $ doas mount /dev/sd1c /mnt

## Other Partition Types

Other partition types are available under other utilities.

### FAT32

The following command creates a FAT32 partition:

    $ doas newfs_msdos -F 32 sd1c

The `-F 32` parameter specifies FAT32 (as opposed to FAT16 or FAT8). To mount
the partition, use the according `mount` command:

    $ doas mount_msdos /dev/sd1c /mnt

### EXT2

In order to create an `ext2fs` file system, the partition type needs to be
specified accordingly. First, you might consider a GPT partition schema instead
of MBR (additional `-g` parameter):

    $ doas fdisk -igy sd1

Then use `disklabel` interactively to define a new partition:

    $ doas disklabel -E sd1

First, delete all the partitions with `z`. Then, create a new partition with
`a`, and make sure to specify the type as `ext2fs` instead of the default
`4.2BSD`. Notice that the new partition has a different letter (say, `a`), so
you need to use `sd1a` instead of `sd1c` for the next steps. Write the changes
by typing `w`, then exit with `q`. Now you can format and mount the partition:

    $ doas newfs_ext2fs sd1a
    $ doas mount_ext2fs /dev/sd1a /mnt

# SSH Key (GitHub)

In order to access my GitHub repositories, I first create a new SSH key:

    $ ssh-keygen -t rsa -b 4096

Since I manage my passwords with `pass` (of which more later), I don't know most
of them by heart. So I can't just login to GitHub and add my public key.
Therefore, I copy my public key to my laptop, on which I'm already logged in to
GitHub.

This can be either done using `scp`, for which `sshd` has to be running on my
laptop (which currently has the IP `192.168.178.53`):

    $ scp ~/.ssh/id_rsa.pub 192.168.178.53:/home/patrick

Or using the USB flash drive formatted with `ext2` from before:

    $ doas newfs_ext2fs -I sd1a
    $ doas mount_ext2fs /dev/sd1a /mnt
    $ doas cp ~/.ssh/id_rsa.pub /mnt/

Then `id_rsa.pub` can be copied into the according [GitHub Settings
Page](https://github.com/settings/ssh/new), after which cloning GitHub
repositories should work on the OpenBSD machine:

    $ git clone git@github.com:patrickbucher/conf

# GPG Key

My passwords are encrypted using GPG. To encrypt them, I need to copy my private
key from my other machine. First, I list my private keys:

    $ gpg --list-keys --keyid-format SHORT
    pub   rsa2048/73CE6620 2016-11-11 [SC]
          22F91EE20D641CBCF5B8678E82B7FE3A73CE6620
    uid         [ultimate] Patrick Bucher &lt;patrick.bucher@mailbox.org&gt;
    sub   rsa2048/AF6246E3 2016-11-11 [E]

Then I export both public and private key to an according file using the armored
key format:

    $ gpg --export --armor 73CE6620 &gt; public.key
    $ gpg --export-secret-key --armor 73CE6620 &gt; private.key

The two key files can be copied via SSH or the USB flash disk again, which I
won't show here.

Back on my OpenBSD machine, I need to install GnuPG first, because OpenBSD only
has `signify` installed by default:

    $ doas pkg_add gnupg

I pick the 2.2 version. Now I can import my keys:

    $ gpg2 --import private.key
    $ gpg2 --import public.key

The key is not trusted so far, so I need to change that:

    $ gpg2 --edit-key 73CE6620
    &gt; trust
    &gt; 5
    &gt; y
    &gt; quit

5 stands for ultimate trust, which seems appropriate.

# Password Manager

I use `pass` as a password manager, which can be installed as the
`password_store` package in OpenBSD:

    $ doas pkg_add password-store

Now that I have both my GPG private key and a working SSH key for GitHub, I can
clone my passwords stored on a private GitHub repository:

    $ git clone git@github.com:patrickbucher/pass .password-store

Now I can copy my GitHub password to the clipboard as follows:

    $ pass -c github

# Aliases

I use a lot of aliases, such as `gcl` as a shortcut for `git clone`, and `gad`
for `git add`, etc. Since OpenBSD uses a Public Domain Korn Shell by default,
the `.bashrc` configuration from my Linux machines won't work here, unless I
switch to `bash`, which is not exactly the point of using OpenBSD.

I define my aliases in `~/.kshrc` (excerpt):

    alias gcl='git clone'
    alias gad='git add'

In order to load those settings, an according `ENV` parameter needs to be
defined in `~/.profile` (see `man 1 ksh` for details):

    export ENV=$HOME/.kshrc

After the next login, `~/.profile` is reloaded, and the aliases are ready to be
used.

# Conclusion

Not only is my enhanced setup now ready to do some serious work, but I also
increased my understanding of some OpenBSD subjects. There are still things to
be improved and to be understood, but my setup is now good enough so that I no
longer need a Linux machine running next to it. I'm looking forward to use and
learn about OpenBSD in the time to come. I'll write additional articles on the
subject as soon as I have enough subject material ready.
</content>
    </entry>
    <entry>
        <title>OpenBSD on the Desktop (Part I)</title>
        <link href="https://paedubucher.ch/articles/2020-09-05-openbsd-on-the-desktop-part-i.html"/>
        <author>
            <name>Patrick Bucher</name>
        </author>
        <id>https://paedubucher.ch/articles/2020-09-05-openbsd-on-the-desktop-part-i.html</id>
        <updated>2020-09-05T20:00:00Z</updated>
        <content type="text/markdown; charset=UTF-8">
Let's install OpenBSD on a Lenovo Thinkpad X270. I used this computer for my
computer science studies. It has both Arch Linux and Windows 10 installed as
dual boot. Now that I'm no longer required to run Windows, I can ditch the dual
boot and install an operating system of my choice.

# Preparation

First, I grab my work Thinkpad running Arch Linux and some USB dongle big enough
for the [amd64 miniroot
image](https://cdn.openbsd.org/pub/OpenBSD/6.7/amd64/miniroot67.fs) (roughly
five megabytes, that is). This small image does not include the file sets, which
will be downloaded during installation instead. I also download the [SHA256
checksums](https://mirror.ungleich.ch/pub/OpenBSD/6.7/amd64/SHA256) from the
Swiss mirror, and verify the downloaded image, before I copy it on my dongle:

    $ sha256sum -c --ignore-missing SHA256 
    miniroot67.fs: OK
    $ sudo dd if=miniroot67.fs of=/dev/sda bs=1M

# Installation

The Thinkpad X270 is connected to my network through Ethernet. The WiFi firmware
usually needs to be installed separately, so only Ethernet will work out of the
box. The BIOS has UEFI activated. OpenBSD and UEFI has issues on older hardware
(at least on a 2014 Dell laptop I have), but let's try it on this laptop,
anyway.

I plug in the dongle prepared before, and start the computer. I interrupt
the regular boot with Enter and pick an alternative boot method by pressing F12.
Now I pick my USB dongle. After roughly a minute, the installer has been
started. Now I follow these steps:

- I choose the option `I` to install OpenBSD.
- For the keyboard layout, I pick `sg`, for Swiss German.
- As a hostname, I simply pick `x270`, because it's a Thinkpad X270, and I'm not
  very creative when it comes to naming things.
- From the available network options (`iwm0`: WiFi, `em0`: Ethernet, and
  `vlan0`: Virtual LAN), I pick `em0`.
- I try to get an IPv4 address over DHCP, which seems to work very quickly.
- Next, I type in my very secret root password twice.
- I do _not_ start `sshd` by default, because I don't need to connect to this
  machine through SSH. It's supposed to be a workstation, not a server.
- The X Window System should not be started by `xnodm(1)`, so I leave it to
  `no`.
- Neither do I want to change the default to `com0`.
- I set up my user `patrick` with my proper name `Patrick Bucher`, and a decent
  password.
- The time zone has been detected properly as `Europe/Zurich`, which I just
  leave the way it is.
- The installer detected two disks: `sd0` and `sd1`. Since `sd0` is the detected
  SSD in my laptop, the UEFI issue from my Dell laptop doesn't exist on this
  computer. I pick `sd0` for the root disk, since `sd1` is my USB dongle.
- I choose to use the whole disk with a GPT partitioning schema, because it's
  2020.
- An auto-allocated layout for `sd0` is presented. It looks decent to me, so I
  just go with that auto layout.
- I don't want to initialize another disk, so I just press Enter (`done`).
- Since the miniroot image does not come with the file sets, I pick `http` as
  the location for the sets.
- I don't use a proxy, and use the mirror `mirrog.ungleich.ch` and the server
  directory `pub/OpenBSD/6.7/amd64` as proposed.
- Next, I unselect the game sets by entering `-game*`. (I heard that they're not
  much fun to play.) I leave all the other sets activated, including the `x`
  sets, which will be required for the GUI later on.
- After those sets are installed, I press Enter (`done`). Now the installer
  performs various tasks, after which I choose to `halt` the computer. This
  gives me time to remove the USB dongle.

# First Boot

I now restart my laptop, and OpenBSD boots. This takes more time than booting
Arch Linux, which uses `systemd`, whereas OpenBSD uses `rc`, which performs the
startup tasks sequentially.

There's a message showing up that various firmware (`intel-firmware`,
`iwm-firmware`, `inteldrm-firmware`, `uvideo-firmware`, and `vmm-firmware`) has
been installed automatically. Very nice, indeed.

## WiFi Connection

Now that the `iwm-firmware` has been installed, I can connect right away to my
WiFi network `frzbxpdb5`. I create a file called `/etc/hostname.iwm0`, wich
`hostname` being a literal string, and `iwm0` being the WiFi network card. The
connection to my WiFi network consists of a single line:

    dhcp nwid frzbxpdb5 wpakey [my-wpakey]

Whereas `frzbxpdb5` is my WiFi network's ESSID, and `[my-wpakey]` needs to be
replaced by the actual WPA key.

Then the networking can be restarted for that device:

    # sh /etc/netstart iwm0

This script is kind enough to set the file permissions of `/etc/hostname.iwm0`
to `640`, and then connects to my WiFi network.

I unplug the Ethernet cable and `ping openbsd.org`, which works fine, even after
a restart.

# Installing the GUI

My GUI on Unix-like systems is based on the Dynamic Window Manager (`dwm`) and a
couple of other tools, such as `dmenu`, `st`, `slstatus`, `slock`, all created and
maintained by the [Suckless](http://suckless.org/) community.

This software doesn't come with configuration facilities, but needs to be
configured in the respective C header file `config.h`, and then re-compiled.
Even though OpenBSD offers `dwm` as a package, customizing and configuring that
window manager requires to build it from source.

## Building `dwm` and Friends

First, I need to install `git` to fetch the source code:

    # pkg_add git

Then I fetch the source code for `dwm`, `dmenu`, `st`, and `slstatus` from [Suckless](http://suckless.org/):

    $ git clone https://git.suckless.org/dwm
    $ git clone https://git.suckless.org/dmenu
    $ git clone https://git.suckless.org/st
    $ git clone https://git.suckless.org/slstatus

### Building `dwm`

Next, I try to build `dwm`:

    $ cd dwm
    $ make

This fails with an error message (`'ft2build.h' file not found`), which reminds
me of building `dwm` on FreeBSD roughly a month before. Since I can finde the
header file at another location:

    # find / -type f -name ft2build.h
    /usr/X11R6/include/freetype2/ft2build.h

I simply can modify the `config.mk` accordingly by changing

    FREETYPEINC = /usr/include/freetype2

to

    FREETYPEINC = $(X11INC}/freetype2

Actually, I only need to comment the above line, and uncomment the line below

    # OpenBSD (uncomment)

The Suckless folks obviously are friendly towards OpenBSD, which is also
noticable in other places (more evidence to be shown further below).

The next compilation attempt succeeds:

    $ make

So let's install `dwm`, too:

    # make install

By default, and as to be seen in `config.h`, the keyboard combination
`[Alt]+[Shift]+[Enter]` (deeply engraved into the muscle memories of many `dwm`
users) starts the `st` terminal. This will be built in a while. However, I
prefer to use the _Super_ or _Windows_ key instead of `Alt`, since the former
is of no use in OpenBSD, and the latter still comes in handy when working with
the emacs readline mode. Therefore, I change the `MODKEY` from

    #define MODKEY Mod1Mask

to

    #define MODKEY Mod4Mask

Then I rebuild and reinstall `dwm`:

    # make install

### Building `st`

Let's switch over to the `st` source directory and just try to compile it:

    $ cd ../st
    $ make

Here, we get a warning that the function `pledge` (an OpenBSD mitigation, which
is built into the `master` branch, but surrounded by an `ifdef` preprocessor
statement, so that it will only be compiled for OpenBSD) is imported implicitly.
Let's just ignore this warning for now.

What's worse, the compilation fails with the error message:

    ld: error: unable to find library -lrt

Here, the FAQ comes in handy, stating that

    If you want to compile st for OpenBSD you have to remove -lrt from
    config.mk, ...

Having done so in `config.mk`, `st` compiles without any further issues, and,
thus, can be rebuilt and installed:

    # make install

### Building `dmenu`

Even OpenBSD users with Suckless tools have to open another GUI application than
a terminal emulator once in a while. For this purpose, Suckless offers `dmenu`.
Let's switch over to it and compile it:

    $ cd ../dmenu
    $ make

Again, we have the issue with `ft2build.h`, which can be resolved as above with
`dwm`: by using the proper path for `FREETYPEINC` in `config.mk`. Afterwards,
the build succeeds, and `dmenu` can be installed:

    # make install

### Building `slstatus`

`dwm` has a status bar on the top right, which can be used to show various
information. I used to write some shell commands in `.xinitrc` to compose such a
status line, and then set it by `xset -b` once every five seconds or so. This
approach generates a multitude of processes every couple of seconds.

`slstatus` is a C programm that is capable of showing various kinds of more or
less useful information. Let's switch over to `slstatus` and see, what is
available in `config.def.h`:

    $ cd ../slstatus
    $ less config.def.h

The comments section lists different functions (`battery_perc` for the battery
percentage, `datetime` for date and time information, `temp` for thermal
information, etc.). I usually display the CPU load, the battery percentage, the
memory usage, the current keyboard layout, and the current date and time.

Before configuring those, let's try to compile `slstatus`:

    $ make

This worked fine, so let's configure the information to be displayed in
`config.h`:

    static const struct arg args[] = {
        /* function    format    argument */
        { datetime,    &quot;%s&quot;,     &quot;%F %T&quot; },
    };

This renders the current date as follows:

    $ date +&quot;%F %T&quot;
    2020-09-05 19:26:38

I also like to have the weekday included, but not the seconds, so I define a
different argument string:

    $ date +&quot;%a %Y-%m-%d %H:%M&quot;
    Sat 2020-09-05 19:27

That's better, so let's use it in `config.h` (surrounded with some spaces in the
format string):

    static const struct arg args[] = {
        /* function    format    argument */
        { datetime,    &quot; %s &quot;,   &quot;%a %Y-%m-%d %H:%M&quot; },
    };

The other settings I like to have do not require any arguments, at least not on
OpenBSD, so I only need to define a decent format string (with `|` as a
seperator) for those:

    static const struct arg args[] = {
        /* function    format           argument */
        { cpu_perc,     &quot; cpu: %s%% |&quot;, NULL },
        { battery_perc, &quot; bat: %s%% |&quot;, NULL },
        { ram_used,     &quot; mem: %s |&quot;,   NULL },
        { keymap,       &quot; %s |&quot;         NULL },
        { datetime,     &quot; %s &quot;,         &quot;%a %Y-%m-%d %H:%M&quot; },
    };

This actually compiles, so let's install it:

    # make install

## Configuring X Startup

Now that all software is compiled and installed, let's run X. To do so, a file
`.xinitrc` in the user's directory is required (`/home/patrick/.xinitrc`):

    setxkbmap ch
    slstatus &amp;
    exec dwm

This sets the keyboard map for X to Swiss German, starts `slstatus` in the
background, and then executes `dwm`.

X can now be started by typing `startx`. This is a bit cumbersome to type every
time, so let's define a symbolic link to it:

    # ln -s &quot;$(which startx)&quot; /usr/bin/x

Now let's start X:

    $ x

If everything was configured properly, `dwm` shows up, and the status line says
that the whole system only uses roughly 60 megabytes of RAM. That's slim. The
keyboard combinations to open `st` and `dmenu` work, too.

# Conclusion

Installing a basic GUI with Suckless software was a rather smooth experience on
OpenBSD. (For FreeBSD, I deliberately have chosen a rather fine-grained approach
to installing X packages, which caused some additional work.) However, various
settings require additional tweaking. I also didn't use audio yet, which require
the volume buttons to be configured accordingly in `dwm`.

I'll also need to setup `sudo` or `doas`. As a regular Linux user, I'm used to
`sudo`, of course, but the simplicity of `doas` is a good argument to try it as
an alternative.

But those are things I'd like to cover in an upcoming article.
</content>
    </entry>
    <entry>
        <title>FreeBSD on the Desktop</title>
        <link href="https://paedubucher.ch/articles/2020-08-11-freebsd-on-the-desktop.html"/>
        <author>
            <name>Patrick Bucher</name>
        </author>
        <id>https://paedubucher.ch/articles/2020-08-11-freebsd-on-the-desktop.html</id>
        <updated>2020-08-11T22:00:00Z</updated>
        <content type="text/markdown; charset=UTF-8">
I'm a happy user of [Arch Linux](https://www.archlinux.org/) both on my private
computers and on my work laptop. I even managed to get through four years of
university with my setup, and only had to bring a Windows machine on some rare
occasions, even though some professors are openly hostile towards a Linux setup.
(It doesn't run Microsoft Project and the real Excel, after all…)

Recently, I got interested in the BSDs, especially in
[OpenBSD](https://www.openbsd.org/) and in [FreeBSD](https://www.freebsd.org/).
Even though OpenBSD with its minimalistic appeal is better suited to my taste,
I'm currently looking at FreeBSD for a couple of reasons. First, I have to
maintain a storage server (a FreeBSD box using ZFS) at work. Second, I've also
built up a storage server at home. FreeBSD gives me the things I need mostly out
of the box: ZFS with redundancy on really cheap hardware. And third, I just like
to learn new things.

# Why FreeBSD?

However, when it comes to learning new things in my spare time, I'd rather spend
my time on something that will be useful in the long run. I use the [Lindy
Effect](https://en.wikipedia.org/wiki/Lindy_effect) as a guide: Technologies
like Kubernetes, the latest JavaScript framework, or Web Assembly have only been
around for a couple of years, and it's possible that those will undergo major
changes or vanish alltogether as fast as they came. Older technologies _that are
still around nowadays_, on the other hand, can be expected to be around for many
more years. Examples are the C programming language, various Unix shells, and ‒
FreeBSD. (I also make exceptions to this rule now and then. For example, I
learned Go and Rust in the summers of 2018 and 2019, respectively. Go, which had
its 1.0 release earlier than Rust, proofed to be the more stable choice.)

FreeBSD is now more than 25 years old. Its roots, however, go back to AT&amp;T's
original Unix from the 1970s. (All the code has been replaced or rewritten
since.) Being old is not enough, of course; a technology is only worthwhile
learning if it is still alive. Even though FreeBSD is rarely on the front page
of [Hacker News](https://news.ycombinator.com/), it is still widely used.
[Netflix](https://papers.freebsd.org/2019/fosdem/looney-netflix_and_freebsd/)
streams videos through FreeBSD systems, and the operating system of the
PlayStation 4, [Orbis
OS](https://en.wikipedia.org/wiki/PlayStation_4_system_software), is based on
FreeBSD.

FreeBSD is not only likely to stay around for a long time, it probably also
won't undergo fundmental changes very soon or very often. One example is the
startup system of FreeBSD, which is still based on `init` and `rc` scripts.
Ubuntu, on the other side, switched from SysVinit to Upstart in 2006, and again
from Upstart to systemd in 2015. That is one init system to learn for a lifetime
(FreeBSD) vs. three in less than a decade (Ubuntu).

Documentation is another advantage of FreeBSD. Having a system that rarely
introduces breaking changes makes it easier and more worthwile to provide good
documentation. The FreeBSD team not only provides good [manual
pages](https://www.freebsd.org/cgi/man.cgi), but also a well curated
[FAQs](https://www.freebsd.org/doc/en_US.ISO8859-1/books/faq/), a
[Wiki](https://wiki.freebsd.org/), and the very useful
[Handbook](https://www.freebsd.org/doc/en_US.ISO8859-1/books/handbook/).
FreeBSD material has such a long shelf life so that it is even worthwhile to
print books about that operating system. I'm reading [Absolute
FreeBSD](https://nostarch.com/absfreebsd3) now (from front to back, that is).
Michael Warren Lucas, the author of this book, has written [even more
books](https://www.tiltedwindmillpress.com/product-category/tech/) on FreeBSD
(and OpenBSD), which are not only useful and of high-quality, but also
well-written and fun to read.

As I started to work with FreeBSD, I suddenly realized what the term
«distribution» is probably supposed to mean: Not just a bunch of software
cobbled together with more or less frequent upgrades, but an entire operating
system that not only provides working software, but also the means to build that
very software on a standard installation. (Such a system is technically
described as _self hosting_.) A kernel can be compiled and installed with a
single command. Thanks to the ports tree, the packages can be compiled easily
and in a consistent way. I haven't tried _all_ Linux distributions, of course,
but quality standards that high are certainly not the rule in the GNU/Linux
world. (Debian and Arch, the Linux distributions I use the most and know the
best, are still absolutely great operating systems.)

# FreeBSD on the Desktop?

It's safe to say that FreeBSD is a good choice for servers in a Unix
environment. But does it also work well on a desktop computer? And is it even an
option for the kind of desktop I like to run: not GNOE, KDE, or Xfce, but a
minimalistic setup based on [dwm](http://dwm.suckless.org/), which probably
isn't used by many. Since dwm can only be configured by modifying the `config.h`
file, I won't be able to use the version from the ports tree. I use my desktop
computer mainly for work (programming, reading, writing, researching information
on the internet) and some entertainment mostly provided through the web browser
(reading, videos).

I generally use mid-range commodity hardware with on-board graphics, so hardware
compatibility should not be an issue. My desktop computer is a small-form Dell
OptiPlex 7040 from 2016 with an Intel Core i5 CPU. I replaced the original 128
TB SSD with a 500 GB model last year, and upgraded the original 8 GB of memory
with a ridiculous amount of 24 GB. (Some RAM bars just happened to lay around
here…) The computer has a WiFi card, of course, but since my router is just next
to my desk, I rather use a stable ethernet connection.

I already tried out OpenBSD once on that computer, and didn't have any issues
getting it to run. So FreeBSD having access to the same code base under a
compatible license is likely to suppport this hardware as well. Let's just try
it out!

# Preparations

FreeBSD supports multiple versions at any given point in time. At the time of
this writing, 12.1 and 11.4 are the current versions intended for production.
Let's pick the most recent version 12.1. The [download
page](https://download.freebsd.org/ftp/releases/amd64/amd64/ISO-IMAGES/12.1/)
for the `amd64` architecture offers various options. The compressed
`mini-memstick` archive weighs the least and provides everything that is needed
for an installation on a computer with internet connection. I download it to my
laptop running Arch Linux, and then verify the checksum:

    $ wget https://download.freebsd.org/ftp/releases/amd64/amd64\
    /ISO-IMAGES/12.1/FreeBSD-12.1-RELEASE-amd64-mini-memstick.img.xz
    $ wget https://download.freebsd.org/ftp/releases/amd64/amd64/\
    ISO-IMAGES/12.1/CHECKSUM.SHA512-FreeBSD-12.1-RELEASE-amd64
    $ sha512sum -c CHECKSUM.SHA512-FreeBSD-12.1-RELEASE-amd64 --ignore-missing
    FreeBSD-12.1-RELEASE-amd64-mini-memstick.img.xz: OK

The archive (389 MBs) needs to be unpacked and is then copied to a USB dongle
(`/dev/sda`):

    $ unxz FreeBSD-12.1-RELEASE-amd64-mini-memstick.img.xz
    # dd if=FreeBSD-12.1-RELEASE-amd64-mini-memstick.img of=/dev/sda bs=1M
    $ sync

# Initial Setup

The BIOS is setup to use UEFI rather than legacy boot. I plug in the USB dongle
and start the FreeBSD installer. These are the settings I use during setup:

- Keymap: Swiss-German (`ch.kdb`)
- Hostname: `optiplex` (I'm not very imaginative when it comes to naming
  things.)
- Components: just `lib32`, `ports`, and `src`
- Network: interface `em0` with DHCP and IPv4
- Mirror: Main Site
- Partitioning: Auto (UFS, I'm going to learn about ZFS later) on the entire
  disk `ada0` using GPT and the following partitions (device, space, type,
  label, mount point):
    - `ada0p1`: 200 MB `efi` boot (no mount point)
    - `ada0p2`: 24 GB `freebsd-swap` swap0 (no mount point, size of physical memory)
    - `ada0p3`: 16 GB `freebsd-ufs` root `/`
    - `ada0p4`: 4 GB `freebsd-ufs` temp `/tmp`
    - `ada0p5`: 4 GB `freebsd-ufs` var `/var`
    - `ada0p6`: 32 GB `freebsd-ufs` usr `/usr`
    - `ada0p7`: 386 GB `freebsd-ufs` home `/home` (remainder of the space)
- Root password: I won't tell you, but a strong one!
- CMOS clock: UTC
- Time Zone: Europe/Switzerland (`CEST`)
- Services: `sshd`, `moused`, `ntpd`, `powerd`, and `dumpdev`
- Security Hardening Options: everything
- User: `patrick` with additional group `wheel` (to become root), the `tcsh`
  shell, a strong password, and, otherwise, suggested settings

It can be argued if the chosen partition sizes are reasonable. However, it is
always a good idea to use separate `/tmp` and `/var` partitions to make sure
that no process can fill up the entire disk. (Using a separate `/usr` partition
is an issue on Linux nowadays, since the widely used init system systemd
requires access to `/usr`. On FreeBSD, it is still possible to do so without any
issues.)

Make sure to use `efi` as the type for the boot partition, not `freebsd-boot` as
suggested in _Absolute FreeBSD_ (3rd Edition on page 36).

# First Boot, First Issues

After the installation, I choose to shutdown the system. I unplug my USB dongle
as soon as the screen turns black.

Before the first boot, I have to change my boot options in the BIOS so that the
computer boots from the SSD on which FreeBSD was just installed.

The system boots and even shows my mouse on the terminal! The network is up and
running. However, there is a message warning me that the leapsecond file is
expired. The
[solution](https://forums.FreeBSD.org/threads/leapseconds-file-expired.56645/post-322290) suggested in the FreeBSD Forum

    # service ntpd onefetch

fails with a certificate verification error. A [bug
report](https://bugs.freebsd.org/bugzilla/show_bug.cgi?id=230017) suggest to
install the package `ca_root_nss`:

    # pkg install ca_root_nss

Which not only installs the package management software, but also solves the
issue above: The warning concerning the leapsecond file doesn't appear after the
next boot. Now that the basic system is up and running, let's tackle the GUI!

# Installing the GUI

Since _Absolute FreeBSD_ doesn't cover graphical user interfaces, I have to
resort to the handbook. In [Chapter
5.3](https://www.freebsd.org/doc/handbook/x-install.html) it says that the
easiest way to setup the X Window System is to install the `xorg` package. Since
I prefer a minimalistic setup, I opt for the `x11/xorg-minimal` package instead:

    # pkg install x11/xorg-minimal

This package depends on Python 3.7, Perl 5, and Wayland, among others, and
weighs roughly 1 GB, which is not exactly minimalistic in my opinion. On the
other hand, it is notable that the base setup works without Perl or Python.
(Which _is_ minimalistic.)

## Compiling `dwm`

Since I like to keep my `dwm` version up to date, I fetch the sources using
`git`, which first needs to be installed:

    # pkg install git
    # git clone https://git.suckless.org/dwm

A first naive compilation attempt fails:

    # cd dwm
    # make
    drw.c:5:10: fatal error: 'X11/Xlib.h' file not found
    #include &lt;X11/Xlib.h&gt;

The `config.mk` file expects the header files to be located under
`/usr/X11R6/include`. However, FreeBSD has those files stored under a different
location:

    # find / -type f -name Xlib.h
    /usr/local/include/X11/Xlib.h

So in `config.mk`, the lines

    X11INC = /usr/X11R6/include
    X11LIB = /usr/X11R6/lib

need to be replaced with

    X11INC = /usr/local/include
    X11LIB = /usr/local/lib

The next compilation fails with another error (different error message, yay!):

    # make
    drw.c:6:10: fatal error: 'X11/Xft/Xft.h' file not found

That's the price I have to pay for minimalism, I guess. Executing `pkg search
Xft` reveals the package `libXft`, which I install:

    # pkg install libXft

This shows to be a good idea, because now I'm getting a different error message:

    # make
    Xft.h:39:10: fatal error: 'ft2build.h' file not found

It turns out that the file is on the system, but cannot be found:

    # find / -type f -name ft2build.h
    /usr/local/include/freetype2/ft2build.h

Again, the `local` path segment is missing in `config.mk`:

    FREETYPEINC = /usr/include/freetype2

Which is changed as follows:

    FREETYPEINC = /usr/local/include/freetype2

Retry, fresh error again:

    # make
    dwm.c:40:10: fatal error: 'X11/extensions/Xinerama.h' file not found

The `config.mk` contains the following section:

    # Xinerama, comment if you don't want it
    XINERAMALIBS  = -lXinerama
    XINERAMAFLAGS = -DXINERAMA

So what is _Xinerama_, after all? According to
[Wikipedia](https://en.wikipedia.org/wiki/Xinerama):

&gt; Xinerama is an extension to the X Window System that enables X
&gt; applications and window managers to use two or more physical displays as
&gt; one large virtual display. 

Since I have only one screen, I can do without Xinerama, so I comment out those
lines:

    # Xinerama, comment if you don't want it
    # XINERAMALIBS  = -lXinerama
    # XINERAMAFLAGS = -DXINERAMA

Now `dwm` compiles, and I can install it:

    # make dwm
    # make install

## Starting Xorg with `dwm`

I switch to my personal account and create the file `/home/patrick/.xinitrc`
with the following content:

    exec dwm

Now I run `startx`, which unfortunately fails:

    Fatal server error:
    (EE) no screens found(EE)

The error log `/var/log/Xorg.0.log` does not offer any additional information
that seems helpful to me. It turns out that `/etc/X11` is empty. [Section
5.4](https://www.freebsd.org/doc/handbook/x-config.html) of the handbook is
about Xorg configuration. I create a minimalistic configuration for my graphics
card (onboard Intel GPU) in `/etc/X11/xorg.conf`:

    Section &quot;Device&quot;
        Identifier &quot;Card0&quot;
        Driver     &quot;intel&quot;
    EndSection

I also need to install the display driver with the matching kernel module,
because my choice of `xorg-minimal` from before.

    # pkg install xf86-video-intel drm-kmod

(Note that «drm» doesn't stand for «digital rights management» in this context,
but for «direct rendering modules».) The kernel module can be activated on
startup by adding it to the `rc.conf` as follows:

    # echo 'lkd_list=&quot;/boot/modules/i915kms.ko&quot;' &gt;&gt; /etc/rc.conf

After a restart, the console is shown in a much higher resolution. However,
`startx` now complains about a missing font. Let's install the `xorg-fonts` meta
package, which should provide a monospace font needed for `dwm`:

    # pkg install xorg-fonts

Now, finally, `dwm` works! Since `startx` is long to type, I define the alias
`x` for it in `~/.cshrc`:

    alias x startx

And start `dwm`:

    $ x

## Configure `dwm`

By default, `dwm` uses the Alt key as the modifier key (`MODKEY`). I prefer to
use the «Windows» or «Super» Key, for it has no other purpose on my system.
(`Alt` is useful for some emacs-style readline commands.) To do this, the
`MODKEY` variable has to be changed in `config.h` as follows:

    #define MODKEY Mod4Mask

The default rules make Firefox appear on the last tag, and Gimp to be used with
floating layout, which makes no sense with more recent versions of Gimp. Let's
just undefine those rules:

    static const Rule rules[] = {
        {NULL, NULL, NULL, 0, 0, -1},
    };

I also like my windows to be split evenly:

    static const float mact = 0.50;

As a terminal, let's use `qterminal` instead of `st`, for the latter does not
support scrollback buffers:

    static const char *termcmd[] = {&quot;qterminal&quot;, NULL};

`qterminal` and `dmenu` need to be installed:

    # pkg install qterminal dmenu

## Status Line

`dwm` can display status information using the `xsetroot` command. The text to
be displayed is computed in a background task that can be defined in `.xinitrc`.
On laptops, I usually print the battery status. On desktops, the current date
and time suffices. Here's the `.xinitrc` that displays this information
(surrounded by spaces) in five second intervals:

    while true
    do
        xsetroot -name &quot; $(date +'%Y-%m-%d %H:%M') &quot;
    done &amp;
    setxkbmap ch
    exec dwm

The keymap is also set to the `ch` (i.e. Swiss German) variant just before
executing `dwm`. The `xsetroot` and `setxkbmap` utilities need to be installed
for this:

    # pkg install xsetroot setxkbmap

## Volume Control

In order to test audio, let's download the Free Software Song:

    $ curl https://www.gnu.org/music/free-software-song.ogg &gt; fss.ogg

I prefer `mplayer`, which needs to be installed:

    # pkg install mplayer

Make sure to include `/usr/local/bin` in your `$PATH` variable in order to run
`mplayer` without further path specification (`.cshrc`):

    export PATH=&quot;$PATH:/usr/local/bin&quot;

Playing the song as follows works if I plug in a headphone into one of the front
audio sockets:

    $ mplayer fss.ogg

The devices are listed in `/dev/sndstat` and switched by setting the respective
device number:

    # sysctl hw.snd.dfault_unit=1

The default volume is set to 85, which is quite loud for Richard Stallman's
singing voice. The volume can be changed relatively or absolutely using the
`mixer` command:

    $ mixer vol -10
    Setting the mixer from 85:85 to 75:75
    $ mixer vol 50
    Setting the mixer from 75:75 to 50:50

I don't always want to type that command, but rather use the volume keys on my
keyboard. So let's add a couple of commands to the `dwm` config (`config.h`,
just before the `keys[]` section):

    static const char *upvol[] = {&quot;mixer&quot;, &quot;vol&quot;, &quot;+5&quot;});
    static const char *downvol[] = {&quot;mixer&quot;, &quot;vol&quot;, &quot;-5&quot;});

For the key mapping, I first need to figure out the key codes for my volume
keys, which can be done using `xev`:

    # pkg install xev
    $ xev &gt; xev.out

Just press the volume up and volume up button in that order. Then close the
`xev` window and inspect `xev.out`.

**Unfortunately, the volume keys do not trigger an event.** There must be
something wrong with the keyboard configuration. So let's use Page Up and Page
Down to increase and decrese the volume (`keys[]` array in `config.h`):

    static Key keys[] = {
        // lines omitted
        { MODKEY, XK_Page_Up,   spawn, {.v = upvol}   },
        { MODKEY, XK_Page_Down, spawn, {.v = downvol} },
    };

Then simply re-compile, re-install, and re-start `dwm`:

    # make install
    $ x

Now Richard Stallman can be made to sing louder or quieter by pressing
Super+PgUp and Super+PgDown, respectively, _which is goood, hackers, which is,
goo-oo-ood!_

# Conclusion

Setting up the FreeBSD base system was rather easy. I made the mistake of using
`freebsd-boot` and not `efi` as the partition type for the boot partition, which
seems to be a mistake in the otherwise amazing book _Absolute FreeBSD_.

Installing the `x11/xorg-minimal` package instead of the full `xorg` package
caused some additional trouble, but helped me to better understand which
components are actually required to compile and run `dwm`. Instead of just
installing Xinerama, as I always did on Linux, the extra pain of libraries not
found made me investigate if I actually need that component. It turned out, I
don't.

I also needed to install the graphics driver and according kernel module
manually. Doing so, I realized that FreeBSD offers a nice graphical console,
which is a good fit for a `tmux` environment I use once in a while to work
absolutely focused.

Having audio running (almost) out of the box was a positive surprise. The
`mixer` interface is very simplistic. Switching audio devices, however, requires
an option to be changed using `sysctl`. This calls for some additional `dwm`
shortcuts!

My keyboard (a Cherry board with MX Brown switches) doesn't work properly out of
the box. I read about `uhidd`, which could be used to fix my issue with the
volume keys. But for the moment, I have a working setup.

I'll come back to the open issues in a later article. But first, I'd like to
work with my new FreeBSD desktop as much as possible to gain more experience.
</content>
    </entry>
    <entry>
        <title>«Four in a Row» in Haskell (Part II)</title>
        <link href="https://paedubucher.ch/articles/2020-08-05-four-in-a-row-in-haskell-part-ii.html"/>
        <author>
            <name>Patrick Bucher</name>
        </author>
        <id>https://paedubucher.ch/articles/2020-08-05-four-in-a-row-in-haskell-part-ii.html</id>
        <updated>2020-08-05T23:00:00Z</updated>
        <content type="text/markdown; charset=UTF-8">
In my [last
article](https://paedubucher.ch/articles/2020-08-03-four-in-a-row-in-haskell-part-i.html),
I outlined the purpose of a _stock program_: a non-trivial coding exercise to
be done in every new programming language somebody is learning. I also stated
that «Four in a Row» is becoming my personal stock program, and that I'd like to
implement it in Haskell.

The main challenge in Haskell is the functional programming paradigm.
Immutability is the main difference between an implementation of «Four in a
Row» in a functional programming language compared to rather structured
programming languages such as C or Python. The object-oriented aspect of an
implementation in Python makes hardly any difference, for OOP equally allows for
mutable an immutable programming beneath the surface. (In introductory courses
on OOP, hidden mutability is rather praised as a virtue than frowned upon; the
disadvantages of mutability are only taught in advanced courses by showing the
advantages of constructs like immutable classes. Learn and unlearn, but I'm
digressing…)

A later re-implementation of my stock program in Python might profit from the
experiences made in Haskell. Structured programming also allows for
immutability, and list comprehensions allow for compact code to produce new
state based on older state, without modifying existing state. (This
re-implementation could be subject matter for a fourth article, but let's not
get ahead of ourselves.)

In this article, I'm going to show how the board logic for the game «Four in a
Row» can be implemented in Haskell.

# Let There Be Code

As analyzed in my previous article, the board logic consists of five building blocks:

1. Create an empty grid with given dimensions.
2. Validate if a move (i.e. the choice of a column) is allowed for a given board.
3. Set a player's stone in the right place on the grid based on the choice of a column.
4. Detect if a player has won the game by checking if four of the player's
   stones lay in a horizontal, vertical, or diagonal line.
5. Format the grid as a string in order to display it on the command line.

The last building block, formatting, won't be covered in this article. I first
have to learn more about strings, formatting, and IO in Haskell, but I don't
like to wait to cover the other parts, which I'm already capable of
implementing with my current knowledge.

## Type Glossary

Before implementing the actual logic, let's define a couple of type aliases:

    type Grid = [Row]
    type Row = [Int]
    type Col = [Int]
    type Stone = Int

A grid (type `Grid`) is a list of rows. A row (type `Row`) itself is a list of
integers. As discussed in my previous article, 0 is going to be used for empty
fields. The fields occupied by player one and two shall be represented by the
numbers 1 and 2, respectively.

Just like a row, a column (type `Col`) is a list of integers. It is an
alternative way to express the relationships between individual fields. The
`Grid`, however, uses the `Row` type as its building blocks.

A `Stone` is an integer, too. It represents a player's number for fields
occupied by his or her stones.

Those types won't add powerful abstractions to the program, but make the
signature of certain functions a bit clearer. (It's also possible to limit the
scope of the types declared to certain values, but let's focus on the program
logic instead.)

## Creating a Grid

The function `new_grid` accepts two integer parameters (number of rows and
columns), and produces a grid of those dimensions:

    new_grid :: Int -&gt; Int -&gt; Grid
    new_grid r c = [new_row c | _ &lt;- [1..r]]

A list comprehension is used to build up the grid as a list of `r` rows. The
row itself is created by a function `new_row`:

    new_row :: Int -&gt; Row
    new_row c = [0 | _ &lt;- [1..c]]

Again, a list comprehension is used to build a single row consisting of `c`
elements: one per column.

The `new_grid` function can be used as follows (`&gt;` indicates the REPL, the
output has been wrapped for better readability):

    &gt; new_grid 6 7
    [[0,0,0,0,0,0,0],
     [0,0,0,0,0,0,0],
     [0,0,0,0,0,0,0],
     [0,0,0,0,0,0,0],
     [0,0,0,0,0,0,0],
     [0,0,0,0,0,0,0]]

## Validating a Move

A move solely consists of a column index. Let's assume a 6x7 grid (6 rows, 7
columns) if nothing else is stated. A valid move must be in the range of 0
(leftmost column) to 6 (inclusive, rightmost column).

For a move to be valid, the column must have an empty field, i.e. it must
contain the value 0. Since the columns are filled up from the bottom, a column
is not full if its the top-most field is equal to 0. So this validation seems
trivial.

However, in order to deal with _columns_ rather than _rows_ (remember, the grid
is defined in terms of rows, not the other way around), we first need a way to
gather the fields of a column. The function `get_column` expects a grid and a
column index and returns the fields belonging to that particular column:

    get_column :: Grid -&gt; Int -&gt; Col
    get_column g c = [row !! c | row &lt;- g]

A list comprehension is used to select the element at index `c` in every grid
row using the index operator (`!!`).

The function `is_valid_move` simply extracts the column chosen by the player
and checks its topmost field to be empty (equals 0, that is):

    is_valid_move :: Grid -&gt; Int -&gt; Bool
    is_valid_move g c = (get_column g c) !! 0 == 0

(Notice that no boundary checks are implemented throughout the program, unless
absolutely necessary for getting the logic right.)

This function can be used as follows:

    &gt; g = new_grid 6 7
    &gt; is_valid_move g 0
    True

## Setting a Stone

The first two building blocks were easy to write without modifying state.
Performing a move on the grid by setting a stone into a certain column,
however, is a step that requires a modification of some sort. The solution is
to not mutate the given grid, but to produce a new grid based on the given grid
by accounting for a player's move.

The function `apply_move` expects a grid, a column (chosen by the player and
validated using `is_valid_move`), and the player's number (to set the right
value in the new grid):

    apply_move :: Grid -&gt; Int -&gt; Int -&gt; Grid

Because only a column is given, the row coordinate has to be figured out. Since
stones played are falling down the grid in the physical version of the game,
the bottom-most free field of a column has to be found:

    bottom_most :: Grid -&gt; Int -&gt; Int -&gt; Int
    bottom_most g v c = length (takeWhile (\x -&gt; x == v) col) - 1
                        where col = get_column g c

The lowest free position is found by extracting a subsequent list of a given
value `v`, which can be handed in as an argument. (The value 0 has to be used
for this particular use case by the caller.) The built-in function `takeWhile`
is used to extract a list based on a lambda expression: Elements are taken from
the column as long as the lambda expression holds true. The bottom-most position
of a column with the given value `v` is simply the length of the extracted sub
list minus one (indexes are zero-based). Again, the `get_column` function is
used to get access to the fields of a particular column. 

Now `apply_move` can be implemented as follows:

    apply_move g c p = replace_value g r c p
                       where r = bottom_most g 0 c

Another function is needed: `replace_value`, which creates a new grid based on
the existing grid `g`, by setting the player's stone value `p` to the coordinate
`(r,c)`. (The row coordinate is figured out using `bottom_most`, as shown
above.)

The function `replace_move` is implemented as follows:

    replace_value :: Grid -&gt; Int -&gt; Int -&gt; Stone -&gt; Grid
    replace_value g r c p = take r g ++ [new_row] ++ drop (r + 1) g
                            where new_row = replace_row_value (g !! r) c p

Given the row index `r`, the first `r` rows are taken. (This excludes the row
to be transformed, because the index is zero-based.) The row at index `r` is
computed as `new_row` in a further step. The remaining rows are extracted from
the existing grid by dropping the first `r + 1` rows from it. Those three
components are concatenated to a new grid using the `++` operator.

The `new_row` looks like the old row at index `r`, expect that a single value
at index `c` (the column) has to be replaced with the player's value `v`. The
function `replace_row_value` performs this transformation:

    replace_row_value :: Row -&gt; Int -&gt; Stone -&gt; Row
    replace_row_value r c p = take c r ++ [p] ++ drop (c+1) r

The same logic using `take` and `drop` can be implemented for the column's
fields like for the grid's rows before. The empty field at column index `c` can
simply be replaced by a list solely consisting of the player's stone value `v`.
List concatenation is used again to produce the tranformed column.

A move can be applied as follows:

    &gt; g = new_grid 6 7
    &gt; g1 = apply_move g 3 1
    &gt; g1
    [[0,0,0,0,0,0,0],
     [0,0,0,0,0,0,0],
     [0,0,0,0,0,0,0],
     [0,0,0,0,0,0,0],
     [0,0,0,0,0,0,0],
     [0,0,0,1,0,0,0]]

    &gt; g2 = apply_move g1 4 2
    &gt; g2
    [[0,0,0,0,0,0,0],
     [0,0,0,0,0,0,0],
     [0,0,0,0,0,0,0],
     [0,0,0,0,0,0,0],
     [0,0,0,0,0,0,0],
     [0,0,0,1,2,0,0]]

`apply_move` could also be invoking `is_valid_move` for validation. But this
task should be left for the client to be implemented later on.

## Detecting a Win

Figuring out whether or not a player's most recent move leads to a win is the
hardest part of this program, no matter what implementation language is used.
(However, I didn't try Prolog _yet_ for this.) Let's analyze the problem.

First, what do we know? The player with a number (1 or 2) just picked a column
(between 0 and 5 in our 6x7 grid). A stone was set in the bottom-most empty
field of that column. The actual row where the stone landed in is unknown.
However, this information can be found out: it is the top-most row of the chosen
column holding the player's stone value. All the fields above must be empty.

Second, what do we need to find out? Starting from the coordinates (given
column, row figured out as described above), there are three possibilities to
build a row of four values: horizontal, vertical, and diagonal lines. A
horizontal line is a row, and a vertical row is a column. Diagonal lines can
occur in two directions: ascending or descending. So we actually need to account
for four kinds of rows, which need to be extracted from the row/column
coordinates.

Third, once the horizontal, the vertical, and the two diagonal lines going
through the player's stone most recently set are established, a simple check can
be done: Does the line, which can be represented as a list, contain a list of
four of the player's stones? If that's the case, the player just won the game.

Let's implement that algorithm in a top-down manner!

The function `is_win` expects a grid, a column, and a player's stone value, and
returns a boolean value indicating if the player just won the game:

    is_win :: Grid -&gt; Int -&gt; Stone -&gt; Bool
    is_win g c p = horizontal_win g row p ||
                   vertical_win g c p ||
                   diagonal_win g row c p
                   where row = top_most g p c

Three predicate functions `horizontal_win`, `vertical_win`, and `diagonal_win`
handle the three different shapes of winning rows. To check for a vertical win,
the row is irrelevant. For the other wins, the row where the player's stone just
landed in is figured out using the `top_most` function:

    top_most :: Grid -&gt; Stone -&gt; Int -&gt; Int
    top_most g v c = length (takeWhile (\x -&gt; x /= v) col)
                     where col = get_column g c

This function expects a grid, a player's stone value, a column, and returns the
top-most row containing the player's stone. Going through the column from top to
bottom, values are read into a list as long as they are not equal to the
player's stone value. The length of that list is the row coordinate of the
player's top-most stone in that column. Again, the column is extracted using the
`get_column` function explained further above.

### Vertical and Horizontal Win

A vertical and horizontal winning row can be detected in the same manner. The
only difference is that the former works on columns, and the latter on rows:

    horizontal_win :: Grid -&gt; Int -&gt; Stone -&gt; Bool
    horizontal_win g r p = contained fiar (g !! r)
                           where fiar = [p | _ &lt;- [1..4]]

    vertical_win :: Grid -&gt; Int -&gt; Stone -&gt; Bool
    vertical_win g c p = contained fiar (get_column g c)
                         where fiar = [p | _ &lt;- [1..4]]

In both cases, a grid, an index (row or column, respectively), and a player's
stone value is expected. The boolean return value indicates whether or not the
row or column contains a sub-list consisting of four of the player's stone
values: `fiar`, which is built using a list comprehension.

For the horizontal win, the row can be directly accessed from the grid using the
row index (`g !! r`). For the vertical win, the `get_column` function is used
once again.

The function `contained` is the tricky part. This function checks whether or not
a smaller list (first argument) is part of a larger list (second argument). A
possible implementation looks as follows:

    contained :: Eq a =&gt; [a] -&gt; [a] -&gt; Bool
    contained [] []                     = True
    contained [] ys                     = True
    contained xs []                     = False
    contained (x:xs) (y:ys) | x == y    = and [x == y | (x,y) &lt;- zip xs ys]
                                          &amp;&amp; length xs &lt;= length ys
                                          || contained (x:xs) ys
                            | otherwise = contained (x:xs) ys

The lists processed can be of any type that supports the comparison operator
(`Eq a`). A boolean value is returned indicating whether or not the first list
is contained in the second list. The function is implemented using pattern
matching, which covers the following cases:

1. An empty list is contained in another empty list (first base case).
2. An empty list is contained in any non-empty list (second base case).
3. A non-empty list is not contained in an empty list (negative base case).
4. A non-emtpy list is _possibly_ contained in another non-empty list (complex
   case).

The «possibly» in the fourth case can be resolved as follows: If the first
elements of the two lists do match, the remainders of the two lists need to be
checked for a match. A list comprehension zipping those tails together and
comparing the corresponding elements creates a list of booleans indicating
matches. If all those booleans are `True`, the first list must be contained in
the second list, _if the second list is at least as long as the first list_.
(Notice that the `zip` function only picks values until the shorter of the two
zipped lists is exhausted. The length check ensures that the comparison of the
lists does not end prematurely.)

The `otherwise` case is processed when the two list's heads do not match. In
this case, the `contained` function is invoked again with the full first list
and the second's list tail: It shall be checked whether or not the first list is
contained in the second's list tail.

### Diagonal Win

Detecting a diagonal win works in the same manner as detecting a horizontal or
vertical win. However, there are two subtle details that make the implementation
more complicated:

First, there are _two_ kinds of diagonal lines: ascending and descending. This
can be handled by implementing two different functions.

Second, extracting a diagonal line as a list from the two-dimensional grid is
much more complicated than extracting a horizontal line (row) or a vertical line
(column).

Let's start with the `diagonal_win` function, which accounts for both winning
rows in ascending or descending order:

    diagonal_win :: Grid -&gt; Int -&gt; Int -&gt; Stone -&gt; Bool
    diagonal_win g r c p = contained fiar (diag_asc g r c) ||
                           contained fiar (diag_desc g r c)
                           where fiar = [p | _ &lt;- [1..4]]

The function expects a grid, both row and column indication, and the player's
stone value. As always, a boolean value indicating a win is returned. A win is
detected, if the list of four player's stone values is contained in either the
ascending or the descending diagonal line.

Those lines are extracted from the grid using the `diag_asc` and `diag_desc`
functions, respectively. The two functions look quite similar, but have subtle
differences in the way they process the grid:

- An _ascending_ row starts at the bottom of the grid, i.e. with the highest row
  index. It starts at the left, i.e. with the lowest column index.
- A _descending_ row starts at the top of the grid, i.e. with the lowest row
  index. It also starts at the left, and, thus, with the lowest column index.

The function `diag_asc` expects a grid and both row and column indices. It
returns the ascending diagonal row containing that coordinate:

    diag_asc :: Grid -&gt; Int -&gt; Int -&gt; [Int]
    diag_asc g r c = [g !! i !! j | (i,j) &lt;- zip rows cols]
                     where
                       nrows   = length g
                       ncols   = length (g !! 0)
                       offset  = max (min (nrows - r - 1) (ncols - c - 1)) 0
                       max_row = r + offset
                       min_col = c - offset
                       rows    = reverse [0..max_row]
                       cols    = [min_col..ncols-1]

The function is implemented using a list comprehension. The variable `i` is the
row index, `j` the column index. Those indices are obtained by zipping a list of
row indices (`rows`) with a list of column indices (`cols`). The starting and
end point of those lists are the tricky part.

Consider this grid, in which `-` stands for an empty field, and the upper-case
`F` for the field played most recently (with the `r` and `c` arguments as
indices). All the fields indicated with a lower-case `f` are to be extracted for
the ascending diagonal holding the upper-case `F`:

        !
    0 1 2 3 4 5 6
    - - - - - - f 0
    - - - - - f - 1
    - - - - f - - 2
    - - - f - - - 3
    - - F - - - - 4 !
    - f - - - - - 5

The row and column indices of `F` are given as 4 and 2. The starting point at
the bottom-left can be figured out by shifting the coordinates by an _offset_.
This offset is the smaller value of the following two differences:

- `rows - r - 1`: the number of rows minus the row index (minus one to account
  for the zero-based row index)
- `cols - c - 1`: the number of columns minus the column index (minus one;
  zero-based index again)

The offset is set to 0, if either difference becomes negative (boarder
clipping). The offset is calculated as follows:

    offset = max (min (nrows - r - 1) (ncols - c - 1)) 0
    offset = max (min (6 - 4 - 1) (7 - 2 - 1)) 0
    offset = max (min 1 4) 0
    offset = max 1 0
    offset = 1

And the starting points `max_row`/`min_col` (bottom left) are calculated based
on the given indices of `F` as follows:

    max_row = r + offset
    max_row = 4 + 1
    max_row = 5

    min_col = c - offset
    min_col = 2 - 1
    min_col = 1

The diagonal line can be drawn up to the row index 0 and the column index 6.
Here, it is possible to always use the maximum value, because the `zip` function
will stop picking values once the shorter list is exhausted.

The number of rows and columns can simply be figured out using the `length`
function applied on the grid as a whole and on a single row thereof:

    nrows = length g
    ncols = length (g !! 0)

Notice that in order to create a list containing the _falling_ values from
`max_row` down to 0 (`rows`), a rising list from 0 to `max_row` has to be
created and reversed:

    &gt; reverse [0..max_row]
    [0,1,2,3,4,5]

The other way around, an empty list would be created:

    &gt; [max_row..0]
    []

The somewhat easier to understand function `diag_desc` is simply pasted here
without any further comments.  Figuring out how it works is left to the reader.
The extensive comments above on `diag_asc` certainly help for this purpose:

    diag_desc :: Grid -&gt; Int -&gt; Int -&gt; [Int]
    diag_desc g r c = [g !! i !! j | (i,j) &lt;- zip rows cols]
                      where
                        offset  = min r c
                        min_row = r - offset
                        min_col = c - offset
                        nrows   = length g
                        ncols   = length (g !! 0)
                        rows    = [min_row..nrows-1]
                        cols    = [min_col..ncols-1]

# Conclusion

The complete board logic required to implement a basic «Four in a Row» game has
been implemented in Haskell. The whole code described, plus some additional
attempts to format the grid as a string, can be found on
[GitHub](https://github.com/patrickbucher/programming-in-haskell/blob/master/four-in-a-row/Board.hs).

The linked code also defines a module `Board` which exports the public interface
of the board consisting of the four building blocks discussed in this article
and its predecessor. The file
[BoardTest.hs](https://github.com/patrickbucher/programming-in-haskell/blob/master/four-in-a-row/BoardTest.hs)
defines a couple of unit tests written in HUnit for basic verification of the
logic.

The actual board logic requires a little less than 100 SLOC. Comparable
implementations I've written in Python and C only take up slightly more lines. I
could have made some functions _shorter_, but probably not _clearer_ with my
limited knowledge of Haskell.

The `contained` function, for example, looks a bit bulky, but actually contains
very little logic. It is possible that the negative base case could be
eradicated, because a length check is already performed in the complex case.
However, I rather have a clear statement of the base cases than saving an easy
to understand line of code.

I might revisit this code and improve it as my knowledge of Haskell improves.
But the next step in my journey is to implement an interactive game based on
this board, which will be the subject of an article to be published in weeks or
maybe months.
</content>
    </entry>
    <entry>
        <title>«Four in a Row» in Haskell (Part I)</title>
        <link href="https://paedubucher.ch/articles/2020-08-03-four-in-a-row-in-haskell-part-i.html"/>
        <author>
            <name>Patrick Bucher</name>
        </author>
        <id>https://paedubucher.ch/articles/2020-08-03-four-in-a-row-in-haskell-part-i.html</id>
        <updated>2020-08-03T12:00:00Z</updated>
        <content type="text/markdown; charset=UTF-8">
In a [recent interview](https://youtu.be/O9upVbGSBFo?t=3741), Brian W. Kernighan
said that he always re-implements the same program when he's learning a new
programming language. In his case, it's a programm to process a text file
containing a variable number of lines. In this task, his programming language
AWK (Kernighan is the «K» in «AWK») shines, for it was designed for that kind of
a task.

Such a _stock program_ allows to evaluate a programming language from a certain
perspective. Different programs offer different perspectives. I personally
didn't have such a stock program yet, but there is at least one program I have
already implemented in multiple programming languages: the board game _Four in a
Row_.

# Four in a Row: My Stock Program

This game is played by two players, usually on a 7x6 grid (seven columns, six
rows). The grid is setup to be perpendicular to the table, so that the stones
fall to the lowest free field of the chosen column. The players take turns
setting their stones (red for one player, yellow for the other one). The player
that first can set four of stones into a horizontal, vertical, or diagonal row
wins the game.

I first implemented this game as a program towards the end of my first year as
an apprentice. The task was an optional assignment in an introductory
programming class. C was used as the implementation language. A more recent
re-implementation of that program is available on
[GitHub](https://github.com/patrickbucher/prog/blob/master/vier_gewinnt/vier_gewinnt.c).
The hardest part was to get the winning detection right, especially for the
diagonal rows. Since the grid was implemented as a two-dimensional array,
diagonals clipping the edge would erroneously also be detected as a winning row.
Some additional checks for index boundaries fixed the issue.

16 years later, my apprenticeship already was far in the past. I was studying
computer science in my eight and last term. For a _Game Design_ class, I had to
write a case study on improving an existing game. I picked _Four in a Row_ and
extended it with a couple of new game mechanics. The case study, written in
German, and the source code, can be found on
[GitHub](https://github.com/patrickbucher/v13r93w1nn7), too. This time, I used
Python as the implementation language. The [NumPy](https://numpy.org/) library
made this task very comfortable, and I was able to implement the board logic
with rather few lines of Python code. The unit tests, implemented using
[PyTest](https://docs.pytest.org/en/stable/), took up far more lines than the
actual code.

Both versions were implemented for the command line. However, the latter
version was implemented in a way that would also support graphical frontends.

## Building Blocks

Having implemented the same program with much more programming experience and
using a different programming language, the resulting code looked quite
different. However, I was able to detect some common patterns.

On a very high level, there are two parts for such a program: First, the _board
logic_ that deals with the grid, its manipulations and validations (Is a row not
full yet?  What is the bottom-most empty row in a given column? Are four stones
of the same color in a row?). Second, the _game logic_, which consists of a big
loop that lets the players take turns setting their stones, prints the grid, and
ends the game upon a win or draw.

The board logic can be taken further apart into the following components:

1. **Creating an Empty Grid**: At the beginning of a game, an empty grid with
   given dimensions has to be created. (The physical game is played on a 7x6
   grid, but a computer game can offer additional flexibility with the number of
   rows and columns given as arguments.)
2. **Validating a Move**: As soon as all fields of a column are filled, the
   column must no longer be chosen by players. A function is needed that checks
   which columns still have at least one empty field.
3. **Setting a Stone**: If a stone is to be set into a non-full column, the
   bottom-most empty row of that column has to be figured out. Then, the field
   is modified by setting the player's stone into that position.
4. **Detecting a Win**: After every move, it has to be checked whether or not
   the grid contains four stones of the same color laying in the same
   horizontal, vertical, or diagional row, without any gaps in between. If the
   detection gets to know which player did the last move, and into what
   coordinates that stone was put, the algorithm has to do less work, as opposed
   to an approach where the whole grid is evaluated for both players. (For the
   case-study, I had to use the latter approach, for one of the additional game
   mechanics allowed to flip the grid, which required a full evaluation of the
   whole grid afterwards.)
5. **Formatting the Grid**: This part could also be implemented in the game
   logic.  However, offering the capability to print the current grid from the
   board component (be it a module or a class) in a nicely formatted way is a
   good design decision in terms of cohesion. This function can be made very
   flexible by accepting formatting parameters, such as the characters to be
   used to display fields that are empty, or contain a stone of either player.

A function to format the current grid makes an important separation between the
inner state of the grid and its textual representation on the command line. It
is a good idea to represent the state of the fields as _numbers_ internally,
but to use _characters_ in order to display them nicely on the command line.
Internally, `0` can used for empty fields. For fields holding a stone of player
one or two, the values `1` and `2`, respectively, can be used. The empty field
can be displayed using a whitespace character, an underscore, or a dash. The
stones of the players can be easily distinguished when using `x` and `o` for
their output.

# Towards Haskell

The programming language _Haskell_, which has been mentioned in this article's
title, but not in the text ever since, shall be used to create an additional
implementation of the _Four in a Row_ game. But why Haskell?

First, I'm currently learning Haskell. It turns out that writing useful programs
in Haskell is not that easy, because advanced concepts like Monads have to be
understood in order to perform input/output operations. I'm working through the
rather dense book [Programming in Haskell (Second
Edition)](https://www.cs.nott.ac.uk/~pszgmh/pih.html) (by Graham Hutton) at the
moment, and I've almost finished the first part. The knowledge acquired from
those first nine chapters allows me to implement the board logic. The
interactive part then has to wait until I (nearly) finished the book.

Second, I'm interested in functional programming. I consider Haskell as a
stepping stone into that programming paradigm. I have some minor experience in
Prolog, and I'd like to learn Erlang later on. Knowledge about functional
programming also helps when programming in Python and JavaScript, which also
support features like lambda expressions, higher-order functions, and, in case
of Python, list comprehensions.

Implementing _Four in a Row_ in Haskell gives me a couple of challenges.
Unlike an implementation in C or Python, the grid must not be modified during
gameplay. A new grid, representing the fresh state, has to be build up based on
the previous state and the player's action, instead. I also need to figure out
how to detect a winning row in a declarative way, i.e. without loops and
counter variables. The input/output of the actual game logic will probably be
the biggest challenge later on. The game logic, implemented as a loop in both C
and Python, needs to be implemented using a different mechanism.

My plan is to implement the board logic, consisting of the five components
stated above, in the next couple of days in Haskell. I'll write an article
describing my approach and containing the code for the board logic as soon as I
have a decent solution for the problems stated. The game logic has to wait for a
couple of weeks of even months, depending on my progress with _Programming in
Haskell_.

Stay tuned, and feel free to put (maybe needed?) pressure on me, when those
articles do not appear any time soon…
</content>
    </entry>
    <entry>
        <title>Virtual Machines with libvirt and Networking</title>
        <link href="https://paedubucher.ch/articles/2020-08-01-virtual-machines-with-libvirt-and-networking.html"/>
        <author>
            <name>Patrick Bucher</name>
        </author>
        <id>https://paedubucher.ch/articles/2020-08-01-virtual-machines-with-libvirt-and-networking.html</id>
        <updated>2020-08-01T22:30:00Z</updated>
        <content type="text/markdown; charset=UTF-8">
I'd like to dig deeper into system administration tasks. At work, I have to
manage a fleet of Linux servers with Puppet. And in my spare time, I'd like to
manage the servers I run with Ansible or Puppet in the future.

Virtual Machines are easily obtained nowadays. Cloud providers such as Digital
Ocean or Exoscale offer virtual machines with various operating systems at
rather moderate prices. You only have to pay for the time the virtual machines
are actually running, so you can save money by shutting those hosts down when
not needed.

However, running those virtual machines locally costs even less. No additional
public IPv4 addresses are wasted, and, most importantly, a local setup allows
you to test changes to be applied to your productive environment locally
beforehand.

This article shows how to set up three virtual machines ‒ `master`, `node1`, and
`node2`, which later could be used for a Puppet setup with a Puppetmaster ‒
using [libvirt](https://libvirt.org/) on top of
[KVM](https://www.linux-kvm.org/page/Main_Page). [Debian 10
(«Buster»)](https://www.debian.org/releases/buster/) is going to be used both as
the host and guest operating system. The host operating system is installed on a
Dell Latitude E6430 from 2013 with 8 GB or RAM, which is just laying around
here. (This also proofs that you don't need a whole lot of hardware resources
for such a setup.)

# Setting up the Virtualization

Given a fresh Debian setup with the lightweight LXQt desktop, a couple of
packages need to be installed in order to get virtualization to work:

    # apt-get install \
        qemu-kvm \
        libvirt-clients \
        libvirt-daemon-system \
        virtinst \
        bridge-utils

Make sure to activate virtualization in the BIOS. Check if the `kvm` kernel
module is activated:

    $ lsmod | grep ^kvm
    kvm                 835584  1 kvm_intel

If there is a number not equal to 0 in the third column, `kvm` is up and
running.

# Setting up the Virtual Network

Usually a `default` network is pre-defined, which can be checked as follows:

    # virsh net-list --all
     Name      State      Autostart   Persistent
    ----------------------------------------------
     default   inactive   no          yes

The `default` network can be configured to be started up automatically:

    # virsh net-autostart default
    Network default marked as autostarted

Until the next system restart, it is started up manually:

    # virsh net-start default
    Network default started

A bridge interface `virbr0` should have been created:

    # brctl show
    bridge name     bridge id               STP enabled     interfaces
    virbr0          8000.5254005f4e6b       yes             virbr0-nic

Make sure that NAT is activated:

    # sudo sysctl -a | grep 'net.ipv4.ip_forward ='
    net.ipv4.ip_forward = 1

The value of the above property must be `1`.

## Possible Issues

If `iptables` is in use, make sure to forward the traffic from the guests over
the bridge `virbr0`, so that the guests also have internet access:

    # iptables -I FORWARD -i virbr0 -o virbr0 -j ACCEPT

# Setting up the Virtual Machines

Since networking over the bridge interface requires `root` privileges, all
virtual machine files are put into the `/opt/vms` directory, which first needs
to be created:

    # mkdir /opt/vms
    # cd /opt/vms

The network installer for Debian Buster can be downloaded from the official
website:

    # wget https://cdimage.debian.org/debian-cd/current/amd64/iso-cd/\
    debian-10.4.0-amd64-netinst.iso

The `master` virtual machine is now setup using `virt-install`:

    # virt-install \
        --name master \
        --memory 1024 \
        --vcpus=1,maxvcpus=2 \
        --cpu host \
        --cdrom debian-10.4.0-amd64-netinst.iso \
        --disk /opt/vms/master.qcow2,size=8,format=qcow2 \
        --network network=default \
        --virt-type kvm

The machine gets 1 GB of memory and a 8 GB disk. Most importantly, the network
is set to the `default` network.

A window showing the Debian installer appears. Just install the standard system
utilities and the SSH server. The following users and passwords shall be
configured:

- `root`: `topsecret`
- `user`: `secret`

After the setup is finished, just let the system boot, and login as `root`. Then
shut the virtual machine down:

    # shutdown -h now

The two additional guest nodes can be created by cloning the `master` virtual
machine just set up:

    # virt-clone --original master --name node1 --file node1.qcow2
    # virt-clone --original master --name node2 --file node2.qcow2

Now start up all the nodes:

    # virsh --connect qemu:///session start master
    # virsh --connect qemu:///session start node1
    # virsh --connect qemu:///session start node2

# Configuring the Virtual Network

In order to conveniently access the guests, static IPs should be assigned to
them. The network configuration can be edited as follows:

    # virsh net-edit default

An editor showing an XML configuration appears:

    &lt;network&gt;
      &lt;name&gt;default&lt;/name&gt;
      &lt;uuid&gt;fecb90d5-9b46-48f6-8b93-e57032f8ba6a&lt;/uuid&gt;
      &lt;forward mode='nat'/&gt;
      &lt;bridge name='virbr0' stp='on' delay='0'/&gt;
      &lt;mac address='52:54:00:63:d3:70'/&gt;
      &lt;ip address='192.168.122.1' netmask='255.255.255.0'&gt;
        &lt;dhcp&gt;
          &lt;range start='192.168.122.2' end='192.168.122.254'/&gt;
        &lt;/dhcp&gt;
      &lt;/ip&gt;
    &lt;/network&gt;

The `dhcp` section needs to be extended with static IP definitions, which map
the MAC addresses of the guest's virtual network interfaces to the static IP
addresses to be used.

The MAC addresses of the virtual machines can be extracted from their
configuration as follows:

    # virsh dumpxml master | grep -i '&lt;mac'
        &lt;mac address='52:54:00:db:07:7c'/&gt;
    # virsh dumpxml node1 | grep -i '&lt;mac'
        &lt;mac address='52:54:00:a4:77:a9'/&gt;
    # virsh dumpxml node2 | grep -i '&lt;mac'
        &lt;mac address='52:54:00:51:e8:ef'/&gt;

Using those MAC addresses, new static host definitions can be created as
follows:

    &lt;host mac='52:54:00:db:07:7c' name='master' ip='192.168.122.2'/&gt;
    &lt;host mac='52:54:00:a4:77:a9' name='node1' ip='192.168.122.3'/&gt;
    &lt;host mac='52:54:00:51:e8:ef' name='node2' ip='192.168.122.4'/&gt;

The XML network definition should now look as follows (the `uuid` and `mac
address` of the host will vary):

    &lt;network&gt;
      &lt;name&gt;default&lt;/name&gt;
      &lt;uuid&gt;fecb90d5-9b46-48f6-8b93-e57032f8ba6a&lt;/uuid&gt;
      &lt;forward mode='nat'/&gt;
      &lt;bridge name='virbr0' stp='on' delay='0'/&gt;
      &lt;mac address='52:54:00:63:d3:70'/&gt;
      &lt;ip address='192.168.122.1' netmask='255.255.255.0'&gt;
        &lt;dhcp&gt;
          &lt;range start='192.168.122.2' end='192.168.122.254'/&gt;
          &lt;host mac='52:54:00:db:07:7c' name='master' ip='192.168.122.2'/&gt;
          &lt;host mac='52:54:00:a4:77:a9' name='node1' ip='192.168.122.3'/&gt;
          &lt;host mac='52:54:00:51:e8:ef' name='node2' ip='192.168.122.4'/&gt;
        &lt;/dhcp&gt;
      &lt;/ip&gt;
    &lt;/network&gt;

After saving the configuration, the network `default` needs to be restarted:

    # virsh net-destroy default
    # virsh net-start default

The guest virtual machines must also be restarted so that they will get the new
IP addresses assigned:

    # virsh shutdown master
    # virsh shutdown node1
    # virsh shutdown node2

    # virsh --connect qemu:///session start master
    # virsh --connect qemu:///session start node1
    # virsh --connect qemu:///session start node2

The virtual machines should now be accessible through SSH:

    $ ssh user@192.168.122.2
    $ ssh user@192.168.122.3
    $ ssh user@192.168.122.4

Make sure that the network communication is working between the guests:

    [user@master]$ ping node1
    [user@master]$ ping node2

Also make sure to define the proper hostname in `/etc/hostname`, for it is still
`master` for the two guests that have been cloned from the initial image:

    [root@node1]# echo 'node1' &gt; /etc/hostname
    [root@node2]# echo 'node2' &gt; /etc/hostname

## Adding Some Comfort

Consider adding the following definitions to `/etc/hosts`:

    192.168.122.2   master
    192.168.122.3   node1
    192.168.122.4   node2

So that you can access your virtual machines by their host names:

    $ ssh user@master
    $ ssh user@node1
    $ ssh user@node2

In order to login to the guests without typing a password, create an SSH key
locally without any passphrase:

    $ ssh-keygen -t rsa -b 4096 -f ~/.ssh/id_vms_rsa

Make sure that your `~/.ssh` folder has the access mode `700`, and the contained
files all have the access mode `600` (thanks to [meillo](http://marmaro.de/) for
pointing that out):

    $ chmod 700 ~/.ssh
    $ chmod 600 ~/.ssh/*

Copy the public key to the hosts using `ssh-copy-id` (thanks to meillo again for
hinting that utility to me):

    $ ssh-copy-id -i ~/.ssh/id_vms_rsa user@master
    $ ssh-copy-id -i ~/.ssh/id_vms_rsa user@node1
    $ ssh-copy-id -i ~/.ssh/id_vms_rsa user@node2


Check that the SSH connection now works without any password:

    $ ssh -i ~/.ssh/id_vms_rsa user@master
    $ ssh -i ~/.ssh/id_vms_rsa user@node1
    $ ssh -i ~/.ssh/id_vms_rsa user@node2


# Conclusion

Three virtual machines running Debian GNU/Linux have been installed on a
rather old laptop running Debian GNU/Linux itself. Those virtual machines can be
comfortably accessed without any passwords through SSH, and are able to
communicate with one another over a virtual network.

It took me almost a day ‒ and gave me some additional grey hair ‒ to get all
this information together from various sources. After I figured out how to
create the setup described above, it only took me about two hours to reproduce 
everything on another laptop (including the setup of the laptop itself) and to
write this article.

Since I did the try-and-error part on Arch Linux, this article can also be used
on that distribution, and probably many others as well. Only the packages to be
installed will probably vary on other distributions.

I plan to describe the setup of a local Puppet environment based on the setup
described above in a forthcoming article.
</content>
    </entry>
    <entry>
        <title>Table-Driven Test Design</title>
        <link href="https://paedubucher.ch/articles/2020-07-22-table-driven-test-design.html"/>
        <author>
            <name>Patrick Bucher</name>
        </author>
        <id>https://paedubucher.ch/articles/2020-07-22-table-driven-test-design.html</id>
        <updated>2020-07-22T22:00:00Z</updated>
        <content type="text/markdown; charset=UTF-8">
Many universities teach programming in Java. Writing unit tests is one of the
subjects being taught. Many professional Java programmers, but also university
professors, suggest to build those test cases according to a pattern. _Given,
When, Then_ is a common pattern, and so is _Arrange, Act, Assert_. Both patterns
prescribe the following structure for a test case:

1. _Given_/_Arrange_: An environment (in the broadest sense) is built up.
2. _When_/_Act_: The function or method to be tested is invoked.
3. _Then_/_Assert_: The result of the function or method is checked against some
   expectation.

Such a test case might look as follows (Java):

    public void testAddition() {
        // Given/Arrange
        Calculator calc = new Calculator();
        int a = 3;
        int b = 5;

        // When/Act
        sum = calc.add(a, b);

        // Then/Assert
        assertEqual(8, sum);
    }

A rule often taught is the so-called _single assert rule_ from Robert C. Martin,
[whom I refuse to call «Uncle
Bob»](http://marmaro.de/apov/txt/2016-04-27_schaedlicher-kult.txt). It states
that there should be only one assertion per test case. One can argue whether or
not this rule is useful.

# Unclean Code

However, in my experience this rule leads to a consequence I do not like ‒ and
which also doesn't fit into the _Clean Code_ philosophy (or _cult_, I daresay):
The programming language being used to write test code is a small subset of the
implementation language, often degenerating into a sheer sequence of statements
(imperative programming).

Even though using a subset of a language is often a sensible approach (just
think about C++, or `with` and `eval` in JavaScript, or `unsafe` in Go, etc.),
using a subset of a language that doesn't even contain core features from
structured programming (decisions, loops, data structures) does not sound
sensible to me, except when programming in a purely functional style.

How should an additional test case to cover, say, negative numbers, be added to
the one above? The _single assert rule_ wants us to write an additional test
case:

    public void testAdditionWithNegativeNumbers() {
        // Given/Arrange
        Calculator calc = new Calculator();
        int a = -1;
        int b = 3;

        // When/Act
        sum = calc.add(a, b);

        // Then/Assert
        assertEqual(2, sum);
    }

Who would _type_ in that code, which is almost identical to the one above? Such
code is rather _copied_ than written again. (Why don't I hear somebody shouting
_«Clean Code!!!!11»_ now?)

# Structured Programming to the Rescue

Let's violate the _single assert rule_ for a minute and bring back structured
programming. Let's write a unit test in C!

    typedef struct {
        int a;
        int b;
        int expected;
    } addition_test_case;

    void test_addition()
    {
        addition_test_case tests[] = {
            {3, 5, 8},
            {-1, 3, 2},
        };
        int n = sizeof(tests) / sizeof(tests[0]);
        for (int i = 0; i &lt; n; i++) {
            addition_test_case test = tests[i];
            int actual = add(test.a, test.b);
            if (actual != test.expected) {
                printf(&quot;add(%d, %d): expected %d, got %d\n&quot;,
                        test.a, test.b, test.expected, actual);
                exit(1);
            }
        }
        printf(&quot;test_addition: %d tests passed\n&quot;, n);
    }

This test case, which does not make use of any unit testing framework, was
designed in a _table-driven_ manner. I first got to know the concept of
_table-driven test design_ when learning Go by reading [The Go Programming
Language](http://www.gopl.io/) (p. 306) by Alan A. A. Donovan and the great
Brian W. Kernighan.

However, the concept must predate Go, for I can at least remember one article by
Rob Pike, who later designed Go, mentioning table-driven test design.
(Ironically ‒ or not so ironically ‒ that article was a critique of
object-oriented programming, as far as I can remember.)

# Table-Driven Test Design

Let's break down the parts that make up a table-driven test design.

First, a single test case is defined using a structure that contains all the
input parameters, and the expected result of the test:

    typedef struct {
        int a;
        int b;
        int expected;
    } addition_test_case;

Second, an array ‒ the test _table_ ‒ containing all the test definitions is
defined (_Given_/_Arrange_):

    addition_test_case tests[] = {
        {3, 5, 8},
        {-1, 3, 2},
    };

Third, the test table is processed using a _loop_ (structured programming,
remember that?):

    int n = sizeof(tests) / sizeof(tests[0]);
    for (int i = 0; i &lt; n; i++) {
        // omitted
    }

For every test case, the result is computed (_Act_/_When_):

    addition_test_case test = tests[i];
    int actual = add(test.a, test.b);

Fourth, the result is validated against the definition (_Then_/_Assert_):

    if (actual != test.expected) {
        printf(&quot;add(%d, %d): expected %d, got %d\n&quot;,
                test.a, test.b, test.expected, actual);
        exit(1);
    }
    printf(&quot;test_addition: %d tests passed\n&quot;, n);

An error message is printed if the `actual` value is not equal to the `expected`
value (in case `add` was implemented incorrectly):

    add(3, 5): exptected 8, got 666

Note that this test terminates after the first error. No assertions are used.
The lack of a test framework is compensated by manually defined error and
success messages.

Yes, I'm well aware of the fact that there are unit testing libraries in C. The
point is that this C code covering two test cases is only slightly longer than
the Java code to cover the same amount of test cases would be. (Using Python or
Go rather than C would have shaved off some additional lines.)

Now let's add a third and a fourth test case:

    addition_test_case tests[] = {
        {3, 5, 8},
        {-1, 3, 2},
        {13, 17, 30}, // new
        (-100, 100, 0}, // new
    };

No code was copied. No existing code was modified. Only _two_ lines of code were
added to define _two_ additional test cases. The table-driven test is
_extensible_.  Robert C. Martin would love it, wouldn't he?

# Comparing Apples to Rotten Tomatoes

So why isn't everybody writing table-driven tests instead of triple-A copy-paste
tests?

First, some programming languages make it harder to define data structures as
literals. Languages like JavaScript, Python, or Go are quite good at that. Even
C, as shown above, can be quite concise when it comes to defining static data
structures. Java recently got better at that, but up to version 8, defining a
static map structure was done by adding single elements subsequently. (Why don't
I hear _«DRY principle!!!1»_ now?)

Second, the unit testing framework plays an important role. In C, (at least as
shown above), and in Go (as it is done using the standard library), no
assertions are used. The programmer instead performs the checks manually and
reacts with a reasonable error message. The programmer is supposed to _program_
the tests.

Some unit testing frameworks that do make use of assertions also allow to add
custom error messages to every `assert` call. Other frameworks, such as
[Jest](https://jestjs.io/), just will tell you _on which line_ an assertion
failed. This is not very useful when having assertions within a loop, for the
programmer does not know which test case failed. At least for Jest, writing pure
sequential assertion code is a necessity, and the _single assert rule_ looks
quite reasonable from that perspective.

The [PyTest](https://docs.pytest.org/en/latest/) framework, for example, has
table-driven test design built-in, by providing the static test definitions
through a decorator, which is basically an annotation in Java lingo. (Check
`@pytest.mark.parametrize` for details.) However, this approach makes it
impossible to include information into the test table that needs prior
construction within the test function.

More recent versions of JUnit also allow for parametrized tests (check out the
`@ParametrizedTest` and `@ValueSource` annotations). The restrictions stated
above for PyTest also apply here. Again, the poor programmer is put into
straightjacket, for he's not supposed to _program_, but only to _test_.

My favourite test framework is from the Go standard library, which on one hand
gives the programmer total flexibility, and on the other hand provides an useful
API to construct small but powerful test runners. Checkout the
[testing](https://golang.org/pkg/testing/) package for details. (And read [The
Go Programming Language](https://gopl.io) by all means, even if you don't need
to learn Go. You'll pick up a lot about computer science in this book.)

# Single Assert Rule Revisited

The discussion about testing frameworks and programming languages (and text
editors, and tabs vs. spaces) could be extended here ad nauseam. But let's
review the _single assert rule_ instead, which could be interpreted from two
perspectives:

1. Runtime: `assert` should only be called once per execution of every test
   function/method.
2. Code: There should only be one reference to `assert` in every test
   function/method.

While the first interpretation makes table-driven design impossible, the second
interpretation might be closer to the rule's original intention: Each test case
should only verify one aspect of the function/method being called.

I'll therefore continue to happily violate the first interpretation of the rule,
for the advantages of table-driven test design (extensibility, flexibility, more
concise code) outhweigh the indiscriminate application of some hand-wavy
statements about «doing only one thing» by far. Please let me just _program_
those tests…

As an additional example, check out my test cases for some time formatting
routines
([test_timefmt.c](https://github.com/patrickbucher/countdown/blob/master/test_timefmt.c)).
Here, the test table can be used in two directions: One function uses the left
value as input and the right value as the expected outcome, while the other
function does the opposite. Here, _two_ new test cases are defined by adding
_one_ (very short) line of code.

Am _I_ allowed to shout _«Clean Code!»_ and _«DRY principle!»_ now, by the way?
</content>
    </entry>
    <entry>
        <title>Optimierung und Externalisierung</title>
        <link href="https://paedubucher.ch/articles/2020-07-04-optimierung-und-externalisierung.html"/>
        <author>
            <name>Patrick Bucher</name>
        </author>
        <id>https://paedubucher.ch/articles/2020-07-04-optimierung-und-externalisierung.html</id>
        <updated>2020-07-04T15:00:00Z</updated>
        <content type="text/markdown; charset=UTF-8">
Ich habe diesen Frühling _Heute schon einen Prozess optimiert?_ von Gunter Dueck
gelesen. Der Autor beschreibt in diesem Buch, wie in Deutschland (und im
ähnlichen Stil wohl auch in anderen Ländern) derzeit Prozessoptimiertung überall
das Gebot der Stunde ist. Historisch gesehen habe man das Wirtschaftswachstum
seit dem zweiten Weltkrieg vor allem Prozessoptimierungen im zweiten
Wirtschaftssektor (Industrie) zu verdanken. Die Autos, die wir heute fahren,
unterscheiden sich nicht grundlegend von denjenigen, die vor 50 Jahren
produziert worden sind. Ihre Herstellungsweise hat sich jedoch radikal
verändert und läuft heute grösstenteils automatisch ab.

In der Industrie sind wir mittlerweile an die Grenzen der Optimierung und des
Wachstums geraten. Grosses Wachstum gibt es nur noch im Dienstleistungssektor.
Das Problem, das Dueck beschreibt, bezieht sich auf die Dienstleistungen. Denn
hier wird genau nach dem gleichen Prinzip verfahren wie in der Industrie:
Prozessoptimierung, was das Zeugs hält! Doch sind optimierte Dienstleistungen
wirklich das, was sich der Kunde wünscht?

McDonald's ist das Paradebeispiel für Prozessoptimierung in der Gastronomie. Ich
esse sehr selten dort, und das praktisch nur, wenn es keine Alternativen gibt,
und/oder wenn ich betrunken bin. Die Bedienung erfolgt hocheffizient. Dank der
neuen Bestell- und Bezahlterminals muss man nicht einmal mehr lange an der Kasse
anstehen und sich dort mit dem Personal unterhalten. Der Bestellprozess ist
mittlerweile soweit durchoptimiert, wie es der Herstellungsprozess in der Küche
schon längstens ist.

Doch möchte ich auch in einem «richtigen» Restaurant so bedient werden? Ich gehe
gerne zwischendurch in der Mittagspause mit Bekannten ausgedehnt in einem
Restaurant essen. Dort steht neben dem guten Essen auch die Unterhaltung im
Mittelpunkt. So eine Mittagspause ist oft bereichernd und entspannend, quasi ein
Kurzurlaub vor dem Nachmittag.

Merke ich jedoch, dass die Bedienung sichtlich gestresst ist, kann ich mich beim
Restaurantbesuch kaum entspannen. Ich wähle und bestelle mein Essen sehr schnell
und versuche, die Bedienung nicht unnötig lange aufzuhalten, denn ansonsten
könnte das Ärger mit dem Vorgesetzten geben, was bloss für noch mehr Stress und
schlechte Laune sorgt. Ein Mittagessen in einem Restaurant, das
Prozessoptimierung betreibt, geht zwar schneller, ist aber kein sehr angenehmes
Erlebnis. Man könnte auch gleich zu McDonald's gehen.

Ein anderes Beispiel ist die Zustellung von Paketen. In den 90er-Jahren kam
einmal täglich ein Postbote vorbei, der auf einem kleinen Anhänger Pakete
mitführte. Für ein Dorf mit den weit ausserhalb gelegenen Bauernhöfen waren
meistens ein oder zwei Postboten verantwortlich. Zu dieser Zeit gab es
wesentlich weniger Pakete, jedoch mehr Briefe, Zeitungen, Zeitschriften usw.

Diese Postboten haben immer einen sehr entspannten Eindruck auf mich gemacht.
Oft konnte ich beobachten, dass sich der Postbote nach der Brief- und
Paketzustellung noch mit den Nachbarn unterhielt, bis er zum nächsten Haus
weiterzog. Offensichtlich hatte man damals noch Zeit…

Heutzutage ist Effizienz angesagt. Der Paketbote rennt aus seinem Kastenwagen
und will siene Ware möglichst schnell loswerden. Das ist auch nötig, denn seine
Route wurde zuvor nach tayloristischen Methoden vermessen. Die Post weiss, wie
lange der Bote für welche Anzahl Pakete maximal benötigen darf. Wird diese
Zielvorgabe nicht eingehalten, hat der Bote mit negativen Konsequenzen zu
rechnen.

Manche Paketzusteller, denn es gibt ja mittlerweile Konkurrenz zur Post,
klingeln sich so oft bei einem Mehrfamilienhaus durch. Schliesslich muss die
Sendung nicht unbedingt dem Empfänger übergeben, sondern nur in das Gebäude
hineingebracht werden. Der Bote klingelt also bei allen Hausbewohnern, und
unterbricht dabei möglicherweise eine Vielzahl von Personen bei ihrer
Beschäftigung. In den letzten Monaten könnte das durchaus Büroarbeit (in meinem
Fall Softwareentwicklung) gewesen sein, zumal viele Leute im Home-Office tätig
sind. Wie schädlich solche Unterbrechungen sein können, weiss ich als
Programmierer nur zu gut.

Ergebnis: Durch die Unterbrechungen sind die Leute weniger produktiv. Ihre
Arbeitgeber verlieren Arbeitsleistung und damit Geld, müssen ihre Angestellten
aber genau gleich entlöhnen. Der Paketzusteller spart hingegen einen Bruchteil
seiner Personalkosten, da der Zustellungsprozess mittels Durchklingeln optimiert
worden ist. Der Paketzusteller externalisiert seine Kosten ‒ das Umfeld hat
diese zu bezahlen.

Diese Prozessoptimierung führt nicht nur zu schlechteren Dienstleistungen ‒ das
Paket wurde unsanft beim Eingang abgeworfen, und nicht dem Empfänger überreicht
‒ sondern auch zu externalisierten Kosten. Denn der entstandene Schaden taucht
nicht in der Bilanz des Paketzustellers auf, jedenfalls nicht sofort. (Und
sollten die Versandhändler wegen schlechter Rückmeldungen der Logistikfirma ihre
Aufträge entziehen, dürfte diese zum Ausgleich wiederum mit weiteren
Prozessoptimierungen reagieren.)

Ich bin keinesfalls gegen die Automatisierung von mechanischen Abläufen, denn
diese ist als Softwareentwickle mein täglich Brot, ja meine
Existenzberechtigung. Es gibt Aufgaben, die der Computer schneller und präziser
ausführen kann als ein Mensch. Die zwischenmenschlichen Interaktionen sollten
jedoch nicht optimiert werden, denn diese machen oftmals die Qualität einer
Dienstleistung aus. Solche Optimierungen führen oft bloss zu Frust auf beide
Seiten ‒ und eben zu externalisierten Kosten, von denen wir sonst schon viele
haben (Umweltverschmutzung, Lärmbelastung, Littering usw.)

Fazit: Wir sollten beim Optimieren von Prozessen nicht nur darauf achten, dass
dabei die Dienstleistung und der zwischenmenschlicher Umgang nicht
beeinträchtigt werden. Wir sollten auch darauf achten, dass wir unsere
Einsparungen nicht unseren Mitmenschen als externalisierte Kosten aufbürden.
</content>
    </entry>
    <entry>
        <title>Meine Linux-Distributionen</title>
        <link href="https://paedubucher.ch/articles/2020-06-28-meine-linux-distributionen.html"/>
        <author>
            <name>Patrick Bucher</name>
        </author>
        <id>https://paedubucher.ch/articles/2020-06-28-meine-linux-distributionen.html</id>
        <updated>2020-06-28T22:00:00Z</updated>
        <content type="text/markdown; charset=UTF-8">
Ich verwende seit 2005 hauptsächlich Linux als Betriebssystem. Dabei habe ich
schon Erfahrungen mit verschiedenen Distributionen sammeln können. Meistens
hatte ich eine Hauptdistribution, die ich praktisch auf all meinen Rechnern
installiert war. Dies ändert sich jetzt vielleicht. Doch der Reihe nach…

# Mandrake: Wie alles begann

Meine ersten Erfahrungen mit Linux habe ich im Jahr 2004 gemacht. Alles begann
damit, dass eMule (das damals wichtigste File-Sharing-Tool, das einen
Stellenwert hatte, wie es heute BitTorrent hat) auf dem Windows-Rechner der
Familie nicht mehr richtig funktionierte. Irgendetwas musste ich am
Betriebssystem kaputt gemacht haben.

Eine mögliche Lösung wäre es gewesen, den Rechner neu mit Windows XP
aufzusetzen. Das konnte ich aber nicht so einfach tun, da auch andere
Familienmitglieder Dateien auf dem Rechner hatten. So musste ich immer um
Erlaubnis bitten, wenn ich den Rechner neu aufsetzen wollte. Ausserdem dauerte
es oft Tage, bis wieder alles funktionstüchtig war.

Da ich eine zweite Festplatte hatte, die ich sonst für nichts brauchte, wollte
ich stattdessen einen Dual-Boot einrichten. So gab ich _Mandrake Linux_ (heute
_Mandriva_) eine Chance. Die Installation lief problemlos ab, und auch der Dual
Boot mit Windows klappte problemlos. Meine Familie konnte weiter standardmässig
nach Windows booten.

Die KDE-Oberfläche war für mich einfach bedienbar. Da ich bereits auf Windows
diverse OpenSource-Programme (OpenOffice.org, VLC Media Player, Firefox)
verwendete, kam ich recht schnell mit dem Betriebssystem zurecht. eMule lief
tatsächlich unter Mandrake. Das Problem war aber, wie ich die heruntergeladenen
Dateien vom Rechner wegkopieren sollte.

Der USB-Stick (Kapazität: 128 MB), den ich an einem überbetrieblichen Kurs
(Computer zusammenbauen) erhalten hatte, wurde nicht automatisch erkannt. Und
das mit dem `mount`-Befehl, was von der Google-Suche ausgespuckt worden war,
überforderte mich dann doch noch. Der Zugriff auf die Windows-Festplatte (NTFS)
funktionierte (out of the box) leider nur lesend. So werde ich mir wohl die
heruntergeladenen Dateien auf CDs gebrannt haben, denn die Brennsoftware
funktionierte problemlos.

Ansonsten verlor ich bald das Interesse an Mandrake und bootete nur noch nach
Windows.

# SuSE: Linux als neues Zuhause

Es muss wohl Ende 2004 oder Anfang 2005 gewesen sein, als ich mir zum ersten mal
SuSE installierte. Wahrshceinlich war es Version 9.2 oder 9.3. Wieder
installierte ich es auf der zweiten Festplatte neben Windows. Doch dieses mal
sollte ich dabei bleiben.

Im Sommer 2005 wechselte ich nach zwei Jahren Lehrlingsaustausch bei der [Data
Unit AG](https://www.dataunit.ch) in die Softwareentwicklung bei [Bison Schweiz
AG](https://www.bison-group.ch). Nach zwei eher Microsoft-geprägten Jahren
sollte ich nun also ein Java-Entwickler werden. In der Schule arbeiteten wir mit
C#. Doch unser Lehrer in den Programmierfächern, Roland Bucher, der beide
Programmiersprachen kannte, war so flexibel, dass er uns die Wahl der
Programmiersprache frei liess. So rückte ich ab von C# und beschäftigte mich
bereits im zweiten Lehrjahr, also bevor ich den Arbeitsplatz wechselte, mit
Java.

Es muss kurz vor diesem Wechsel gewesen sein, als ich auf
[Heise.de](https://www.heise.de) einen Artikel über die Zukunftsstrategie von
Microsoft gelesen hatte. Dabei kündigte der damalige CEO Steve Ballmer an, dass
Microsoft so etwas wie _full spectrum dominance_ in der IT erreichen wollte. Das
Forum zu dieser News-Meldung war damals voller ablehnender Beiträge. Microsoft
wurde zu dieser Zeit vom unsympathischen Monopolisten zum absoluten Hassobjekt,
und das nicht nur für mich. Für mich war klar, dass ich von Microsoft und damit
von Windows weg musste.

Es kam dazu, dass ich die Dokumentation [The
Code](https://www.youtube.com/watch?v=XMm0HsmOTFI) gesehen hatte. Nun
interessierte ich mich nicht nur für GNU/Linux als Betriebssystem, sondern für
die Freie-Software-Bewegung als Ganzes. Für mich war eine neue Welt aufgegangen.
Leute wie Richard Stallman, Linus Torvalds und Alan Cox waren meine neuen Idole.

Im Herbst 2005 hatten wir in der Lehre unsere Zwischenprüfungen (pardon:
Teilabschlussprüfungen). Hierfür habe ich mit einer Gruppe von fünf
Klassenkameraden einige Zusammenfassungen geschrieben. Diese sind immer noch
in einem [Archiv](https://github.com/patrickbucher/archive/tree/master/pdfs) auf
GitHub zu finden. Wir nannten uns damals «Team Eichhof». (Das würde ich heute
auch nicht mehr machen…) All diese Dokumente wurden in OpenOffice.org
geschrieben. Ich war der einzige von uns sechs, der das verwendete. Ich weiss
nicht einmal mehr genau, wie ich die Beiträge meiner Kollegen eingebunden hatte.
Wahrscheinlich habe ich sie aus den Word-Dokumenten der Kameraden rauskopiert.

Die meiste Zeit war ich nun auf Linux unterwegs, wobei ich diese
Zusammenfassungen natürlich auch unter Windows hätte bearbeiten können. Wichtig
war, dass mein jeweils aktuelles Arbeitsverzeichnis nun auf der Linux-Festplatte
lag. Beim Dual Boot wählte ich nun immer seltener Windows aus.

Sollten die Zwischenprüfungen problemlos ablaufen, und sollte ich alles
bestehen, wollte ich mir meinen ersten eigenen Computer zur Belohnung kaufen.
Natürlich würde ich mir den selber zusammenbauen, und bloss die Komponenten dazu
kaufen. Wichtig war, dass die Komponenten alle gut von Linux unterstützt wurden.
Das war damals beispielsweise bei WiFi-Karten gar nicht selbstverständlich. Und
da der Computer in meinem Zimmer stehen sollte, war ein Ethernet-Kabel leider
keine Option.

Ein Berufsschulkollege, der schon seit frühem Jugendalter mit Linux arbeitete,
und auch bereits seine eigene Firma hatte, war hierfür ein guter
Ansprechpartner. Ich bestellte die Hardware bei ihm. (Die Prüfungen waren
übrigens sehr gut gelaufen.) Ich staunte sehr, dass er mir die Komponenten mit
seinem eigenen Firmenauto lieferte.

Den Computer hatte ich bald zusammengebaut. Doch leider liess sich SuSE Linux
darauf nicht installieren ‒ oder zumindest funktionierte das WiFi nicht, so
genau kann ich mich nicht mehr darain erinnern. Auf jeden Fall gab es ein
Problem mit SuSE. So habe ich einen Plan B gebraucht.

# Ubuntu: Ein gelungener Umstieg

Zu dieser Zeit wurde gerade _Ubuntu_ einigermassen populär. Ich war zwar auf
SuSE ein begeisterter KDE-Benutzer und hätte darum auch zu _Kubuntu_ wechseln
können. Ich wollte aber doch lieber das «Original» einmal ausprobieren.

Ubuntu liess sich problemlos installieren. Ich weiss nicht mehr, ob es _Breezy
Badger_ (5.10, am 12. Oktober 2005 erschienen) oder die Vorgängerversion _Hoary
Hedgehog_ (5.04, am 8. April 2005 erschienen) war. Auf jeden Fall funktionierte
alles auf Anhieb, auch das WiFi.

An GNOME gewöhnte ich mich sehr schnell. Es war übersichtlicher und eleganter
als KDE. Es funktionierte alles so, wie es musste. Aus dieser Zeit ist mir
ansonsten eher wenig geblieben.

Ab und zu musste ich wohl auch noch am Windows-Rechner arbeiten, denn in der
Berufsschule wurde immer noch der Microsoft-Stack unterrichtet. _Microsoft SQL
Server_ habe ich mit Sicherheit einmal verwenden müssen. Geblieben ist mir davon
wenig. Die gleichen Übungen hätte man auch mit MySQL oder PostgreSQL machen
können.

2006 kaufte ich mir dann sogar einen eigenen Laptop. Der Lehrlingslohn war ja
mit dem dritten Lehrjahr bedeutend angestiegen. Das HP-Notebook hatte einen
verspiegelten Bildschirm. (Diesen Fehler würde ich heute nicht mehr machen.)
Doch Ubuntu lief darauf problemlos. Ich konnte den Laptop auch in die Schule
mitbringen und darauf arbeiten. Aber ans Netzwerk durfte ich ihn nicht
anschliessen, aus Sicherheitsgründen, versteht sich. Eine externe USB-Festplatte
diente zum Dateiaustausch.

So bin ich bis zum Lehrabschluss bei Ubuntu geblieben. Für die
Lehrabschlussprüfungen haben wir wieder in der gleichen Gruppe wie zwei Jahre
zuvor Zusammenfassungen geschrieben. Dieses mal nicht mehr als «Team Eichhof»,
aber wiederum mit OpenOffice.org. Die Zusammenfassung für die Allgemeinbildung
hatte ich selbständig mit LaTeX verfasst. (Diese war Jahre später noch einem
Lehrling hilfreich, sodass sich dieser per E-Mail bei mir bedankte.)

# Debian: Ubuntu für Erwachsene

2009 kaufte ich mir gleich zwei Computer. Einerseits einen Dell OptiPlex als
Computer für mein Zimer, und andererseits ein Lenovo Thinkpad (mit grosszügigem
Studentenrabatt) für mein Informatik-Studium.

Ich weiss nicht mehr, ob Ubuntu auf einem der beiden Rechnern nicht
funktionierte. Auf jeden Fall stieg ich in dieser Zeit auf Debian um, das den
Ruf hatte, schwer installierbar zu sein. Tatsächlich waren es einfach ein paar
Klicks mehr im Setup-Menü als bei Ubuntu.

Auf meinem Laptop hatte ich einen Dual Boot eingerichtet, da ich ja im
Informatikstudium weiterhin würde Windows verwenden müssen. (Daran hat sich bis
heute kaum etwas geändert.)

Von Ubuntu her waren mir viele Konzepte für Debian schon bekannt, zumal ja
Ubuntu auf Debian basiert. Den Paketmanger `apt-get` verwendete ich auch über
die Kommandozeile, und kaum noch über ein grafisches Tool, dessen Name mir
entfallen ist.

Ich arbeitete nun schon seit etwa fünf Jahren mit Linux, war aber nur ein
Anwender, und keinesfalls ein Profi. Wenn ich etwas auf der Kommandozeilen
machen musste, dann kopierte ich mir diese Befehle von einer Webseite, und
hoffte, dass sie funktionieren würde. Ich war auch weiterhin in der alten
Windows-Routine verhaftet, dass ich das Betriebssystem komplett neu
installierte, wenn etwas grundsätzliches nicht mehr funktionierte. Verstanden
habe ich vom System sehr wenig.

Zu dieser Zeit verlor ich auch die Lust an der Informatik. Der Grund dafür
dürfte eine Kombination aus meiner Situation in Beruf und Hochschule gewesen
sein, wobei auch der Mangel an Freizeit über mehrere Jahre (Berufsmatura,
Studium) mit Lektionen am Samstag, an den Abenden und Lernen am Wochenende auch
eine Rolle gespielt haben dürfte.

Ich entschloss mich dazu, mein Informatikstudium abzubrechen (bzw. offiziell
bloss zu unterbrechen), und die Matura nachzuholen. Ich wollte lieber
Geistes- und Sprachwissenschaften studieren, als mich noch länger mit der
Informatik zu beschäftigen. Zunächst wollte ich aber mein Französisch aufbessern
und ging im Sommer 2010 für einige Wochen nach Paris.

Auf diese Zeit geht auch meine Aversion gegen Bloatware zurück. Ein
Schlüsselerlebnis dürften für mich die Vorträge von [meillo](http://marmaro.de/)
beim Chaos Computer Club Ulm gewesen sein. Schliesslich war es der Window
Manager [dwm](http://dwm.suckless.org/), der mich nachhaltig auf einen anderen
Pfad bringen sollte: Weg vom GUI, hin zur Kommandozeile!

Zunächst verwendete ich weiterhin den GNOME-Login-Bildschirm. Ich schaffte es,
`dwm` als zweite Option (neben dem GNOME-Desktop) zu konfigurieren. So konnte
ich notfalls immer noch auf GNOME ausweichen. Meine grafische Oberfläche war
aber nun `dwm`. Dies hat sich bis heute nicht geändert.

Ich verwendete dieses Setup einige Jahre lang auf meinem Laptop und meinem
Heimrechner. Nun machte ich auch Fortschritte auf der Kommandozeile. Ich
verwendete aber immer noch grösstenteils die Konfigurationstools des Systems.
Für die Netzwerkverbindung war beispielsweise WICD im Einsatz.

In der Zwischenzeit war in meinem Leben einiges passiert: Ich absolvierte die
Passerelle, hatte ein einjähriges Gastspiel in Fribourg, wo ich Slavistik und
Germanistik studierte ‒ und kehrte 2012 dann doch wieder in die Informatik
zurück. Meine Lust am Programmieren hatte ich wohl wiederentdeckt.

In diesen Jahren hatte ich mir auch ein Netbook angeschafft: eine Gattung
Geräte, die von den Tablets verdrängt worden sind. Es muss auf diesem Netbook
gewesen sein, wo ich zum ersten mal ein Betriebssystem ohne GUI installiert
habe. Seither startete ich `dwm` direkt von der Kommandozeile, einen
Login-Screen hatte ich nicht mehr. Diese Installation dokumentierte ich in einem
Artikel namens [Lean Debian](https://web.archive.org/web/20150217043316/http://paedubucher.ch/docs/lean-debian.html).

# Arch: Das vorläufige Ende einer Reise

2016 entschied ich mich dazu, mein Informatik-Studium an der Hochschule Luzern
wieder aufzunehmen und also doch noch zu beenden. Im Sommer hatte ich eine
Aktion entdeckt: einen ultraschwachen Acer-Laptop für 199 Franken mit 32 GB
internem Speicher, der dafür aber extrem leicht und energieeffizient war: der
ideale Laptop fürs Studium!

Die Debian-Installation scheiterte dabei leider. Ich stand wieder vor dem
gleichen Problem, das mich schon früher hat die Distribution wechseln lassen.
Doch mit Debian war ich doch so zufrieden…

Ich probierte verschiedenste Distributionen aus. Einige davon basierten auf _Arch
Linux_. Damit funktionierte alles auf Anhieb, ich hatte aber immer die grafische
Benutzeroberfläche dabei. So wagte ich mich an die manuelle Installation des
«richtigen» Arch Linux heran, wofür ich seither eine personalisierte
[Dokumentation](https://github.com/patrickbucher/docs/blob/master/arch-setup/arch-setup.md)
führe.

Die ganze Sache lief doch recht problemlos ab, sodass ich Arch gleich noch auf
meinem «richtigen» Laptop installierte. (Ich wollte damals diesen Laptop für
Windows brauchen, war aber jetzt zu begeistert von Arch.) Dabei musste ich wohl
vergessen haben, das Mounten der `/boot`-Partition in `/etc/fstab` festzuhalten,
sodass sich der Laptop nach dem nächsten Kernel-Update nicht mehr aufstarten
liess.

Ich verfluchte Linux wie kaum jemals zuvor ‒ und wie seither niemals wieder.
Denn der Fehler war ganz klar auf meiner Seite. Endlich lernte ich etwas übers
System. Das Problem löste ich nicht durch eine komplette Neuinstallation,
sondern indem ich das System mit dem USB-Stick startete und das Mounten der
`/boot`-Partition korrekt konfigurierte. Für mich war das ein Meilenstein.

Im Studium habe ich mich dann weitgehendst an Linux gehalten. Ausnahmen waren
Prüfungen mit dem _Safe Exam Browser_, der eben nur unter Windows und macOS das
System komplett blockieren konnte. In den Modulen _C# in Action_ und
_Microcontroller_ stand auch gezwungenermassen Windows-Einsatz auf dem Programm,
sodass es kaum ein Zufall ist, dass ich diese beiden Module abgebrochen habe.

In der Zwischenzeit arbeitete ich in einer Firma mit macOS. Auf meiner neuen
Stelle kann ich komplett mit Linux arbeiten. Neben Arch Linux auf dem Laptop
kommt auf den Servern Ubuntu zum Einsatz.

# Ausprobiert: Alpine Linux, OpenBSD, FreeBSD

Wenn ich mit Docker-Containern arbeite, ist oft das schlanke _Alpine Linux_
meine Wahl für das Base-Image. Auf einem Heimrechner oder auf einem Laptop habe
ich es bisher noch nicht ernsthaft verwendet. Das dürfte wohl mit der etwas
älteren Kernel-Version zusammenhängen. Auch auf Servern verwende ich es nicht,
da es von vielen Cloud-Anbietern nicht angeboten wird. Dort verwende ich Debian
‒ oder Ubuntu, wenn ich auf neuere Packages angewiesen bin. (Lokal kann man
schon einmal Debian Testing verwenden, das läuft dermassen stabil.)

Weiter habe ich dieses Jahr einige kleinere Ausflüge in die BSD-Welt
unternommen. OpenBSD scheint mir wie geschaffen zu sein für meine Ansprüche:
alles ist minimal, standardmässig sinnvoll konfiguriert und sicher. FreeBSD ist
mir in der Firma begegnet, wo ein Backup-Server (mit ZFS als Dateisystem) damit
läuft.

Für meinen privaten Einsatz konnte sich aber noch keines der beiden Systeme
gegen Arch durchsetzen. Gerade bei Laptops läuft Linux mittlerweile so gut, dass
die BSDs eher ein Rückschritt in vielerlei Hinsicht wäre.

Seit einigen Monaten betreibe ich einen kleinen Server in der Cloud auf Debian.
Hier wäre vielleicht OpenBSD eine sinnvolle Alternative, die ich gelegentlich
prüfen sollte. Überhaupt möchte ich mich gelegentlich stärker mit den BSDs
befassen als mit Linux.

Für die «Hardcore»-Distributionen wie _Gentoo_ und _Linux from Scratch_ konnte
ich mich bisher noch nicht begeistern. Es wären wohl beides gewinnbringende
Übungen.

Im Moment stehen für mich aber andere Themen an, z.B. die funktionale
Programmierung. So bleibe ich vorerst bei Arch Linux, und lasse mich von der
Zukunft überraschen… OpenBSD und FreeBSD laufen mir ja nicht weg.
</content>
    </entry>
    <entry>
        <title>Testing Is Not About Programming</title>
        <link href="https://paedubucher.ch/articles/2018-12-16-testing-is-not-about-programming.html"/>
        <author>
            <name>Patrick Bucher</name>
        </author>
        <id>https://paedubucher.ch/articles/2018-12-16-testing-is-not-about-programming.html</id>
        <updated>2018-12-16T12:00:00Z</updated>
        <content type="text/markdown; charset=UTF-8">
Programming is the process of writing code in order to solve a problem.
Software Engineering is programming, combined with the factors time and other
programmers.

Tests cannot prove the correctness of a program. A test case makes sure that a
couple of hand-picked values from the input set are mapped correctly to a
couple of hand-picked values from the solution set. As Dijkstra wrote, in order
to get a program right, forget about the individual items of the set and work
with the set definition—which is quite the contrary of writing test cases. So
tests are not the right tool when it comes to getting programs right in the
first place.

However, tests can be very helpful in the area of Software Engineering. There, they
aren't used to proof the correctness of a program, but to make sure that a test
covered program doesn't change its test-covered behaviour as time goes, and
other programmers work on it.
</content>
    </entry>
    <entry>
        <title>Distracted by Tools</title>
        <link href="https://paedubucher.ch/articles/2018-10-18-distracted-by-tools.html"/>
        <author>
            <name>Patrick Bucher</name>
        </author>
        <id>https://paedubucher.ch/articles/2018-10-18-distracted-by-tools.html</id>
        <updated>2018-10-18T12:00:00Z</updated>
        <content type="text/markdown; charset=UTF-8">
I’m doing a bachelor’s degree at a university of applied sciences with its own
computer science department. I work part-time besides and already have a couple
of years of practice in the field. In fact, I’ll soon reach the point of being
longer in IT than I have spent time outside of it, for I’m already 31 years old
and started my apprenticeship with the age of 16.

I’m mostly interested in programming. This term, I picked two rather big
courses on that topic:

- _Programming Concepts and Paradigms_: This course has four parts. First, we
  got an overview of different fields of languages and looked at the
  differences between structural and object-oriented programming languages.
  Then we went over to declarative programming languages and spent three weeks
  on logical programming with Prolog, which was a really rewarding experience.
  The last couple of weeks we’ll get the chance to dig into a programming
  language of our own choice.  But today, we started with
  declarative-functional programming, which we’ll do in Racket/Scheme.
- _Microcontrollers_: This course has two parts: C Programming in general, and
  working on a specific microcontroller platform with assembly language and C
  in particular.

I already glanced at LISP a few times, and am somewhat familiar with the C
programming language (I worked through the K&amp;R book and wrote a couple of tiny
helper programs for my personal use). Therefore, I don’t feel anxious about
failing and can look forward to do the things well with reasonable effort. (I’d
also like to spend some time on Go besides, if possible.) I am also willing to
do the things the way it suits my working style best (working alone, not in a
group; skip classes and read about the subjects on my own if, I feel like it).

However, the professors sometimes give me a hard time.

In the microcontrollers course, we’re supposed to use Windows. All the example
projects are for the Codewarrior IDE, which is based on Eclipse. I asked a
couple of Linux users, who did the course before, and all of them sticked to
Windows for that particular part. Because working with microcontrollers is a
total new field for me (except some Arduino toy projects), I’d rather stick to
the professor’s instructions and use the proposed environment. 

For the C part of the course, however, which takes place on a different day, I
use my usual Arch Linux setup consisting of `vim`, `gcc` and `Makefile`s. The
professor told us to use NetBeans for that purpose, which is a bit weird in my
opinion.  If we use the Eclipse-based IDE Codewarrior for C programming on
microcontrollers, we could certainly also use it for C programming on our local
machine. But I won’t use an IDE for the C exercises anyway.

So I did fine with the C setup of my choice so far. But today, I had to present
my solution to a mandatory exercise. It was about refactoring the
reverse-polish calculator from the K&amp;R book. The one big `main.c` file needed
to be broken down into a header file and a couple of `.c` files. I also wrote a
`Makefile` and used some additional compiler flags, which I both quickly showed
and commented on. I demonstrated the program and showed the code in my terminal
using `vim`.

The professor considered the exercise being well done and gave me a good hint
that I could handle some variables differently. I realized that I should make
them `static`, which I did on the fly. I re-compiled the program and ran it
again, everything worked.

When I went back to my desk, the professor made an additional remark. He asked
me, why I use `vim`. I simply stated that it is a very capable text editor, and
that I’m extremely efficient working with it. He then told me a story about his
apprenticeship, where apprentices sometimes used the wrong tools for a job,
say, using pliers instead of a wrench for tightening a nut (it was mechanics,
not IT). So he suggested using the _best_ tool for the task at hand, which is a
modern IDE like NetBeans, and not `vim`, according to him. Besides that,
`Makefile`s were something the IDE generates, nothing to write on one’s own. So
I should really get into using a “real and professional IDE”, and not hacky old
`vim`.

I didn’t want to start an argument, because it was already late in the evening,
and three more students needed to demonstrate their exercises within the next
couple of minutes left. I just said that I use IDEs at work (mostly Eclipse and
IntelliJ for Java, and sometimes RStudio for R), but that I prefer _simple_
tools for simple tasks, like the exercise at  hand.

A fellow student, who also mostly uses Linux, used Visual Studio Code for both
C programming and for the presentation. (The professor didn’t comment on his
choice of tools.) After the course, this fellow student and I walked to the
train station, and we made some ironic remarks about the professor’s comments
to blow off some steam. (As a Linux user, they really give you a hard time at
that university.) The discussion got a bit more serious, and we both agreed
that we should spend more time on really understanding the basics (compiler
flags, the structure of a `Makefile`) and not waste our time with big and
bloaty tools, where we spend time figuring out the difference between two
slightly different icons showing a hammer, which are used to trigger a build
with slightly different options.

On my way home, I had a couple of other thoughts:

- Aren’t we supposed to learn the basics instead of some workflows of specific
  products, which are going to change every couple of years? In the first
  couple of lessons, the professor walked us through the build output, which
  was produced by an IDE-generated `Makefile`. He spent about ten minutes on
  that, because the output was rather big. The program being compiled was just
  a “Hello World” program, which I ran twice during that time by typing `make
  hello &amp;&amp; ./hello` and `tcc -run hello.c`. (I even had enough time for
  installing `tcc` during his explanations.)
- An IDE is not a _tool_, it’s rather an entire _production line_, if the
  manufacturing metaphor should be preserved. And one specific production line
  (NetBeans) isn’t probably the best fit for the construction of every C
  programm. (If it were so, why are we supposed to use _two_ different IDEs for
  the same language in the same course?)
- Do Linux kernel hackers use an IDE other tham `vi`/`vim` or `emacs` with a
  couple of plugins and customisations?  I can’t imagine that, at least not
  something bloaty and Java-based such as NetBeans (or Eclipse). So the point
  that simple tools fail for big code bases, and that an IDE is better suited,
  seems to fail. (I don’t have any hard evidence for my claim, but I read that
  Linus Torvalds uses a customized MicroEMACS, for example. Having watched
  videos featuring other great hackers like Eric Raymond, Richard Stallman and
  Rob Pike, I cannot remember having seen an IDE once.)
- Aren’t computer science professors supposed to motivate students to
  experiment and try things out, at least during their studies? Aren’t we going
  to be told to use language X with IDE Y on platform Z for decades _after_ our
  studies? And aren’t young people the ones that come up with new and better
  ways of doing things once in a while?
- The professor sees the merits in the K&amp;R book, but could he imagine its
  authors having used anything like NetBeans for writing the examples? Unix is
  about simplicity and frugality, not about unconditional use of the most
  powerful tools and concepts, no matter what the task at hand is.
- An IDE uses a lot of resources. One could argue that this is hardly an issue
  with our cheap and fast computers nowadays. Using a rather new Lenovo
  ThinkPad, memory, disk space and CPU load really aren’t the problem. However,
  the space on the screen is very limited, and my minimalistic desktop
  environment based on [dwm](http://dwm.suckless.org/) uses this particular
  resource very efficiently. So does `vim`, which doesn’t distract me with
  dozens of menu items, but gives me a simple and effective interface for
  dealing with text files. One could compare the IDE to an SUV and my setup to
  a bicycle, but that bicycle would run with the speed of a Formula 1 car
  consuming an ounce of olive oil for an entire race. (My setup also works
  great on a Rasperry Pi, even on the single-core version 1.)
- The professor also objected that `vim` doesn’t offer a debugger facility. I
  neither know nor care if there’s a `gdb` plugin for `vim`, but when I had to
  debug a program, I did just well with using `gdb` outside of `vim`. Hell,
  NetBeans also uses `gdb` as a C debugger, and so does Eclipse, but when I use
  raw `gdb`, I at least do not need to remember whether it was Eclipse or
  NetBeans that uses F6 for stepping over a line of code, I just learn the
  self-explanatory `step` command for `gdb` once and use it forever. (Using
  `gdb` productively requires debug symbols, which are included with the `gcc`
  flag `-g`, which would be a great opportunity for “joined-up thinking”—a
  notion to be found on every education expert’s slide desks.)
- An IDE is not only a tool that you can pick up and lay down again, it shapes
  the way your work is organized (folder structures, configuration files).
  Multiple programmers using different IDEs for the same project will certainly
  cause trouble sooner or later, and so will the transition of one IDE to
  another. (I personally don’t see a bright future for NetBeans, which I only
  saw being used _once_ outside of my school.) Working collaboratively with
  `vim` and `emacs`, or switching from one editor to another doesn’t give you
  that amount of pain.

Then I also thought about the other course in the morning, which was about
getting in touch with Racket/Scheme. We saw very little code (not even a
stereotypical Fibonacci or factorial function), talked only a bit about
concepts, but spent a good deal of the two lessons learning about options and
menu points in DrRacket, which is the IDE to be used during the course. I
didn’t figure out yet how to get the previous command again in the REPL
(`Ctrl-P` or up-arrow in most shells), but heard about the option to deactivate
case-sensitivity for identifiers. (I even found out how to switch the user
interface language to Russian, so that for once _I_ could give the professor a
hard time.) Wouldn’t I’ve been motived enough yet to learn Scheme, those two
lessons wouldn’t have done it either.

There are probably dozens of freely available LISP and Scheme implementations
with capabilities exceeding our needs for a three-week introductory course by
far. Pick one, get to learn how to use it, and spend the rest of the time
learning about the concepts (functional programming) and the language (be it
LISP, Scheme, Arc or whatever). But why wasting time installing and fine-tuning
tools we’re only going to use for a couple of weeks and never will see again?

And _if_ time is assigned to getting in touch with tools, why is it wasted on
things as ephemeral as menu items, configuration forms, and not spent more
wisely to get to know the command line flags (which most certainly must be
known in production) or the syntax of a `Makefile`?

So much time is wasted on irrelevant details. (I’m not even talking about CPU
time wasted during startup of those tools, nor about download traffic, memory
and hard disk usage.) For tuition, one should pick the most simple tools that
get a job done, learn how to work with them—and then focus on the subject
matter at hand.

And, _please_, if a student does an effort learning things on his own and
teaching them to his fellow students, be happy that the classroom gets to see a
new perspective on the subject matter—including the professor, who thinks he
already figured out the one and only way.
</content>
    </entry>
    <entry>
        <title>Cargo Cult</title>
        <link href="https://paedubucher.ch/articles/2018-10-05-cargo-cult.html"/>
        <author>
            <name>Patrick Bucher</name>
        </author>
        <id>https://paedubucher.ch/articles/2018-10-05-cargo-cult.html</id>
        <updated>2018-10-05T12:00:00Z</updated>
        <content type="text/markdown; charset=UTF-8">
Accepting interfaces instead of requiring specific implementation is a good
practice. Take Java collections as an example. A method accepting an `ArrayList`
argument will only be useful if the caller is already using an `ArrayList`.
Otherwise he’d need to convert the implementation he’s using, say, `LinkedList`,
to an `ArrayList` first. A method accepting the `List` or even the `Collection`
interface is far more likely to accept what the caller already has.

Many programmers recite this practice as “program on interfaces, not on
implementations”. However, this rule is misleading, because the somewhat blurry
notion of “programming on interfaces” does not mean the same as “accepting
interfaces”.

Once in a while, a unexperienced Java programmer tries to follow the “program
on interfaces” rule and stumbles upon a problem. He wrote:

    List&lt;String&gt; names = new ArrayList&lt;&gt;();

Where’s the problem? The unexperienced Java programmer objects that he’s not
“programming on an interface”, because he wrote `new ArrayList&lt;&gt;()` and hence
has a reference to a specific implementation.

Maybe he’ll try to write a `ListFactory` (he’s a Java Programmer, after all)
that provides a method with the signature `List&lt;T&gt; createList()` that will do
the “dirty work” for him.  But guess what: this method will contain the
instantiation of `ArrayList` as well. The problem is not solved, but wrapped in
an additional layer of pointlessnes.

What will the unexperienced Java programmer do now? He might search the issue
on the web. Maybe he finds an article about dependency injection.  But then
he’ll be wondering where to get that dependency injected from. So he’ll either
give up and stick to his original solution, or post the “issue” on a chat or
forum.

What’s wrong with the original code? Nothing. Interfaces are for references,
classes are for instances. At one point, somebody has to decide on an
implementation. One could wonder, if the compiler or a framework might figure
out the right implementation, optimized for performance according to what the
programmer uses the collection for. A linked list is faster than an array list
if items have to be inserted at the beginning. An array list is faster than a
linked list for random access. But the compiler is unable to figure that out,
at least at this point.

The issue is not only about performance, it’s also about semantics. Whereas the
programmer doesn’t have to care about whether a list is implemented using a
linked list or an array, semantics matter a lot to him in other cases. Take
maps for example. The Java interface `Map&lt;K, V&gt;` has the implementations
`TreeMap&lt;K, V&gt;` and `LinkedHashMap&lt;K, V&gt;`, among others.  When adding an
element to a `TreeMap`, the element will be inserted at a specific position, so
that the map’s element will be sorted in their natural order. A
`LinkedHashMap`, however, preserves the insertion order of the elements.

Of course, methods dealing with maps should still use the `Map` interface. It’s
entirely up to the caller, if and how the elements of the map passed in are
sorted. Maybe he doesn’t care, maybe he cares a lot. But he has to take the
decision. Such decisions not only affect the runtime properties of a program
(performance, memory consumption), but possibly also its semantics and
therefore its results.

So why the misunderstanding of the unexperienced Java programmer that insisted
on “programming on interfaces”? He’s guilty of _Cargo Cult Programming_. He
heard the general rule to “accept interfaces instead of implementations” and
understood it as “use interfaces instead of implementations” or even
interpreted it as “replace class names with interface names” (because he only
used the former in his code). He translated the rule “use A instead of B in
context C” and forgot about the context. He just remembered the “A instead of
B” part.

So how could the unexperienced programmer avoid such misunderstandings? Instead
of literally following some rules he heard of, or following the part of the
rule he understood, he should ask himself, if there’s an actual problem in his
code. If he’s unable to explain that problem in a couple of sentences, he
either doesn’t understand the problem―or maybe there is no problem at all.
</content>
    </entry>
</feed>
